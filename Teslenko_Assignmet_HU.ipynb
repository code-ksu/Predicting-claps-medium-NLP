{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('deeplearing': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "0458830d11edced8e6d53938ab850b9282e4b4a952612c4b0aa81a1edbc4dc60"
   }
  },
  "interpreter": {
   "hash": "0458830d11edced8e6d53938ab850b9282e4b4a952612c4b0aa81a1edbc4dc60"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " Humboldt-University of Berlin  \r\n",
    " Chair of Information Systems School of Business and Economics  \r\n",
    " Sose 2020  \r\n",
    " Advanced Data Analytics for Management Support  \r\n",
    " Stefan Lessmann  \r\n",
    "\r\n",
    "# Assignment \r\n",
    "\r\n",
    "made by  \r\n",
    "Kseniia Teslenko    "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# Content\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Explore the data](#Explore-the-data)\n",
    "3. [Data preparation](#Data-preparation)\n",
    "     * [Test data preparation](#Test-data-preparation)\n",
    "          *   [Treatment of the \"PublicationDetails\" column](#Treatment-of-the-\"PublicationDetails\"-column) \n",
    "          *   [Responses features](#Responses-features)\n",
    "          *   [Online since and month of the year](#Online-since-and-month-of-the-year)\n",
    "          *   [Image count, word count and HTML removal](#Image-count,-word-count-and-HTML-removal)\n",
    "          *   [Reading time](#Reading-time)\n",
    "          *   [Missing values](#Missing-values)\n",
    "          *   [Publication feature](#Publication-feature)\n",
    "     * [Train data preparation](#Train-data-preparation)\n",
    "          *   [Missing values](#Missing-values)\n",
    "          *   [Feature \"publicationname\" in test and train set](#Feature-\"publicationname\"-in-test-and-train-set)\n",
    "          *   [Duplicates](#Duplicates)\n",
    "          *   [Removing non-English articles](#Removing-non-English-articles)\n",
    "          *   [Creating new features: article’s age and month when it was published](#age-and-month)\n",
    "          *   [Calculate reading time](#Calculate-reading-time)\n",
    "          *   [Adjusting column names in test and train data](#Adjusting-column-names-in-test-and-train-data)\n",
    "4. [Text Tokenization](#Text-Tokenization)\n",
    "     * [Text and Header Tokenization](#Text-and-Header-Tokenization)\n",
    "     * [Author and publisher Tokenization](#Author-and-publisher-Tokenization)\n",
    "6. [Word2Vec](#Word2Vec)\n",
    "     * [About Word2Vec](#About-Word2Vec)\n",
    "     * [Implementaion](#Implementaion)\n",
    "     * [Train the model](#Train-the-model)\n",
    "7. [Creating a validation set](#Creating-a-validation-set)\n",
    "8. [Develop predictive models](#Develop-predictive-models)\n",
    "     * [Sequential benchmark model](#Sequential-benchmark-model)\n",
    "     * [Second model](#Second-model)\n",
    "     * [Third model](#Third-model)\n",
    "     * [Final model](#Final-model)\n",
    "     * [Hyperparameter tuning](#Hyperparameter-tuning)\n",
    "10. [Conclusion](#Conclusion)\n",
    "11. [Literature](#Literature)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Importing standard packages. \r\n",
    "import importlib\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "# Assess sentiment classification models \r\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\r\n",
    "\r\n",
    "# Library re provides regular expressions functionality\r\n",
    "import re\r\n",
    "\r\n",
    "# To keep an eye on runtimes\r\n",
    "import time\r\n",
    "\r\n",
    "# Saving and loaded objects\r\n",
    "import pickle\r\n",
    "\r\n",
    "# Library beatifulsoup4 handles html\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "\r\n",
    "# Standard NLP workflow\r\n",
    "import nltk\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk.corpus import wordnet\r\n",
    "from nltk import sent_tokenize"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "Medium is an online publishing platform for people or organisations who want to express themselves or want to share information and knowledge by writing articles and reaching a broad audience. The website defines itself as \"Our sole purpose is to help you find compelling ideas, knowledge, and perspectives\" (Medium). The platform, opposing to traditional blog hosts but very similar to most social media platforms, has its own content rating system, similar to likes in Instagram or reactions in Facebook. Here readers can “clap” to reward the author of a post. Clapping is a way readers show if they like an article and this way they recommend the story to their followers. Another function of clapping is to determine the monetary worth of an article. Medium explains this as follow: \"Currently, Partner Program writers are paid every month based on how members engage with stories. Some factors include reading time (how long members spend reading a story) and applause (how much members clap) […]\"(Botticello 2019). As we can see the number of claps has an important role within the platform. As a result an important question comes into play, namely: What type of article gain the maximum number of claps? By finding an answer to this question we can generalize the type of articles that are interesting for readers, attract them and bring the author popularity and monetary benefits.\n",
    "Nowadays machine learning techniques are used in a variety of fields. These techniques help to gain knowledge from data to make a decision. Machine learning techniques are very diverse and vary greatly. The main approach of this case study is to gain a deeper understanding of neural networks from a practical point of view. The first neural network – perceptron was invented by Frank Rosenblatt in 1957. The perceptron is a logistic regression that has weights and its output is a function of the dot product of the weights and input. Today we can develop not only the perceptron - an example of a simple one-layer neural feedforward network – but also language based neural networks. During this case study we are using an NLP modelling techniques to predict the number of claps for new post on medium. The prediction should be based on an article's content and optionally its metadata. Two data sets are available a train and a test set to assemble the prediction. The main difference between both data sets are the difference in columns. The recreation of the columns content type is essential if the main purpose is to avoid the decrease of the predictive accuracy of the model. The first part of the case study is to get a better understanding of the test and train data, to clean it and recreate the columns in a compatible way. The second part it will focus on developing a predictive model.\n",
    "I developed different models where each iteration is becoming more complex and adding more layers. The first one is the simplest model which only uses one input variable “texts” and only one layer. To improve my results, I decided to create a Word2Vec model and use its weights as a pre-trained embedding in embedding layer. Later I also added more inputs to the models: header and the numeric data from my previously prepared column (see #Data preparation). Even if the results have been improved it was obvious that finding the best suitable parameters could further increase the accuracy of the prediction. I prepared hyperparameter tuning using Facebook’s Ax platform but ultimately was not able to finish it because my computer does not have enough RAM and computational on my CPU would have taken almost a week. That is why during the preparations for parameter tuning I decided to develop a new final model, with new parameters that are based on my previous experience, namely on the results of the three previous models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Explore the data\n",
    "Starting from the test set to analyse the features that are present for the final model is the most obviously approach. The train data exploration is the next step. It is necessary to understand how I can link together both to achieve my goal of creating a model based on the train data to predict on the test set. Because the model can only use information which exists in both, this already limits my possible features and allows to ignore all others. The data preparation on both sets is the most important step to ensure the future model can access as many data sources as possible.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df_test = pd.read_csv(\"C:/ADAMS_tutorials/ADAMS_Assignment/Test.csv\", sep=\",\", encoding=\"utf-8\") #\"ISO-8859-1\"\r\n",
    "df_test.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 514 entries, 0 to 513\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Unnamed: 0          514 non-null    int64 \n",
      " 1   index               514 non-null    int64 \n",
      " 2   Author              514 non-null    object\n",
      " 3   PublicationDetails  514 non-null    object\n",
      " 4   Responses           432 non-null    object\n",
      " 5   Header              506 non-null    object\n",
      " 6   Text                514 non-null    object\n",
      " 7   Length              514 non-null    int64 \n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 32.2+ KB\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "df_test.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0  index           Author  \\\n",
       "0           0      0  Daniel Jeffries   \n",
       "1           1      1    Noam Levenson   \n",
       "2           2      2  Daniel Jeffries   \n",
       "3           3      5   Haseeb Qureshi   \n",
       "4           4      7     William Belk   \n",
       "\n",
       "                              PublicationDetails      Responses  \\\n",
       "0  Daniel Jeffries in HackerNoon.comJul 31, 2017  627 responses   \n",
       "1     Noam Levenson in HackerNoon.comDec 6, 2017  156 responses   \n",
       "2  Daniel Jeffries in HackerNoon.comJul 21, 2017  176 responses   \n",
       "3   Haseeb Qureshi in HackerNoon.comFeb 19, 2018   72 responses   \n",
       "4     William Belk in HackerNoon.comJan 28, 2018   19 responses   \n",
       "\n",
       "                                              Header  \\\n",
       "0  Why Everyone Missed the Most Mind-Blowing Feat...   \n",
       "1  NEO versus Ethereum: Why NEO might be 2018’s s...   \n",
       "2                   The Cryptocurrency Trading Bible   \n",
       "3  Stablecoins: designing a price-stable cryptocu...   \n",
       "4       Chaos vs. Order — The Cryptocurrency Dilemma   \n",
       "\n",
       "                                                Text  Length  \n",
       "0  There’s one incredible feature of cryptocurren...   23401  \n",
       "1  <img class=\"progressiveMedia-noscript js-progr...   23972  \n",
       "2  So you want to trade cryptocurrency?You’ve see...     402  \n",
       "3  A useful currency should be a medium of exchan...   19730  \n",
       "4  Crypto crypto crypto crypto. It’s here. It’s h...    5324  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>Author</th>\n",
       "      <th>PublicationDetails</th>\n",
       "      <th>Responses</th>\n",
       "      <th>Header</th>\n",
       "      <th>Text</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Daniel Jeffries</td>\n",
       "      <td>Daniel Jeffries in HackerNoon.comJul 31, 2017</td>\n",
       "      <td>627 responses</td>\n",
       "      <td>Why Everyone Missed the Most Mind-Blowing Feat...</td>\n",
       "      <td>There’s one incredible feature of cryptocurren...</td>\n",
       "      <td>23401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Noam Levenson</td>\n",
       "      <td>Noam Levenson in HackerNoon.comDec 6, 2017</td>\n",
       "      <td>156 responses</td>\n",
       "      <td>NEO versus Ethereum: Why NEO might be 2018’s s...</td>\n",
       "      <td>&lt;img class=\"progressiveMedia-noscript js-progr...</td>\n",
       "      <td>23972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Daniel Jeffries</td>\n",
       "      <td>Daniel Jeffries in HackerNoon.comJul 21, 2017</td>\n",
       "      <td>176 responses</td>\n",
       "      <td>The Cryptocurrency Trading Bible</td>\n",
       "      <td>So you want to trade cryptocurrency?You’ve see...</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Haseeb Qureshi</td>\n",
       "      <td>Haseeb Qureshi in HackerNoon.comFeb 19, 2018</td>\n",
       "      <td>72 responses</td>\n",
       "      <td>Stablecoins: designing a price-stable cryptocu...</td>\n",
       "      <td>A useful currency should be a medium of exchan...</td>\n",
       "      <td>19730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>William Belk</td>\n",
       "      <td>William Belk in HackerNoon.comJan 28, 2018</td>\n",
       "      <td>19 responses</td>\n",
       "      <td>Chaos vs. Order — The Cryptocurrency Dilemma</td>\n",
       "      <td>Crypto crypto crypto crypto. It’s here. It’s h...</td>\n",
       "      <td>5324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df_test = df_test.drop('Unnamed: 0', axis=1)\r\n",
    "df_test = df_test.drop('index', axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Load data train data to compare with test data and get a first look\r\n",
    "df = pd.read_csv(\"C:/ADAMS_tutorials/ADAMS_Assignment/Train.csv\", sep=\",\", encoding=\"utf-8\")\r\n",
    "df.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 279577 entries, 0 to 279576\n",
      "Data columns (total 50 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   audioVersionDurationSec      279577 non-null  int64  \n",
      " 1   codeBlock                    25179 non-null   object \n",
      " 2   codeBlockCount               279577 non-null  float64\n",
      " 3   collectionId                 137878 non-null  object \n",
      " 4   createdDate                  279577 non-null  object \n",
      " 5   createdDatetime              279577 non-null  object \n",
      " 6   firstPublishedDate           279577 non-null  object \n",
      " 7   firstPublishedDatetime       279577 non-null  object \n",
      " 8   imageCount                   279577 non-null  int64  \n",
      " 9   isSubscriptionLocked         279577 non-null  bool   \n",
      " 10  language                     279577 non-null  object \n",
      " 11  latestPublishedDate          279577 non-null  object \n",
      " 12  latestPublishedDatetime      279577 non-null  object \n",
      " 13  linksCount                   279577 non-null  int64  \n",
      " 14  postId                       279577 non-null  object \n",
      " 15  readingTime                  279577 non-null  float64\n",
      " 16  recommends                   279577 non-null  int64  \n",
      " 17  responsesCreatedCount        279577 non-null  int64  \n",
      " 18  socialRecommendsCount        279577 non-null  int64  \n",
      " 19  subTitle                     271217 non-null  object \n",
      " 20  tagsCount                    279577 non-null  int64  \n",
      " 21  text                         279577 non-null  object \n",
      " 22  title                        279572 non-null  object \n",
      " 23  totalClapCount               279577 non-null  int64  \n",
      " 24  uniqueSlug                   279577 non-null  object \n",
      " 25  updatedDate                  279577 non-null  object \n",
      " 26  updatedDatetime              279577 non-null  object \n",
      " 27  url                          279577 non-null  object \n",
      " 28  vote                         279577 non-null  bool   \n",
      " 29  wordCount                    279577 non-null  int64  \n",
      " 30  publicationdescription       137231 non-null  object \n",
      " 31  publicationdomain            53972 non-null   object \n",
      " 32  publicationfacebookPageName  100874 non-null  object \n",
      " 33  publicationfollowerCount     0 non-null       float64\n",
      " 34  publicationname              137231 non-null  object \n",
      " 35  publicationpublicEmail       101982 non-null  object \n",
      " 36  publicationslug              137231 non-null  object \n",
      " 37  publicationtags              130298 non-null  object \n",
      " 38  publicationtwitterUsername   119851 non-null  object \n",
      " 39  tag_name                     279577 non-null  object \n",
      " 40  slug                         279577 non-null  object \n",
      " 41  name                         279577 non-null  object \n",
      " 42  postCount                    279577 non-null  float64\n",
      " 43  author                       279577 non-null  object \n",
      " 44  bio                          225480 non-null  object \n",
      " 45  userId                       279577 non-null  object \n",
      " 46  userName                     279577 non-null  object \n",
      " 47  usersFollowedByCount         279577 non-null  float64\n",
      " 48  usersFollowedCount           279577 non-null  float64\n",
      " 49  scrappedDate                 279577 non-null  int64  \n",
      "dtypes: bool(2), float64(6), int64(10), object(32)\n",
      "memory usage: 102.9+ MB\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test data consists of 7 variables. We have an indexing and it is obvious that some cases were removed because the index is not in a proper order. We have the author’s name. A column named “Publishing details” which consists of the author’s name, the name of the blog and when an article was published. The next column represents the number of comments on the article. The train data set has the column “Header” and “Text” and also the “Length”. After a small research it became clear that the variable “Length” is the number of words without html content.\n",
    "The function info() gives us the general information about the dataset. As we can see here there are 49 columns in the test set and four types of data: bool(2), float64(6), int64(10), object(32). This means we have an integer, text and categorical data. Most of them will be removed because they are not useful for the model and the future prediction. The labels of features in test and train sets are different. Identifying the columns that have the same or almost the same information in train and test set increases the decision-making power what features should be removed without potentially decreases in the predictive accuracy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation\n",
    "## Test data preparation\n",
    "### Treatment of the \"PublicationDetails\" column\n",
    "\n",
    "Publication details in the train set consist of the author, a “publicationname” and a date but it wasn’t immediately clear which field matches this date. To solve this I was forced to compare two columns and articles on the internet using Google by searching for the title and author. As a result of this investigation it was decided to match this date with the column “firtspublisheddate”. Of course, the probability exists that some articles were updated later and the date here represents another field. I’m using Regular Expressions to separate the data into three different columns. First, I am looking for possible errors in the column. I found that some date data consists of only day and month. Random examination of the articles on the internet helped me to state that the most of them are written in the 2019. Still there is at least one article from the year 2020 with the same missing information. I discovered that medium.com does not display the year if the article was written in the current year. Therefore, I concluded that most articles were scrapped in 2019 and I will use this as default here. There is no way without additional data input to give the accurate year for this missing data. To build a correct function with regular expression I used this website https://regex101.com/ to test my pattern. After splitting the data into three separate columns I am deleting the old one. The columns are named the same way as in the train set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#df_test.PublicationDetails.tail() \r\n",
    "#df_test.PublicationDetails[511]\r\n",
    "# finding broken data\r\n",
    "broken_years = []\r\n",
    "for index, row in df_test.iterrows():\r\n",
    "    detail = row.PublicationDetails\r\n",
    "    date = re.search(r'\\d{4}$', detail)\r\n",
    "    if (date == None):\r\n",
    "        broken_years.append(row)\r\n",
    "#broken_years"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def publicationDetails_split(row):\r\n",
    "  \r\n",
    "  published_string = row['PublicationDetails']\r\n",
    "  date_found = re.search(r'[A-Za-z]{3}\\s\\d{1,2}(,\\s\\d{4})?$', published_string)[0] \r\n",
    "  \r\n",
    "  if ' in ' in published_string:\r\n",
    "    author = re.search(r'^.*(?=\\sin\\s)', published_string)[0] \r\n",
    "  else:\r\n",
    "    author = re.sub(date_found + '$', '', published_string)\r\n",
    "  \r\n",
    "  publication = re.sub(date_found + '$', '', published_string)\r\n",
    "  publication = re.sub('^' + author, '', publication)\r\n",
    "  publication = publication.lstrip(' in ')\r\n",
    "  \r\n",
    "  if not re.match(r'.*\\d{4}$', date_found):\r\n",
    "    date_found = date_found + ', 2019'\r\n",
    "        \r\n",
    "  return pd.Series([author, date_found, publication])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "splitted_df_test = df_test.apply(lambda row: publicationDetails_split(row), axis=1)\r\n",
    "splitted_df_test.columns = [\"author\", \"published\", \"blog\"]\r\n",
    "\r\n",
    "df_test[\"author\"] = splitted_df_test[\"author\"]\r\n",
    "df_test[\"published\"] = splitted_df_test[\"published\"]\r\n",
    "df_test[\"blog\"] = splitted_df_test[\"blog\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "df_test = df_test.drop(\"PublicationDetails\", axis=1)\r\n",
    "df_test = df_test.drop(\"Author\", axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "df_test.columns = [\"responses\", \"header\", \"texts\", \"length\", \"author\", \"published\", \"publication\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Online since and month of the year\n",
    "The examination of medium's statistics, which is freely available, on its website has shown the overall traffic fluctuating during the common year. These conclusions inspired me to create the new variables “month”, which represents the current month of the year, and “online_since”, which gives us an indication of how long an article is available online. I suppose that during cold months people spend more time home and potentially are more interested to read more. At the same time older articles have more chances to become viral and gain more readers and as a result more claps in comparison with a newer ones.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "df_test[\"published\"] = pd.to_datetime(df_test[\"published\"])\r\n",
    "df_test[\"online_since\"] = (pd.datetime.today() - df_test[\"published\"]).dt.days\r\n",
    "df_test = df_test.drop(['published'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "df_test[\"month\"] = pd.DatetimeIndex(df_test['published']).month\r\n",
    "df_test.month.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3     55\n",
       "6     54\n",
       "12    47\n",
       "11    47\n",
       "5     45\n",
       "4     45\n",
       "1     45\n",
       "8     39\n",
       "7     38\n",
       "2     35\n",
       "9     34\n",
       "10    30\n",
       "Name: month, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Responses feature\n",
    "The column responses consist of missing values and it is a mix string. Instead of mix string it is preferable to have it as an integer. First, I set all missing values to \"0 responses\". The next step is to only have numbers in the column. I use a regular expressions method to solve this task.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "df_test.responses.head(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    627 responses\n",
       "1    156 responses\n",
       "2    176 responses\n",
       "3     72 responses\n",
       "4     19 responses\n",
       "5     23 responses\n",
       "6     67 responses\n",
       "7     31 responses\n",
       "8     49 responses\n",
       "9      5 responses\n",
       "Name: responses, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "df_test[\"responses\"].fillna(\"0 responses\", inplace = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "responses = []\r\n",
    "for response in df_test.responses:\r\n",
    "    amount = None\r\n",
    "    if (isinstance(response, str) ):\r\n",
    "        amount = re.search(r'^\\d+', response)\r\n",
    "    if (amount == None):\r\n",
    "        responses.append(0)\r\n",
    "    else:\r\n",
    "        responses.append(amount[0])\r\n",
    "df_test[\"responses\"] = responses"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "df_test.responses.tail()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "509    181\n",
       "510     24\n",
       "511     24\n",
       "512    116\n",
       "513     34\n",
       "Name: responses, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "df_test['responses'] = df_test['responses'].astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Image count, word count and HTML removal \n",
    "Images are presented as html tags. Generally, I need to count the “img” HTML-tags. After counting the images, I need to remove all html tags using the BeautifulSoup library, to count the number of words. I have detected that in some articles there is no space between punctuation and a word. Before counting the words, I need to solve this problem. Sadly, another problem occurred during this process: Some articles included links which naturally come with several punctuations. To avoid http://www.link.de to become “http //www link de” and increase the word count from one to four I needed to replace links with a placeholder “___LINK___” before fixing the punctuation.\n",
    "A chain of several regular expressions seems a suitable method to solve these tasks quick and successful. Finally, I was able to count the words and save the texts without HTML and link overhead. During this process I created two new columns that are necessary to calculate the reading time in the next step.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "texts = []\r\n",
    "images_count = []\r\n",
    "for text in df_test.texts:\r\n",
    "    soup = BeautifulSoup(text)\r\n",
    "    images = soup.findAll('img')\r\n",
    "    len_images = len(images)\r\n",
    "    cleaned_text = soup.get_text()\r\n",
    "    images_count.append(len_images)\r\n",
    "    texts.append(cleaned_text)\r\n",
    "#images_count\r\n",
    "#df_test[\"images_count\"] = images_count\r\n",
    "df_test[\"texts\"] = texts\r\n",
    "df_test[\"images_count\"] = images_count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "#replace links with dummy for word count because removing punctuation would change http://link.de => http //link de  == 3 words\r\n",
    "link_dummy = '___LINK___'\r\n",
    "df_test['texts'] = df_test.apply(lambda row: re.sub(r'https?:\\/\\/[\\w\\-\\.\\/\\?\\#]*(\\s|$)', link_dummy, row['texts']), axis=1)\r\n",
    "df_test['texts'] = df_test.apply(lambda row: re.sub(r'www[\\w\\-\\.\\/\\?\\#]*(\\s|$)', link_dummy, row['texts']), axis=1)\r\n",
    "df_test['texts'] = df_test.apply(lambda row: re.sub(r'\\s[\\w\\-\\.\\/\\?\\#]*\\.com(\\s|$)', link_dummy, row['texts']), axis=1)\r\n",
    "\r\n",
    "#Punctuation\r\n",
    "df_test['texts'] = df_test.apply(lambda row: re.sub(r'(?<=[.,:;\"!?])(?=[^\\sa-z\\\"])', r' ', row['texts']), axis=1)\r\n",
    "df_test['texts'] = df_test.apply(lambda row: re.sub(r'\\xa0', r' ', row['texts']), axis=1)\r\n",
    "\r\n",
    "#word count\r\n",
    "word_counts = []\r\n",
    "for text in df_test.texts:\r\n",
    "    words = len(text.split())\r\n",
    "    word_counts.append(words)\r\n",
    "df_test[\"word_counts\"] = word_counts\r\n",
    "\r\n",
    "df_test = df_test.drop(['length'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "df_test.texts = df_test.apply(lambda row: re.sub(link_dummy, '', row['texts']), axis=1)\r\n",
    "df_test.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 514 entries, 0 to 513\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   responses     514 non-null    int32 \n",
      " 1   header        506 non-null    object\n",
      " 2   texts         514 non-null    object\n",
      " 3   author        514 non-null    object\n",
      " 4   publication   514 non-null    object\n",
      " 5   month         514 non-null    int64 \n",
      " 6   online_since  514 non-null    int64 \n",
      " 7   images_count  514 non-null    int64 \n",
      " 8   word_counts   514 non-null    int64 \n",
      "dtypes: int32(1), int64(4), object(4)\n",
      "memory usage: 34.3+ KB\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading time\n",
    "Each article on medium has a reding time. \"When people see a headline that piques their interest — and know in advance that it only takes a couple of minutes to read — they’re more likely to click the link\". medium.com informs us: \"Read time is based on the average reading speed of an adult […] [translated] into minutes, with an adjustment made for images\" (Medium). To calculate the reading time, we need to calculate the number of words and number of images, which we did in the previous step.\n",
    "The calculation of reading time is based on information from medium itself. Medium.com calculates it is by using the sum of \"the average reading speed of an adult (roughly 265 WPM)” and add “an additional 12 seconds for the first image, 11 seconds for the second image, and minus an additional second for each subsequent image, through the tenth image. Any images after the tenth image are counted at three seconds.\" \n",
    "I wrote a method which replicates this formula to calculate the exact reading time according to Medium and used my previously created columns as input on the test data set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def image_minutes(images_count):\r\n",
    "    sec_per_image = 13\r\n",
    "    sec = 0\r\n",
    "    for x in range(images_count):\r\n",
    "        sec_per_image = sec_per_image - 1\r\n",
    "        if x >= 10:\r\n",
    "            sec_per_image = 3\r\n",
    "        #print(sec_per_image)\r\n",
    "        sec = sec + sec_per_image\r\n",
    "                  \r\n",
    "    return sec\r\n",
    "\r\n",
    "image_minutes(11)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "df_test['read_time'] = df_test.apply(lambda row: 265 / row['word_counts'] + image_minutes(row['images_count']), axis=1)\n",
    "df_test.read_time.head(20)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0     75.069609\n",
       "1     72.072011\n",
       "2      3.581081\n",
       "3     63.087171\n",
       "4      0.317746\n",
       "5     96.049266\n",
       "6     12.518591\n",
       "7      0.274327\n",
       "8     78.054504\n",
       "9     19.794118\n",
       "10     0.970696\n",
       "11     0.985130\n",
       "12     1.095041\n",
       "13    99.086347\n",
       "14     0.136037\n",
       "15    68.335868\n",
       "16    50.430894\n",
       "17    33.143709\n",
       "18     1.292683\n",
       "19    68.222689\n",
       "Name: read_time, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Missing values\n",
    "I was checking for missing values in data set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "total = df_test.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_test.isnull().sum()/df_test.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              Total   Percent\n",
       "header            8  0.015564\n",
       "read_time         0  0.000000\n",
       "word_counts       0  0.000000\n",
       "images_count      0  0.000000\n",
       "online_since      0  0.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>header</th>\n",
       "      <td>8</td>\n",
       "      <td>0.015564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>read_time</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_counts</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>images_count</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>online_since</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "df_test[\"header\"].fillna(\"\", inplace = True)\n",
    "df_test.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 514 entries, 0 to 513\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   responses     514 non-null    int32  \n",
      " 1   header        514 non-null    object \n",
      " 2   texts         514 non-null    object \n",
      " 3   author        514 non-null    object \n",
      " 4   publication   514 non-null    object \n",
      " 5   month         514 non-null    int64  \n",
      " 6   online_since  514 non-null    int64  \n",
      " 7   images_count  514 non-null    int64  \n",
      " 8   word_counts   514 non-null    int64  \n",
      " 9   read_time     514 non-null    float64\n",
      "dtypes: float64(1), int32(1), int64(4), object(4)\n",
      "memory usage: 38.3+ KB\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Publication feature\n",
    "I looked at this feature to understand what publisher is the most common and the most popular.\n",
    "My research and future model is generally based on the features given in the test set. As a result, I do not have any reason to keep a lot of features from train set. The best way to deal with them is to delete them to keep my limited RAM consumption small. I removed them by using the drop() function and I leave only those that are necessary to the further data preparation.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "df_test.publication.unique()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['HackerNoon.com', 'The Coinbase Blog', 'Good Audience',\n",
       "       'Litecoin Project', 'Code Like A Girl', 'Cosmos Blog',\n",
       "       'UX Collective', 'Zebpay', 'freeCodeCamp.org', 'NewCo Shift', '',\n",
       "       'INSURGE intelligence', 'Galleys', 'Slackjaw', 'The Awl',\n",
       "       '60 Months to Ironman', 'IoT For All', 'TE-FOOD', 'Circulate News',\n",
       "       'Sustainable food systems', 'Ensia', 'Endless',\n",
       "       'The Future Market', 'Hyperlink Magazine', 'Dialogue & Discourse',\n",
       "       'One Table, One World', 'Track and Food', 'Real Life Stories',\n",
       "       'The Junction', 'TheBeamMagazine', 'The Establishment', 'The Lily',\n",
       "       'With Love, Zach', 'The Coffeelicious', 'The Mission', 'Frazzled',\n",
       "       'From The Kitchen', 'Lit Up', 'The Billfold', 'Sweetgreen',\n",
       "       'What IF?', 'OriginTrail', 'The Startup', 'Age of Awareness',\n",
       "       '* in theprojects', 'The Haven', 'Self-Driven', 'SAP Design',\n",
       "       'Team XenoX', 'Land And Ladle', 'TomoChain',\n",
       "       'Human Development Project', '八百萬種食法', 'Revista Criado Mudo',\n",
       "       'Local Futures', 'Social Entrepreneurs', 'World of Opportunity',\n",
       "       'Farmdrop', 'Marketing and Entrepreneurship', 'Design @ DoorDash',\n",
       "       'WWF-insights', 'The Synapse', 'BULLY KING Magazine',\n",
       "       'We The Peoples', 'Invironment', 'Thrive Global', 'OneZero',\n",
       "       'The Year of the Looking Glass', 'Netflix TechBlog',\n",
       "       'Entrepreneur First', 'Planport', 'Backchannel',\n",
       "       'Radical Urbanist', 'University of Leeds', 'Track Changes',\n",
       "       'Eniac Ventures', 'Accelerated Intelligence', 'The ASOS Tech Blog',\n",
       "       'Startup Grind', 'Anthemis Insights', 'The Unlisted',\n",
       "       'Figma Design', 'Slack Platform Blog', 'BlockChannel',\n",
       "       'Cognitive Dissident', 'the steamMX™ post', 'Starts With A Bang!',\n",
       "       'WAX.io', 'IoT Chain', 'the Levant', 'ArcNews by Esri',\n",
       "       'ABUNDANCE INSIGHTS', 'Padlet, Ink.', 'Tech Diversity Files',\n",
       "       'Signal v. Noise', 'blog.Stockphoto.com', 'Tamboo',\n",
       "       'ThinkGrowth.org', 'Towards Data Science',\n",
       "       'Startup Lessons Learned', 'Be Yourself'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "df_test.publication.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Netflix TechBlog            175\n",
       "                             94\n",
       "TE-FOOD                      34\n",
       "The Startup                  24\n",
       "HackerNoon.com               21\n",
       "                           ... \n",
       "Hyperlink Magazine            1\n",
       "BlockChannel                  1\n",
       "Accelerated Intelligence      1\n",
       "Zebpay                        1\n",
       "Cognitive Dissident           1\n",
       "Name: publication, Length: 101, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "df_test.publication.head(15)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0        HackerNoon.com\n",
       "1        HackerNoon.com\n",
       "2        HackerNoon.com\n",
       "3        HackerNoon.com\n",
       "4        HackerNoon.com\n",
       "5        HackerNoon.com\n",
       "6     The Coinbase Blog\n",
       "7        HackerNoon.com\n",
       "8        HackerNoon.com\n",
       "9        HackerNoon.com\n",
       "10        Good Audience\n",
       "11       HackerNoon.com\n",
       "12       HackerNoon.com\n",
       "13       HackerNoon.com\n",
       "14     Litecoin Project\n",
       "Name: publication, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Data preparation\n",
    "I took more closer look at the train data and deleted the variables that are not existing in the test set and do not bring any advantages for the future predictive model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   audioVersionDurationSec codeBlock  codeBlockCount  collectionId  \\\n",
       "0                        0       NaN             0.0  638f418c8464   \n",
       "1                        0       NaN             0.0  638f418c8464   \n",
       "2                        0       NaN             0.0  638f418c8464   \n",
       "3                        0       NaN             0.0           NaN   \n",
       "4                        0       NaN             0.0           NaN   \n",
       "\n",
       "  createdDate      createdDatetime firstPublishedDate firstPublishedDatetime  \\\n",
       "0  2018-09-18  2018-09-18 20:55:34         2018-09-18    2018-09-18 20:57:03   \n",
       "1  2018-09-18  2018-09-18 20:55:34         2018-09-18    2018-09-18 20:57:03   \n",
       "2  2018-09-18  2018-09-18 20:55:34         2018-09-18    2018-09-18 20:57:03   \n",
       "3  2018-01-07  2018-01-07 17:04:37         2018-01-07    2018-01-07 17:06:29   \n",
       "4  2018-01-07  2018-01-07 17:04:37         2018-01-07    2018-01-07 17:06:29   \n",
       "\n",
       "   imageCount  isSubscriptionLocked  ...        slug        name postCount  \\\n",
       "0           1                 False  ...  blockchain  Blockchain  265164.0   \n",
       "1           1                 False  ...     samsung     Samsung    5708.0   \n",
       "2           1                 False  ...          it          It    3720.0   \n",
       "3          13                 False  ...  technology  Technology  166125.0   \n",
       "4          13                 False  ...    robotics    Robotics    9103.0   \n",
       "\n",
       "         author  bio        userId    userName  usersFollowedByCount  \\\n",
       "0   Anar Babaev  NaN  f1ad85af0169  babaevanar                 450.0   \n",
       "1   Anar Babaev  NaN  f1ad85af0169  babaevanar                 450.0   \n",
       "2   Anar Babaev  NaN  f1ad85af0169  babaevanar                 450.0   \n",
       "3  George Sykes  NaN  93b9e94f08ca    tasty231                   6.0   \n",
       "4  George Sykes  NaN  93b9e94f08ca    tasty231                   6.0   \n",
       "\n",
       "   usersFollowedCount scrappedDate  \n",
       "0               404.0     20181104  \n",
       "1               404.0     20181104  \n",
       "2               404.0     20181104  \n",
       "3                22.0     20181104  \n",
       "4                22.0     20181104  \n",
       "\n",
       "[5 rows x 50 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audioVersionDurationSec</th>\n",
       "      <th>codeBlock</th>\n",
       "      <th>codeBlockCount</th>\n",
       "      <th>collectionId</th>\n",
       "      <th>createdDate</th>\n",
       "      <th>createdDatetime</th>\n",
       "      <th>firstPublishedDate</th>\n",
       "      <th>firstPublishedDatetime</th>\n",
       "      <th>imageCount</th>\n",
       "      <th>isSubscriptionLocked</th>\n",
       "      <th>...</th>\n",
       "      <th>slug</th>\n",
       "      <th>name</th>\n",
       "      <th>postCount</th>\n",
       "      <th>author</th>\n",
       "      <th>bio</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>usersFollowedByCount</th>\n",
       "      <th>usersFollowedCount</th>\n",
       "      <th>scrappedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>638f418c8464</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:55:34</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:57:03</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>blockchain</td>\n",
       "      <td>Blockchain</td>\n",
       "      <td>265164.0</td>\n",
       "      <td>Anar Babaev</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1ad85af0169</td>\n",
       "      <td>babaevanar</td>\n",
       "      <td>450.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>20181104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>638f418c8464</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:55:34</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:57:03</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>samsung</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>5708.0</td>\n",
       "      <td>Anar Babaev</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1ad85af0169</td>\n",
       "      <td>babaevanar</td>\n",
       "      <td>450.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>20181104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>638f418c8464</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:55:34</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>2018-09-18 20:57:03</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>it</td>\n",
       "      <td>It</td>\n",
       "      <td>3720.0</td>\n",
       "      <td>Anar Babaev</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1ad85af0169</td>\n",
       "      <td>babaevanar</td>\n",
       "      <td>450.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>20181104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018-01-07 17:04:37</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018-01-07 17:06:29</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>technology</td>\n",
       "      <td>Technology</td>\n",
       "      <td>166125.0</td>\n",
       "      <td>George Sykes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93b9e94f08ca</td>\n",
       "      <td>tasty231</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20181104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018-01-07 17:04:37</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018-01-07 17:06:29</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>robotics</td>\n",
       "      <td>Robotics</td>\n",
       "      <td>9103.0</td>\n",
       "      <td>George Sykes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93b9e94f08ca</td>\n",
       "      <td>tasty231</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20181104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "df[\"text\"][0]\n",
    "#df.postId.head(20)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Private Business, Government and Blockchain\\n\\nA major private IT company implements blockchain, artificial intelligence, and Internet of Things to optimize and improve high technology workflow. The representatives of a major state structure from the same country like this experiment so much they decide to use it in their work and conclude an agreement with the IT giant. This is an ideal example of interaction between private business and the state regarding blockchain, don’t you think? What is even better is that this story is real: in South Korea a local customs office has signed the respective partnership agreement with Samsung. I believe that the near-term development of blockchain will be built on just such examples of cooperation. In a world where all the best technological decisions are copied at supersonic speed, one cannot remain behind the trends for long. That’s why I’m confident that blockchain and other crypto technologies will soon be adopted around the world. In the 21st century it would be strange to go searching for a telephone booth to make a call, when you can do so from anywhere on the planet with one click on your gadget.\\nhttps://www.coindesk.com/korea-taps-samsungs-blockchain-tech-to-fight-customs-fraud/\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "#df[\"language\"].head()\n",
    "#df.language.value_counts()\n",
    "#df.language.count() - df[df['language']==\"en\"].language.count()\n",
    "count_all_languages = df.language.count()\n",
    "count_english = df[df['language']==\"en\"].language.count()\n",
    "(count_all_languages - count_english) / count_all_languages * 100 #this percent is with dublicates after duplicates removal this number will decrease"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7.841131423543425"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "df.drop([\"publicationfollowerCount\", \"codeBlock\", \"publicationdomain\", \"publicationfacebookPageName\", \"publicationpublicEmail\",\n",
    "          \"publicationtwitterUsername\", \"publicationtags\", \"publicationdescription\", \"publicationslug\", \n",
    "          \"collectionId\", \"bio\", \"audioVersionDurationSec\", \"codeBlockCount\", \"createdDatetime\", \"createdDate\", \"firstPublishedDatetime\", \"isSubscriptionLocked\", \"tagsCount\", \"uniqueSlug\", \"updatedDate\", \"updatedDatetime\", \"slug\", \"postCount\", \"usersFollowedByCount\", \"usersFollowedCount\", \"scrappedDate\", \"latestPublishedDate\", \"latestPublishedDatetime\", \"linksCount\", \"recommends\", \"imageCount\", \"socialRecommendsCount\", \"subTitle\", \"vote\", \"userId\", \"userName\", \"name\"], axis=1, inplace=True)\n",
    "\n",
    "df.info()   "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 279577 entries, 0 to 279576\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   firstPublishedDate     279577 non-null  object \n",
      " 1   language               279577 non-null  object \n",
      " 2   postId                 279577 non-null  object \n",
      " 3   readingTime            279577 non-null  float64\n",
      " 4   responsesCreatedCount  279577 non-null  int64  \n",
      " 5   text                   279577 non-null  object \n",
      " 6   title                  279572 non-null  object \n",
      " 7   totalClapCount         279577 non-null  int64  \n",
      " 8   url                    279577 non-null  object \n",
      " 9   wordCount              279577 non-null  int64  \n",
      " 10  publicationname        137231 non-null  object \n",
      " 11  tag_name               279577 non-null  object \n",
      " 12  author                 279577 non-null  object \n",
      "dtypes: float64(1), int64(3), object(9)\n",
      "memory usage: 27.7+ MB\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Missing values\n",
    "Missing values are common in data sets. Generally, it is necessary to understand the nature of missing values and answer the following question: \"Are the data that are missing random, or are they non-random and potentially biasing?\"(Schlomer et al. 2015, p.2). Researches do not have a consensus what amount of missing data becomes problematic. For example, Schafer states that up to 5% of missing data means that it should best be cut out. Bennett suggested that more than 10% of missing data biased the analyse and others are talking about 20% (Schlomer et al. 2015, p.2. In present case 50 percent of missing values is a sign to better remove a column. But publication name is an important feature for the future predictive model. The better decision on my point of view is to fill missing values. I will use the function fillna() which fills NaNs with some new information like \"No Publication Name\" or with an empty string. Empty strings do not have any grammatical or syntactical pressure for the future NLP model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "total = df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                        Total   Percent\n",
       "publicationname        142346  0.509148\n",
       "title                       5  0.000018\n",
       "author                      0  0.000000\n",
       "tag_name                    0  0.000000\n",
       "wordCount                   0  0.000000\n",
       "url                         0  0.000000\n",
       "totalClapCount              0  0.000000\n",
       "text                        0  0.000000\n",
       "responsesCreatedCount       0  0.000000\n",
       "readingTime                 0  0.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>publicationname</th>\n",
       "      <td>142346</td>\n",
       "      <td>0.509148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_name</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wordCount</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totalClapCount</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>responsesCreatedCount</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>readingTime</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "df[df[\"title\"].isnull()] #the result is like: five NAs are sequential, with the same creation data and the same author\n",
    "#df[\"title\"][43438]\n",
    "#df[\"url\"][43442] #all five have the same url"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      firstPublishedDate language        postId  readingTime  \\\n",
       "43438         2018-04-14       en  3e691fa8543b     0.003774   \n",
       "43439         2018-04-14       en  3e691fa8543b     0.003774   \n",
       "43440         2018-04-14       en  3e691fa8543b     0.003774   \n",
       "43441         2018-04-14       en  3e691fa8543b     0.003774   \n",
       "43442         2018-04-14       en  3e691fa8543b     0.003774   \n",
       "\n",
       "       responsesCreatedCount   text title  totalClapCount  \\\n",
       "43438                      0  N/A\\n   NaN             321   \n",
       "43439                      0  N/A\\n   NaN             321   \n",
       "43440                      0  N/A\\n   NaN             321   \n",
       "43441                      0  N/A\\n   NaN             321   \n",
       "43442                      0  N/A\\n   NaN             321   \n",
       "\n",
       "                                                     url  wordCount  \\\n",
       "43438  https://medium.com/s/story/how-i-became-chief-...          1   \n",
       "43439  https://medium.com/s/story/how-i-became-chief-...          1   \n",
       "43440  https://medium.com/s/story/how-i-became-chief-...          1   \n",
       "43441  https://medium.com/s/story/how-i-became-chief-...          1   \n",
       "43442  https://medium.com/s/story/how-i-became-chief-...          1   \n",
       "\n",
       "         publicationname            tag_name           author  \n",
       "43438  Byzantine.network         Burning Man  Nadia Chilmonik  \n",
       "43439  Byzantine.network    Machine Learning  Nadia Chilmonik  \n",
       "43440  Byzantine.network          Blockchain  Nadia Chilmonik  \n",
       "43441  Byzantine.network  Blockchain Startup  Nadia Chilmonik  \n",
       "43442  Byzantine.network      Cryptocurrency  Nadia Chilmonik  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firstPublishedDate</th>\n",
       "      <th>language</th>\n",
       "      <th>postId</th>\n",
       "      <th>readingTime</th>\n",
       "      <th>responsesCreatedCount</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>totalClapCount</th>\n",
       "      <th>url</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>publicationname</th>\n",
       "      <th>tag_name</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43438</th>\n",
       "      <td>2018-04-14</td>\n",
       "      <td>en</td>\n",
       "      <td>3e691fa8543b</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>321</td>\n",
       "      <td>https://medium.com/s/story/how-i-became-chief-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Byzantine.network</td>\n",
       "      <td>Burning Man</td>\n",
       "      <td>Nadia Chilmonik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43439</th>\n",
       "      <td>2018-04-14</td>\n",
       "      <td>en</td>\n",
       "      <td>3e691fa8543b</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>321</td>\n",
       "      <td>https://medium.com/s/story/how-i-became-chief-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Byzantine.network</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Nadia Chilmonik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43440</th>\n",
       "      <td>2018-04-14</td>\n",
       "      <td>en</td>\n",
       "      <td>3e691fa8543b</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>321</td>\n",
       "      <td>https://medium.com/s/story/how-i-became-chief-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Byzantine.network</td>\n",
       "      <td>Blockchain</td>\n",
       "      <td>Nadia Chilmonik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43441</th>\n",
       "      <td>2018-04-14</td>\n",
       "      <td>en</td>\n",
       "      <td>3e691fa8543b</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>321</td>\n",
       "      <td>https://medium.com/s/story/how-i-became-chief-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Byzantine.network</td>\n",
       "      <td>Blockchain Startup</td>\n",
       "      <td>Nadia Chilmonik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43442</th>\n",
       "      <td>2018-04-14</td>\n",
       "      <td>en</td>\n",
       "      <td>3e691fa8543b</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>321</td>\n",
       "      <td>https://medium.com/s/story/how-i-became-chief-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Byzantine.network</td>\n",
       "      <td>Cryptocurrency</td>\n",
       "      <td>Nadia Chilmonik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "df[\"publicationname\"].fillna(\"\", inplace = True)\n",
    "df = df.dropna(subset=[\"title\"], how=\"all\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "total = df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                       Total  Percent\n",
       "author                     0      0.0\n",
       "tag_name                   0      0.0\n",
       "publicationname            0      0.0\n",
       "wordCount                  0      0.0\n",
       "url                        0      0.0\n",
       "totalClapCount             0      0.0\n",
       "title                      0      0.0\n",
       "text                       0      0.0\n",
       "responsesCreatedCount      0      0.0\n",
       "readingTime                0      0.0\n",
       "postId                     0      0.0\n",
       "language                   0      0.0\n",
       "firstPublishedDate         0      0.0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_name</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publicationname</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wordCount</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totalClapCount</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>responsesCreatedCount</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>readingTime</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postId</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firstPublishedDate</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature \"publicationname\" in test and train set\n",
    "Discovering of this feature in train set has shown the presence of the publisher with the same name but different way of writing. This probably originates from different scaping times and a change of the account name on medium.com in-between. This mistake has a problematic potential in the future. The best way on my point of view is to rewrite such publishers to one consistent name.\n",
    "We can see here the top 20 most important publishers in the test and train dataset. In the train set we can see that only the publisher \"HackerNoon\" differs. I programmatically made them match."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "df.publicationname.value_counts().head(20)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                          142346\n",
       "Towards Data Science                                                       16589\n",
       "Hacker Noon                                                                 5540\n",
       "Becoming Human: Artificial Intelligence Magazine                            3086\n",
       "Data Driven Investor                                                        2834\n",
       "Chatbots Life                                                               2041\n",
       "Chatbots Magazine                                                           1439\n",
       "SyncedReview                                                                1433\n",
       "Planeta Chatbot : todo sobre los Chatbots y la Inteligencia Artificial      1427\n",
       "The Startup                                                                 1383\n",
       "Good Audience                                                               1058\n",
       "freeCodeCamp.org                                                            1005\n",
       "Coinmonks                                                                    995\n",
       "buZZrobot                                                                    787\n",
       "Tech / Telecom News                                                          740\n",
       "AI Hawk                                                                      732\n",
       "Ethercourt Machine Learning                                                  682\n",
       "This Week in Machine Learning & AI                                           672\n",
       "Heartbeat                                                                    653\n",
       "All Turtles                                                                  646\n",
       "Name: publicationname, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "df_test.publication.value_counts().head(20)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Netflix TechBlog        175\n",
       "                         94\n",
       "TE-FOOD                  34\n",
       "The Startup              24\n",
       "HackerNoon.com           21\n",
       "freeCodeCamp.org         15\n",
       "One Table, One World      9\n",
       "60 Months to Ironman      6\n",
       "The Billfold              5\n",
       "The Establishment         4\n",
       "Slackjaw                  4\n",
       "The Future Market         4\n",
       "The Lily                  4\n",
       "The Mission               3\n",
       "Track Changes             3\n",
       "The Awl                   3\n",
       "Startup Grind             3\n",
       "NewCo Shift               3\n",
       "OriginTrail               3\n",
       "What IF?                  2\n",
       "Name: publication, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "df['publicationname'].value_counts().index"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['', 'Towards Data Science', 'Hacker Noon',\n",
       "       'Becoming Human: Artificial Intelligence Magazine',\n",
       "       'Data Driven Investor', 'Chatbots Life', 'Chatbots Magazine',\n",
       "       'SyncedReview',\n",
       "       'Planeta Chatbot : todo sobre los Chatbots y la Inteligencia Artificial',\n",
       "       'The Startup',\n",
       "       ...\n",
       "       'selfdrivingcars', 'Defining Data Science', 'OrbitX', 'YLD Blog',\n",
       "       'adappt Intelligence', 'Crafted Technology— Harel Omer', 'BOTBANHANG',\n",
       "       'Tulua', 'Winning in the Digital Economy', 'Systek'],\n",
       "      dtype='object', length=6507)"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "df.publicationname.str.contains('HackerNoon.com').value_counts().index"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index([False], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "df_test['publication'].value_counts().index"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['Netflix TechBlog', '', 'TE-FOOD', 'The Startup', 'HackerNoon.com',\n",
       "       'freeCodeCamp.org', 'One Table, One World', '60 Months to Ironman',\n",
       "       'The Billfold', 'The Establishment',\n",
       "       ...\n",
       "       'Litecoin Project', 'IoT Chain', 'Galleys', 'ABUNDANCE INSIGHTS',\n",
       "       'The Coffeelicious', 'Hyperlink Magazine', 'BlockChannel',\n",
       "       'Accelerated Intelligence', 'Zebpay', 'Cognitive Dissident'],\n",
       "      dtype='object', length=101)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "pd.Series(df['publicationname'].unique()).isin(df_test['publication'].unique()).value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False    6469\n",
       "True       38\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "df[df.publicationname.str.contains('Netflix')].publicationname"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "43268     Netflix TechBlog\n",
       "43269     Netflix TechBlog\n",
       "43270     Netflix TechBlog\n",
       "43271     Netflix TechBlog\n",
       "62750     Netflix TechBlog\n",
       "                ...       \n",
       "195032    Netflix TechBlog\n",
       "195033    Netflix TechBlog\n",
       "195034    Netflix TechBlog\n",
       "195035    Netflix TechBlog\n",
       "195036    Netflix TechBlog\n",
       "Name: publicationname, Length: 68, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "df_test['publication'] = df_test['publication'].replace(['HackerNoon.com'], 'Hacker Noon')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Duplicates\n",
    "After some research it was discovered that medium allows the author to describe an article by using up to five tags to each story. The data made it obvious that the articles were scraped by querying individual tags e.g. all articles with the tag “machine learning” followed by all articles with the tag “cryptocurrency” and so on. As a result, the same story was scraped multiple times for each tag in the dataset and thus resulting in a massive number of duplicates. Most duplicate rows only differ in the “tag” column. This column is not in our test dataset that is why I decided to just remove them and reduce the workload for my neural network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "doublicateIdentifier = [\"url\", \"postId\"] #url and postId were choseen as an entry that combine all entries that have duplicates in the dataset \r\n",
    "\r\n",
    "multi_tags = df[df.duplicated(subset=doublicateIdentifier, keep=False)]\r\n",
    "\r\n",
    "print(\"There are: \", multi_tags.shape[0], \"Duplicated entries.\")\r\n",
    "print(\"Unique posts with multiple tags: \", multi_tags.shape[0]- df[df.duplicated(subset=doublicateIdentifier, keep=\"last\")].shape[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are:  274050 Duplicated entries.\n",
      "Unique posts with multiple tags:  66814\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "df = df[~df.duplicated(subset=doublicateIdentifier)]\r\n",
    "df.tag_name.head(20)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                  Blockchain\n",
       "3                  Technology\n",
       "7                Data Science\n",
       "11                   Robotics\n",
       "16    Artificial Intelligence\n",
       "21                     Oracle\n",
       "25    Artificial Intelligence\n",
       "26           Machine Learning\n",
       "31    Artificial Intelligence\n",
       "32           Machine Learning\n",
       "36               Data Science\n",
       "39    Artificial Intelligence\n",
       "44                        Sex\n",
       "49                     Python\n",
       "54          Hd Live Streaming\n",
       "57    Artificial Intelligence\n",
       "58                 Technology\n",
       "61           Machine Learning\n",
       "65                      Music\n",
       "70                    Bitcoin\n",
       "Name: tag_name, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removing non-English articles\n",
    "Removing non-English articles is a text mining task. It requires a language identification algorithm. Basically, an NLP algorithm should differentiate different corpuses taking in account the grammatical characteristics of each language. For example, NLTK functions are good for the English language and at the same time FreeLing is the best for Spanish text sources (Data Big Bang). Having multiple networks for each language is the optimal approach. This task is not suitable in this case, because in the test set we have only English articles and non-English content in the train set would be a noise for our future prediction model. This noise can only decrease the prediction accuracy. The best solution in this situation in my point of view is to remove the non-English articles entirely."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "df.language.unique() #the list of all languages"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['en', 'th', 'ja', 'zh', 'ru', 'pt', 'es', 'zh-Hant', 'id', 'my',\n",
       "       'de', 'tr', 'fr', 'ko', 'it', 'lo', 'un', 'vi', 'cs', 'sk', 'is',\n",
       "       'sv', 'bn', 'mn', 'da', 'no', 'bg', 'ar', 'pl', 'nl', 'ro', 'ca',\n",
       "       'hu', 'hi', 'ka', 'el', 'ms', 'uk', 'si', 'sr', 'lt', 'la', 'fa',\n",
       "       'ml', 'sl', 'mr', 'az', 'lv', 'te', 'mk', 'nn', 'fi'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "index = df[df[\"language\"] != \"en\"].index #save the list of all texts that are not english by using their index\r\n",
    "df.drop(index, inplace = True) #drop them\r\n",
    "df.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 66379 entries, 0 to 279572\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   firstPublishedDate     66379 non-null  object \n",
      " 1   language               66379 non-null  object \n",
      " 2   postId                 66379 non-null  object \n",
      " 3   readingTime            66379 non-null  float64\n",
      " 4   responsesCreatedCount  66379 non-null  int64  \n",
      " 5   text                   66379 non-null  object \n",
      " 6   title                  66379 non-null  object \n",
      " 7   totalClapCount         66379 non-null  int64  \n",
      " 8   url                    66379 non-null  object \n",
      " 9   wordCount              66379 non-null  int64  \n",
      " 10  publicationname        66379 non-null  object \n",
      " 11  tag_name               66379 non-null  object \n",
      " 12  author                 66379 non-null  object \n",
      "dtypes: float64(1), int64(3), object(9)\n",
      "memory usage: 7.1+ MB\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "df.language.unique()\r\n",
    "df.drop([\"tag_name\", \"postId\", \"language\"], axis=\"columns\", inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating new features: article’s age and month when it was published\n",
    "These two features were created in test set and the logic of this creation is described above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "df['firstPublishedDate'] = pd.to_datetime(df['firstPublishedDate'])\r\n",
    "df['online_since'] = (pd.datetime.today() - df['firstPublishedDate']).dt.days\r\n",
    "df[\"month\"] = pd.DatetimeIndex(df['firstPublishedDate']).month\r\n",
    "df = df.drop(['firstPublishedDate'], axis=1)\r\n",
    "df.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 66379 entries, 0 to 279572\n",
      "Data columns (total 11 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   readingTime            66379 non-null  float64\n",
      " 1   responsesCreatedCount  66379 non-null  int64  \n",
      " 2   text                   66379 non-null  object \n",
      " 3   title                  66379 non-null  object \n",
      " 4   totalClapCount         66379 non-null  int64  \n",
      " 5   url                    66379 non-null  object \n",
      " 6   wordCount              66379 non-null  int64  \n",
      " 7   publicationname        66379 non-null  object \n",
      " 8   author                 66379 non-null  object \n",
      " 9   online_since           66379 non-null  int64  \n",
      " 10  month                  66379 non-null  int64  \n",
      "dtypes: float64(1), int64(5), object(5)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate reading time\n",
    "On the train dataset the column for reading time already existed but because I discovered that irregularities in the text body and my adjustments regarding punctuation where most likely not present I decided to recalculate it using the same formula as before to have consistent results.\n",
    "The calculation of the reading time in the train set is complicated by the fact that all articles start with a repetition of the title inside the text body. For a proper reading time calculation, we need to remove all titles from text body. I used a regular expression to deal with this.\n",
    "The sequence of actions in this case is important. My research has shown that effective way of removing the title from the body of text is based on the logic of removing the first line, when followed by \\n\\n. Next, I count images, replace hyperlinks, remove html and then I add the new features just like in the test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "df['text'] = df.apply(lambda row: re.sub(re.escape(row['title']) + r'.{0,4}' + '\\n', '', row['text']), axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "texts = []\n",
    "images_count = []\n",
    "for text in df.text:\n",
    "    soup = BeautifulSoup(text) \n",
    "    images = soup.findAll('img')\n",
    "    len_images = len(images)\n",
    "    cleaned_text = soup.get_text() #remove html\n",
    "    cleaned_text = cleaned_text.strip() #remove white spaces\n",
    "    images_count.append(len_images)\n",
    "    texts.append(cleaned_text)\n",
    "#images_count\n",
    "#df_test[\"images_count\"] = images_count\n",
    "df[\"texts\"] = texts\n",
    "df[\"images_count\"] = images_count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "df['texts'] = df.apply(lambda row: re.sub(r'\\n', ' ', row['texts']), axis=1) #still some \\n were detected. to be sure that all are removed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "def image_minutes(images_count):\n",
    "    sec_per_image = 13\n",
    "    sec = 0\n",
    "    for x in range(images_count):\n",
    "        sec_per_image = sec_per_image - 1\n",
    "        if x >= 10:\n",
    "            sec_per_image = 3\n",
    "        #print(sec_per_image)\n",
    "        sec = sec + sec_per_image\n",
    "                  \n",
    "    return sec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "df[df.wordCount == 0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       readingTime  responsesCreatedCount text  \\\n",
       "77229          0.2                      0   \\n   \n",
       "\n",
       "                                                   title  totalClapCount  \\\n",
       "77229  Humanity will suffer under AI if we don´t act ...               0   \n",
       "\n",
       "                                                     url  wordCount  \\\n",
       "77229  https://medium.com/s/story/humanity-will-suffe...          0   \n",
       "\n",
       "      publicationname       author  online_since  month texts  images_count  \n",
       "77229                  Talenter.io          1027     11                   0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>readingTime</th>\n",
       "      <th>responsesCreatedCount</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>totalClapCount</th>\n",
       "      <th>url</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>publicationname</th>\n",
       "      <th>author</th>\n",
       "      <th>online_since</th>\n",
       "      <th>month</th>\n",
       "      <th>texts</th>\n",
       "      <th>images_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77229</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n</td>\n",
       "      <td>Humanity will suffer under AI if we don´t act ...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://medium.com/s/story/humanity-will-suffe...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Talenter.io</td>\n",
       "      <td>1027</td>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It was detected a proble in the row with index 77229. The error sounds as: \"ZeroDivisionError: division by zero\". The best solution is to remove this row."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "df = df.drop([77229], axis = 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "df['read_time'] = df.apply(lambda row: 265 / row['wordCount'] + image_minutes(row['images_count']), axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "link_dummy = '___LINK___ '\n",
    "df['texts'] = df.apply(lambda row: re.sub(r'https?:\\/\\/[\\w\\-\\.\\/\\?=\\#]*(\\s|$)', link_dummy, row['texts']), axis=1)\n",
    "df['texts'] = df.apply(lambda row: re.sub(r'www[\\w\\-\\.\\/\\?\\#]*(\\s|$)', link_dummy, row['texts']), axis=1)\n",
    "df['texts'] = df.apply(lambda row: re.sub(r'\\s[\\w\\-\\.\\/\\?\\#]*\\.com(\\s|$)', link_dummy, row['texts']), axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "#Punctuation\n",
    "df['texts'] = df.apply(lambda row: re.sub(r'(?<=[.,:;\"!?])(?=[^\\sa-z\\\"])', r' ', row['texts']), axis=1)\n",
    "# df['texts'] = df.apply(lambda row: re.sub(r'\\xa0', r' ', row['texts']), axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "df.texts = df.apply(lambda row: re.sub(link_dummy, '', row['texts']), axis=1)\n",
    "df.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 66378 entries, 0 to 279572\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   readingTime            66378 non-null  float64\n",
      " 1   responsesCreatedCount  66378 non-null  int64  \n",
      " 2   text                   66378 non-null  object \n",
      " 3   title                  66378 non-null  object \n",
      " 4   totalClapCount         66378 non-null  int64  \n",
      " 5   url                    66378 non-null  object \n",
      " 6   wordCount              66378 non-null  int64  \n",
      " 7   publicationname        66378 non-null  object \n",
      " 8   author                 66378 non-null  object \n",
      " 9   online_since           66378 non-null  int64  \n",
      " 10  month                  66378 non-null  int64  \n",
      " 11  texts                  66378 non-null  object \n",
      " 12  images_count           66378 non-null  int64  \n",
      " 13  read_time              66378 non-null  float64\n",
      "dtypes: float64(2), int64(6), object(6)\n",
      "memory usage: 7.6+ MB\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "#df= df.drop(['url', 'images_count', 'wordCount', 'text'], axis = 1)\n",
    "df_test = df_test.drop(['word_counts', 'images_count'], axis = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "df.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 66378 entries, 0 to 279572\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   readingTime            66378 non-null  float64\n",
      " 1   responsesCreatedCount  66378 non-null  int64  \n",
      " 2   title                  66378 non-null  object \n",
      " 3   totalClapCount         66378 non-null  int64  \n",
      " 4   publicationname        66378 non-null  object \n",
      " 5   author                 66378 non-null  object \n",
      " 6   online_since           66378 non-null  int64  \n",
      " 7   month                  66378 non-null  int64  \n",
      " 8   texts                  66378 non-null  object \n",
      " 9   read_time              66378 non-null  float64\n",
      "dtypes: float64(2), int64(4), object(4)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "df_test.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 514 entries, 0 to 513\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   responses     514 non-null    int32  \n",
      " 1   header        514 non-null    object \n",
      " 2   texts         514 non-null    object \n",
      " 3   author        514 non-null    object \n",
      " 4   publication   514 non-null    object \n",
      " 5   month         514 non-null    int64  \n",
      " 6   online_since  514 non-null    int64  \n",
      " 7   read_time     514 non-null    float64\n",
      "dtypes: float64(1), int32(1), int64(2), object(4)\n",
      "memory usage: 30.2+ KB\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adjusting column names in test and train data\n",
    "The icing on the top of the first part of this study case is to rename the columns the same way in the test and train sets. For example the header in the train set is labelled title. Responses are named as “responsesCreatedCount”. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "train = df\n",
    "train = train[['responsesCreatedCount', 'title', \"texts\", 'author', 'publicationname', 'month', 'online_since', 'readingTime', 'totalClapCount']]\n",
    "train.rename(columns={ \n",
    "    'responsesCreatedCount': 'responses',\n",
    "    'title': 'header',  \n",
    "    'publicationname': 'publisher',\n",
    "    'readingTime': 'read_time',\n",
    "    'totalClapCount': 'claps',\n",
    "}, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "test = df_test[['responses', 'header', 'texts', 'author', 'publication', 'month', 'online_since', 'read_time']]\n",
    "test.rename(columns={'publication': 'publisher'}, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving data sets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "import pickle\n",
    "#with open('C:\\\\ADAMS_tutorials\\\\ADAMS_Assignment\\\\data_pre_clean_train.pkl','wb') as path_name:\n",
    "    #pickle.dump(train, path_name) #save the data\n",
    "\n",
    "#with open('C:\\\\ADAMS_tutorials\\\\ADAMS_Assignment\\\\data_pre_clean_test.pkl','wb') as path_name:\n",
    "#    pickle.dump(test, path_name) #save the data\n",
    "\n",
    "with open('C:\\\\ADAMS_tutorials\\\\ADAMS_Assignment\\\\data_pre_clean_test.pkl','rb') as path_name:\n",
    "    test = pickle.load(path_name) #load"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "test.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 514 entries, 0 to 513\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   responses     514 non-null    int32  \n",
      " 1   header        514 non-null    object \n",
      " 2   texts         514 non-null    object \n",
      " 3   author        514 non-null    object \n",
      " 4   publisher     514 non-null    object \n",
      " 5   month         514 non-null    int64  \n",
      " 6   online_since  514 non-null    int64  \n",
      " 7   read_time     514 non-null    float64\n",
      "dtypes: float64(1), int32(1), int64(2), object(4)\n",
      "memory usage: 30.2+ KB\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text Tokenization\n",
    "NLTK - The Natural Language Toolkit – provides the necessary tools and methods to process and analyze text data. By using this framework, we can fulfil such tasks as: tokenization, stemming, lemmatization, tagging or parsing. These transformations are the standard procedure of any NLP project.\n",
    "Textual data as a rule is not well formatted and standardized and namely highly unstructured. \n",
    "“Text processing or, to be more specific pre-processing, involves a wide variety of techniques that convert raw text into well-defined sequences of linguistic components that have standard structure and notation. Additional metadata is often also present in the form of annotations to give more meaning to the text components like tags” (Sarkar 2019: 115). Some cleaning techniques such as HTML removing was already made earlier, still the list of further transformation is as follows: \n",
    "\n",
    "    1.\tConvert to lowercase\n",
    "    2.\tSplit into tokens\n",
    "    3.\tRemove punctuation from each token\n",
    "    4.\tFilter out remaining tokens that are not alphabetic\n",
    "    5.\tFilter out tokens that are stop words\n",
    "    6.\tLemmatize tokens\n",
    "    \n",
    "For example, text tokenisation defines syntax and semantics of a smallest textual component. To achieve this the text is broken into separate words. The smallest component is a token.\n",
    "Algorithm can only match specific words or phrases if all words/tokens are in the same case by a upper- or lowercase conversion. In this case I decided to convert them into lowercase tokens. All punctuation is also removed because for algorithm it is just noise. For the same reason it is common to remove all stop words that have little or no significance for our model. Lastly, I lemmatize the tokens. The lemmatization process converts words into their base form (e.g. Sarkar 2019). I used the code from the ADAM’s tutorial as a template. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text and Header Tokenization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "#TRAIN DATA\n",
    "def  clean_text(train):\n",
    "    train['texts'] = train['texts'].str.lower()\n",
    "    train['header'] = train['header'].str.lower()   \n",
    "    # remove numbers\n",
    "    train['texts'] = train['texts'].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    train['header'] = train['header'].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    return train\n",
    "\n",
    "train = clean_text(train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lem_text(text):\n",
    "    #sentences = sent_tokenize(text) #split into sentences\n",
    "    tokens = word_tokenize(text) #splitting strings into tokens nominally words\n",
    "    tokens = [w.lower() for w in tokens] #convert to lower case\n",
    "    words = [word for word in tokens if word.isalpha()] #remove all tokens that are not alphabetic\n",
    "    regex_punc = re.compile('[%s]' % re.escape(string.punctuation)) #prepare regex for char filtering\n",
    "    stripped = [regex_punc.sub('', w) for w in tokens] # remove punctuation from each word\n",
    "    stop_words = stopwords.words('english') #filter out stop words\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    lemma_words =[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]\n",
    "\n",
    "    return lemma_words\n",
    "\n",
    "train['texts'] = train['texts'].apply(lem_text)\n",
    "train['header'] = train['header'].apply(lem_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('C:\\\\ADAMS_tutorials\\\\ADAMS_Assignment\\\\data_lem_train.pkl','wb') as path_name:\n",
    "    pickle.dump(train, path_name) #save the data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('C:\\\\ADAMS_tutorials\\\\ADAMS_Assignment\\\\data_lem_train.pkl','rb') as path_name:\n",
    "    train = pickle.load(path_name) #load"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "#TEST DATA\n",
    "def  clean_text(test):\n",
    "    test['texts'] = test['texts'].str.lower()\n",
    "    test['header'] = test['header'].str.lower()   \n",
    "    # remove numbers\n",
    "    test['texts'] = test['texts'].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    test['header'] = test['header'].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    return test\n",
    "\n",
    "test = clean_text(test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lem_text(text):\n",
    "    #sentences = sent_tokenize(text) #split into sentences\n",
    "    tokens = word_tokenize(text) #splitting strings into tokens nominally words\n",
    "    tokens = [w.lower() for w in tokens] #convert to lower case\n",
    "    words = [word for word in tokens if word.isalpha()] #remove all tokens that are not alphabetic\n",
    "    regex_punc = re.compile('[%s]' % re.escape(string.punctuation)) #prepare regex for char filtering\n",
    "    stripped = [regex_punc.sub('', w) for w in tokens] # remove punctuation from each word\n",
    "    stop_words = stopwords.words('english') #filter out stop words\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    lemma_words =[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]\n",
    "\n",
    "    return lemma_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "test['texts'] = test['texts'].apply(lem_text)\n",
    "test['header'] = test['header'].apply(lem_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the tokenisation and lemmatisation process is done and I got acquainted with the result. I took notice that the lemmatization removed the letter \"s\" from the word \"us\". Potentially it means that there are a lot of single letters in the \"texts\" and \"header\" columns. I noticed that there is also a huge amount of words that are written incorrectly and were therefore tokenized as a new separate word. Removing these mistakes is next necessary step."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "import collections"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "# TRAIN DATA\r\n",
    "word_counter = collections.Counter()\r\n",
    "for t in train.texts:\r\n",
    "    for w in t:\r\n",
    "        word_counter.update({w: 1})\r\n",
    "        \r\n",
    "for h in train.header:\r\n",
    "    for w in h:\r\n",
    "        word_counter.update({w: 1})\r\n",
    "\r\n",
    "word_frequency = sorted(word_counter.items(), key=lambda pair: pair[1], reverse=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "least_common_words = [word for word, freq in word_frequency if freq <= 10]\r\n",
    "len(least_common_words)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "174302"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "#stop_words = stopwords.words('english')\r\n",
    "updated_stopwords = set(least_common_words + ['http']) #there is still http in the text number 3\r\n",
    "print(f'Count stop words: {len(updated_stopwords)}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Count stop words: 174303\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "def filter_words(words):\r\n",
    "    new_words = []\r\n",
    "    for word in words:\r\n",
    "        if len(word) > 2 and not word in updated_stopwords:\r\n",
    "            new_words.append(word)\r\n",
    "    return new_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "train.texts = list(map(filter_words, train.texts))\r\n",
    "train.header = list(map(filter_words, train.header))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "#TEST DATA\r\n",
    "test_word_counter = collections.Counter()\r\n",
    "for t in test.texts:\r\n",
    "    for w in t:\r\n",
    "        test_word_counter.update({w: 1})\r\n",
    "        \r\n",
    "for h in test.header:\r\n",
    "    for w in h:\r\n",
    "        word_counter.update({w: 1})\r\n",
    "\r\n",
    "test_word_frequency = sorted(test_word_counter.items(), key=lambda pair: pair[1], reverse=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "test_least_common_words = [word for word, freq in test_word_frequency if freq <= 10]\n",
    "len(test_least_common_words)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "13755"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "test_updated_stopwords = set(test_least_common_words + ['http']) \n",
    "print(f'Count stop words: {len(test_updated_stopwords)}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Count stop words: 13756\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "def test_filter_words(words):\n",
    "    test_new_words = []\n",
    "    for word in words:\n",
    "        if len(word) > 2 and not word in test_updated_stopwords:\n",
    "            test_new_words.append(word)\n",
    "    return test_new_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "test.texts = list(map(test_filter_words, test.texts))\n",
    "test.header = list(map(test_filter_words, test.header))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Author and publisher Tokenization\n",
    "I noticed that my publisher and author columned needed further transformation to be available in my model that is why I made the decision to leave only the top 500 hundred authors from my dataset and tokenize them. I did the same approach with publishers. The name and family name of an author or the name of a publisher is presented as a number. I did not find any convenient way to do this, so I tried to replicate the process I did for the word tokenization before and unpacked the 1-dimentional list to have a simple integer column. I am convinced there is a better way to do this.\n",
    "I noticed also that it could be usefful to remove the least common words."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "with open('C:\\\\ADAMS_tutorials\\\\ADAMS_Assignment\\\\data_lem_train.pkl','rb') as path_name:\n",
    "    train = pickle.load(path_name) #load"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import collections\n",
    "\n",
    "word_counter = collections.Counter()\n",
    "for t in train.texts:\n",
    "    for w in t:\n",
    "        word_counter.update({w: 1})\n",
    "        \n",
    "for h in train.header:\n",
    "    for w in h:\n",
    "        word_counter.update({w: 1})\n",
    "\n",
    "word_frequency = sorted(word_counter.items(), key=lambda pair: pair[1], reverse=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "least_common_words = [word for word, freq in word_frequency if freq <= 10]\n",
    "len(least_common_words)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "174308"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# keep\n",
    "len([word for word, freq in word_frequency if freq > 10])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "37987"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#stop_words = stopwords.words('english')\n",
    "updated_stopwords = set(least_common_words + ['http'])\n",
    "print(f'Count stop words: {len(updated_stopwords)}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Count stop words: 174309\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def filter_words(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if len(word) > 2 and not word in updated_stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train.texts = list(map(filter_words, train.texts))\n",
    "train.header = list(map(filter_words, train.header))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "NUM_WORDS = 6000 #5000\r\n",
    "# Create tokenizer object and build vocab from the training set\r\n",
    "tokenizer = Tokenizer(NUM_WORDS, oov_token=1)  \r\n",
    "tokenizer.fit_on_texts(train.texts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "tokenizer.fit_on_texts(test.texts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "header_num_words = 1000\r\n",
    "tokenizer_header = Tokenizer(header_num_words, oov_token=1)\r\n",
    "tokenizer_header.fit_on_texts(train.header)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "tokenizer_header.fit_on_texts(test.header)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "train['sequences_text'] = tokenizer.texts_to_sequences(train['texts'])\r\n",
    "train['sequences_header'] = tokenizer.texts_to_sequences(train['header'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "test['sequences_text'] = tokenizer.texts_to_sequences(test['texts'])\r\n",
    "test['sequences_header'] = tokenizer.texts_to_sequences(test['header'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "max_texts_length = max([len(text) for text in train.sequences_text])\r\n",
    "print('The longest review of the training set has {} words.'.format(max_texts_length))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The longest review of the training set has 12180 words.\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "max_header_length = max([len(header) for header in train.sequences_header])\n",
    "print('The longest review of the training set has {} words.'.format(max_header_length))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The longest review of the training set has 14 words.\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "NUM_AUTHORS = 500\n",
    "\n",
    "# Create tokenizer object and build vocab from the training set\n",
    "tokenizer_author = Tokenizer(NUM_AUTHORS, oov_token=1, split='\\n\\n\\n\\n')  # never split = every author is it's own token\n",
    "tokenizer_publisher = Tokenizer(NUM_AUTHORS, oov_token=1, split='\\n\\n\\n\\n')  # never split = every publisher is it's own token\n",
    "\n",
    "def tokenize_simple_column(column, tokenizer, fit):\n",
    "    if (fit):\n",
    "        tokenizer.fit_on_texts(column)\n",
    "    seq = tokenizer.texts_to_sequences(column)\n",
    "    return list(map(lambda arr: arr[0] if len(arr) == 1 else 0, seq)) # only return a single number"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "train['author_tok'] = tokenize_simple_column(train.author, tokenizer_author, True)\n",
    "train['publisher_tok'] = tokenize_simple_column(train.publisher, tokenizer_publisher, True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "test['author_tok'] = tokenize_simple_column(test.author, tokenizer_author, False)\n",
    "test['publisher_tok'] = tokenize_simple_column(test.publisher, tokenizer_publisher, False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "#NB_HIDDEN = 16\r\n",
    "EPOCH = 5\r\n",
    "BATCH_SIZE = 64 \r\n",
    "EMBEDDING_DIM = 100\r\n",
    "MAX_TEXT_LENGTH = 2000\r\n",
    "MAX_HEADER_LENGTH = 8\r\n",
    "VAL_SPLIT = 0.3\r\n",
    "#MAX_TEXT_LENGTH = 12180\r\n",
    "#MAX_HEADER_LENGTH = 14"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec\n",
    "## About Word2Vec\n",
    "Word2Vec is a rule-based way to represent words in an NLP tasks to represent them as an atomic symbol. Traditionally a binary one-hot encoding was used for such tasks. As a result, we get a huge number of zeros and ones: \n",
    "\n",
    "* Hotel [0000…1000]\n",
    "* Motel [0100…0000]\n",
    "\n",
    "One-hot encoding causes a drastically growing dimensionality and immense computational time as the algorithm complexity is getting more difficult, too. One more problem is that the similarities are not visible in this type of model and as a result the contextual meaning of the words will be lost. Model whose training is based on the vocabulary that is one-hot encoded is almost impossible to reuse in other cases. \n",
    "The way to avoid this problem is to represent words as continuous vectors, Word2Vectors. The new model architecture was proposed by Tomas Mikolov and his colleagues. This technique has a much lower computational cost which is essential if it is must learn a dataset with a huge amount of words. In the research paper the authors stated that they used this algorithm to train a model with 1.6 billion words in less than one day. In my case the amount of words is not as high and using this technique has a learning character and was mainly done to learn the architecture of the model and coding techniques. The main advantages of Word2Vec is the opportunity to train models with a huge amount of text faster and reuse the trained models in other applications. Word2Vec is able to detect the similarities and antipodes between words and even calculates a degree of this. Generally speaking, we develop a neural network with a single hidden layer which aims to predict a target word based on its context through other words, in fact its neighbouring words. The scientists have developed two models’ architectures: CBOW and Skip-gram (Mikolov et al., 2013). \n",
    "CBOW architecture is a continuous bag of words model. The algorithm in this model tries to predict a target word by using a context words, neighbouring words of the target word. \n",
    "![W2V](W2V.png \"by Kavita Ganesan\")\n",
    "The Skip-gram model works reverse to CBOW model. In this model the input is a target word and the context words are an output.  \n",
    "CBOW model trains faster than the skip-gram. This model is more suitable for frequently occurring words. Skip-gram in comparison makes its computations slower but works better if the amount of data is smaller. On the other hand, this model type is less efficient for less frequently occurring words (e.g. Vyas 2020)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "from gensim.models import Word2Vec \r\n",
    "import multiprocessing\r\n",
    "from time import time\r\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementaion\n",
    "The implementation of the word2vec model consists of the following steps: create a vocabulary from texts, create a model, create the model’s architecture, train the model, get embeddings for the model (Megret 2018). I used the library “Gensim” for the model creation. The reason to use genism is simply because personally for me this library seems pretty straightforward to use. \n",
    "Before implementing a word2vec model I decided to discover common phrases, called bigrams, in my corpora, also by using the gensim library. The development of bigrams can follow to a very sparse corpus. To avoid this problem the decision was made a to add the parameter “min_count = 30”. This causes bigrams whose total absolute frequency lower than 30 to be ignored. \n",
    "Word2vec development is based on the tutorial of Pierre Megret."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "sent = [row for row in train['texts']]\r\n",
    "phrases = Phrases(sent, min_count=30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "bigram = Phraser(phrases)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "texts_bigram = bigram[sent]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "print(multiprocessing.cpu_count())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CBOW"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "t = time()\r\n",
    "w2v_model = Word2Vec(\r\n",
    "    min_count=20,\r\n",
    "    window=7,\r\n",
    "    size=100,\r\n",
    "    sample=6e-5, \r\n",
    "    alpha=0.03, \r\n",
    "    min_alpha=0.0007, \r\n",
    "    negative=20,\r\n",
    "    sg=0,\r\n",
    "    workers=7)\r\n",
    "print('Time to build W2V: {} mins'.format(round((time() - t) / 60, 2)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time to build W2V: 0.0 mins\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Skip-gram"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "skip_w2v_model = Word2Vec(\r\n",
    "    min_count=30,\r\n",
    "    window=5,\r\n",
    "    size=100,\r\n",
    "    sample=6e-5, \r\n",
    "    alpha=0.03, \r\n",
    "    min_alpha=0.0007, \r\n",
    "    negative=30,\r\n",
    "    sg=1,\r\n",
    "    workers=7)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Building the vocabulary table"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "w2v_model.build_vocab(texts_bigram)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "t = time()\n",
    "\n",
    "skip_w2v_model.build_vocab(texts_bigram)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time to build vocab: 1.56 mins\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the model\n",
    "Determine best suitable parameters for training the model is an important step (Megret 2018).\n",
    "To train CBOW I set a min-count of 20 and in case of Skip-gram to 30. The window size in the first case is 7, because it is not recommended to have a very small window when using CBOW according to Mikolov and his co-authors. At the same time in Skip-gram case the window size is set to 5. This decision is also based on Mikolov and his colleague’s recommendation. In both cases the dimensionality size is 100. The learning rate is 0.03. Sample parameter which controls how much subsampling occurs is very small because smaller values mean words are less likely to be kept. The negative sample is set on 20 and 30. In the paper of Mikolov et al. it is recommended to set this parameter between 5-20 for small data sets to works well. The number of epochs is 30.\n",
    "During my smoke test I noticed that the CBOW model appears to give better results and found contextually better words. Tuning the parameters could provide better results, for example a negative sampling of 20 in the skip-gram instead of 30 or an increase of the number of epochs. However, my corpora is not very big that is why I did not see potential for a big improvement. \n",
    "Therefore, the CBOW model is saved and will be used as a pretrained embedding later. Additionally, we can also use an embedding layer in a network to train the embeddings as a number of researches has shown that using a pre-trained word embeddings improves a model’s performance significantly (Lample  et  al., 2016;  Ma/Hovy,  2016)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "#CBOW\n",
    "t = time()\n",
    "\n",
    "w2v_model.train(texts_bigram, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time to train the model: 49.58 mins\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "w2v_model.wv.most_similar(positive=[\"data\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('collect', 0.7679224610328674),\n",
       " ('raw', 0.7209828495979309),\n",
       " ('structure_unstructured', 0.7120325565338135),\n",
       " ('large_amount', 0.6950662136077881),\n",
       " ('analyze', 0.6918653845787048),\n",
       " ('collection', 0.6613565683364868),\n",
       " ('disparate_source', 0.660919189453125),\n",
       " ('derive_insight', 0.6601530909538269),\n",
       " ('large_volume', 0.6494994759559631),\n",
       " ('massive_amount', 0.6479994058609009)]"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "w2v_model.wv.most_similar('code')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('line_code', 0.7176811695098877),\n",
       " ('code_snippet', 0.7047280073165894),\n",
       " ('code_github', 0.7031026482582092),\n",
       " ('script', 0.695374608039856),\n",
       " ('repo', 0.6953631639480591),\n",
       " ('docstrings', 0.687702476978302),\n",
       " ('codebase', 0.6834437847137451),\n",
       " ('gist', 0.6834126114845276),\n",
       " ('python_script', 0.6800730228424072),\n",
       " ('github_repo', 0.675747811794281)]"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "#Skip-gram\r\n",
    "t = time()\r\n",
    "\r\n",
    "skip_w2v_model.train(texts_bigram, total_examples=skip_w2v_model.corpus_count, epochs=30, report_delay=1)\r\n",
    "\r\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time to train the model: 113.37 mins\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "skip_w2v_model.wv.most_similar(positive=[\"data\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('raw', 0.7005041837692261),\n",
       " ('datasets', 0.6964728832244873),\n",
       " ('set', 0.6769678592681885),\n",
       " ('collect', 0.6662955284118652),\n",
       " ('big', 0.6536503434181213),\n",
       " ('wrangle', 0.6514309644699097),\n",
       " ('analytics', 0.6496230363845825),\n",
       " ('source', 0.6456195116043091),\n",
       " ('dataset', 0.6441148519515991),\n",
       " ('large_datasets', 0.636734664440155)]"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "skip_w2v_model.wv.most_similar('code')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('available_github', 0.7703068256378174),\n",
       " ('gist', 0.7674460411071777),\n",
       " ('code_github', 0.7659567594528198),\n",
       " ('repo', 0.762941837310791),\n",
       " ('github_repo', 0.7538014650344849),\n",
       " ('github', 0.7494617700576782),\n",
       " ('code_snippet', 0.7333778142929077),\n",
       " ('github_repository', 0.7289708852767944),\n",
       " ('notebook', 0.7178595662117004),\n",
       " ('python_script', 0.7177121639251709)]"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "skip_w2v_model.wv.most_similar('polite')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('rude', 0.7224957942962646),\n",
       " ('respectful', 0.6968988180160522),\n",
       " ('swear', 0.6191444396972656),\n",
       " ('politeness', 0.6108940839767456),\n",
       " ('alexa_siri', 0.5898687839508057),\n",
       " ('joke', 0.5882405638694763),\n",
       " ('sorry', 0.5827313661575317),\n",
       " ('reply', 0.5819169878959656),\n",
       " ('laugh', 0.579865038394928),\n",
       " ('politely', 0.5752518177032471)]"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "w2v_model.wv.most_similar('polite')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('rude', 0.7701680660247803),\n",
       " ('politeness', 0.7059785723686218),\n",
       " ('respectful', 0.6654813289642334),\n",
       " ('courteous', 0.6562213897705078),\n",
       " ('rudeness', 0.6077378988265991),\n",
       " ('etiquette', 0.5933928489685059),\n",
       " ('snarky', 0.5905433297157288),\n",
       " ('sorry', 0.5889784693717957),\n",
       " ('considerate', 0.5858083963394165),\n",
       " ('annoyed', 0.5780697464942932)]"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "embedding=\"w2v_CBOW.model\"\r\n",
    "SAVE_BIN = False\r\n",
    "w2v_model.wv.save_word2vec_format(embedding, binary=SAVE_BIN)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from gensim.models import KeyedVectors\n",
    "embedding=\"w2v_CBOW.model\"\n",
    "SAVE_BIN = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Load model from disk\n",
    "model_w2v = KeyedVectors.load_word2vec_format(embedding, binary=SAVE_BIN)\n",
    "model_w2v['code']  # get one embedding to show that loading worked"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.69326967,  2.801525  , -3.3704855 , -0.13535368,  0.19786778,\n",
       "       -2.1539948 , -1.2386867 ,  0.314623  , -1.3124833 , -0.9608743 ,\n",
       "       -0.12792991, -3.139249  , -0.67779756,  3.107435  , -2.028022  ,\n",
       "       -2.259462  , -0.87324476, -5.5098095 ,  1.3169962 ,  1.0432471 ,\n",
       "        0.82132596, -1.1575875 ,  2.1379461 , -0.8487015 , -0.15197147,\n",
       "       -0.07523512, -0.17044109, -0.50479656, -1.0367959 , -3.4859724 ,\n",
       "        0.44297886,  0.91962296, -0.14671884,  0.25288174,  0.44186914,\n",
       "       -0.76035714,  2.6626208 ,  0.26611885, -2.3631518 , -0.58565396,\n",
       "       -0.15573691, -0.5707565 ,  2.845456  , -0.44194275, -3.5254438 ,\n",
       "       -0.30945483, -3.498564  , -1.487518  , -1.5231005 , -2.9451795 ,\n",
       "        2.53046   ,  2.3774047 , -1.8743931 , -2.723858  ,  0.9297845 ,\n",
       "       -0.31513575,  1.2645259 , -1.7860198 ,  1.9045573 ,  0.39395958,\n",
       "       -1.8526822 , -4.006216  ,  0.7152482 , -0.87371045,  2.1261194 ,\n",
       "        3.27963   , -2.5488276 , -2.248979  ,  0.65028185,  2.3912318 ,\n",
       "        0.8382937 ,  3.9598763 ,  0.9938623 ,  0.53167295,  0.13114052,\n",
       "        1.0136986 ,  0.7842258 , -1.1815416 , -0.10269419,  0.23446427,\n",
       "        3.6141458 ,  1.4564234 ,  0.0541147 ,  2.008927  ,  0.62142795,\n",
       "       -0.70317745,  2.0079327 ,  0.3362387 ,  0.16921975, -0.74080473,\n",
       "       -1.5003732 ,  2.768535  ,  0.44196552, -3.3829346 ,  2.7519734 ,\n",
       "       -1.8797123 , -1.5837169 ,  0.12294021,  2.0443892 , -1.4881305 ],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a validation set\n",
    "Split the data into train and validation set is a technique for evaluating the performance of a developed model. In this case it was chosen to use a hold-out method to split the data, which separates 30 percent of the train data in a validation set while 70 percent stays in the train set. Train dataset is used to fit the model and validation set is used to evaluate the model performance. Generally, a Cross-Validation algorithms performs better but because I have only limited computational power on my available hardware, I choose the faster option (e.g. Schneider 1997).\n",
    "To implement it in Python I used the train_test_split function from the scikit-learn Python machine learning library. Then I used the pad_sequences function to create an equal length for the texts and header columns. This step makes sure that when I will eventually run the neural network, the number of observations in the array will be identical. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "#MAX_TEXT_LENGTH = 2000\n",
    "#MAX_HEADER_LENGTH = 8\n",
    "#VAL_SPLIT = 0.3\n",
    "#MAX_TEXT_LENGTH = 12180 as it is\n",
    "#MAX_HEADER_LENGTH = 14 as it is"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def data():\n",
    "  \n",
    "  X = train[[col for col in train.columns if not col == 'claps']]\n",
    "  y = train['claps'].values\n",
    "  #y = np.reshape(y, (-1,1))\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n",
    "    \n",
    "  #train data \n",
    "  X_train_text = pad_sequences(X_train.sequences_text, maxlen=MAX_TEXT_LENGTH, padding='post')\n",
    "  X_train_header = pad_sequences(X_train.sequences_header, maxlen=MAX_HEADER_LENGTH, padding='post')\n",
    "\n",
    "  #test data slices\n",
    "  X_test_text = pad_sequences(X_test.sequences_text, maxlen=MAX_TEXT_LENGTH, padding='post')\n",
    "  X_test_header = pad_sequences(X_test.sequences_header, maxlen=MAX_HEADER_LENGTH, padding='post')\n",
    "\n",
    "  X_train_data = X_train[[col for col in X_train.columns if col not in ['header', 'texts', 'sequences_text', 'sequences_header']]]\n",
    "  X_test_data = X_test[[col for col in X_test.columns if col not in ['header', 'texts', 'sequences_text', 'sequences_header']]]\n",
    "\n",
    "  return X_train_text, X_train_header, X_train_data, y_train, X_test_text, X_test_header, X_test_data, y_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "X_train_text, X_train_header, X_train_data, y_train, X_test_text, X_test_header, X_test_data, y_test = data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "X_test_data.author.head(2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "67743    Ganes Kesari\n",
       "7526         UC Today\n",
       "Name: author, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "NUM_AUTHORS = 500\n",
    "def tokenize_simple_column(column, max_tokens):\n",
    "    # Create tokenizer object and build vocab from the training set\n",
    "    tokenizer_au = Tokenizer(NUM_AUTHORS, oov_token=1, split='\\n\\n\\n\\n') \n",
    "    tokenizer_au.fit_on_texts(column)\n",
    "    seq = tokenizer_au.texts_to_sequences(column)\n",
    "    return list(map(lambda arr: arr[0] if len(arr) == 1 else 0, seq))\n",
    "\n",
    "X_train_data['author_tok'] = tokenize_simple_column(X_train_data.author, NUM_AUTHORS)\n",
    "X_test_data['author_tok'] = tokenize_simple_column(X_test_data.author, NUM_AUTHORS)\n",
    "X_train_data['publisher_tok'] = tokenize_simple_column(X_train_data.publisher, NUM_AUTHORS)\n",
    "X_test_data['publisher_tok'] = tokenize_simple_column(X_test_data.publisher, NUM_AUTHORS)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "test['publisher_tok'] = tokenize_simple_column(test.publisher, NUM_AUTHORS)\n",
    "test['author_tok'] = tokenize_simple_column(test.publisher, NUM_AUTHORS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "X_train_data = X_train_data.drop(['author', 'publisher'], axis=1) \n",
    "X_test_data = X_test_data.drop(['author', 'publisher'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "test = test.drop(['author', 'publisher'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "test.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 514 entries, 0 to 513\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   responses         514 non-null    int32  \n",
      " 1   header            514 non-null    object \n",
      " 2   texts             514 non-null    object \n",
      " 3   month             514 non-null    int64  \n",
      " 4   online_since      514 non-null    int64  \n",
      " 5   read_time         514 non-null    float64\n",
      " 6   sequences_text    514 non-null    object \n",
      " 7   sequences_header  514 non-null    object \n",
      " 8   publisher_tok     514 non-null    int64  \n",
      " 9   author_tok        514 non-null    int64  \n",
      "dtypes: float64(1), int32(1), int64(4), object(4)\n",
      "memory usage: 38.3+ KB\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "with open('C:\\\\ADAMS_tutorials\\\\ADAMS_Assignment\\\\test_dataset.pkl','wb') as path_name:\r\n",
    "    pickle.dump(test, path_name) #save the test data before splitting and assignment"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Develop predictive models \n",
    "To solve an NLP task on a sequential data type it is possible to use the LSTM (Long short-term memory) or GRU (gated recurrent unit) model. The main advantage of GRU is that it works faster also in some cases it might even have a better performance than LSTM. A GRU cell graphically looks as following:![GRU](GRU.png \"by Vasilev et.el \")\n",
    "\n",
    "In the picture the ht is a single hidden state, z<sub>t</sub> and r<sub>t</sub> are the two gates and z<sub>t</sub> represents an update gate. This gate makes the decision which new information will be included and what old information will be dropped. This decision is based on the input information and the previous cell’s hidden state x<sub>t</sub> and h<sub>h-1</sub>: z<sub>t</sub> = σ(W<sub>z</sub>x<sub>t</sub> + U<sub>z</sub>h<sub>t-1</sub>).\n",
    "The reset gate decides the amount of a previous states that will be passed through: r<sub>t</sub> = σ(W<sub>r</sub>x<sub>t</sub> + U<sub>r</sub>h<sub>t-1</sub>). \n",
    "The h’<sub>t</sub> is the candidate state which is calculated as following: h’<sub>t</sub> = than(W<sub>xt</sub> + U(r<sub>t</sub>*h<sub>t-1</sub>)).\n",
    "\n",
    "These calculations bring us to the output equation h<sub>t</sub> = (1-z<sub>t</sub>)*h<sub>t-1</sub> ⊙ z<sub>t</sub>*h’<sub>t</sub> . As we can see the output at a time t is “a linear interpolation between the previous output h<sub>t-1</sub>\n",
    "and the candidate output” (Vasilev et.el 2016: 213-214)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense, Embedding,GRU, Dropout\r\n",
    "from keras.layers.embeddings import Embedding\r\n",
    "from keras.initializers import Constant\r\n",
    "from keras.optimizers import RMSprop\r\n",
    "from keras.layers import Input"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sequential benchmark model\n",
    "For my models creation I use the Keras library. Keras is a popular open-source neural network library, this library allows us to build, train and evaluate neural networks. In addition, we can organise and design layers of a neural network by using it. There are two ways to create a neural network: sequential and functional. A sequential type of neural network’s has a straightforward logic behind: In the first line we create a sequential model and then step by step adding further layers, like embedding layer or dense layer. At the end we summarise the model results. Functional API model on the other hand connects directly input and output layers by passing function to the next layer. This type of modelling is more flexible and allows us to create deeper models with multiple inputs and outputs. \n",
    "Considering this information, I developed my first neural network in sequential way with only one input “texts”.  I started by creating an embedding layer. Embedding layer in Keras convert tokenized words into vectors. The vectors should have the same size that is why we set a maximal length. The maximal length of the text is the vector size. Another parameter is the vocabulary size, which in my case it was set to 5000 words for most models but later changed to 6000. I tried to use more words (12000) but as a result the computational time grew drastically. The output_dim is the dimension of dense embeddings. In my first model it was set to 100. Because my vocabulary size is not very big a small dimension size was chosen. The Embedding layer aim to optimize its weights. As a result, we should get the best word embeddings that will generate a minimum loss.\n",
    "The next layer is the actual neural network which uses Kera’s GRU layer. I need to supply the number of hidden neurons for the network. There is no rule about the best number or how to calculate it. It is recommended to try different variations. As a result, it was decided to put 16 neurons to save computational power and because there is no strongly recommended rule about this number.\n",
    "The last layer is a dense layer that connects the neural network to a one-dimensional output by concentrating its output. It means that each input node is connected to each output node.\n",
    "At last I use compile to combine all layers to a model with loss and optimizer function. As the loss function I chose mean squared error. MSE is used to calculate regression for regression tasks. “Mean squared error is calculated as the average of the squared differences between the predicted and actual values. The result is always positive regardless of the sign of the predicted and actual values and a perfect value is 0.0.”(Brownlee 2019). Firstly, mean squared error was chosen because we deal with a numeric output in other words, we need a loss functions for regression. We are calculating the square of our error and then take it’s mean by using the MSE. This type or loss function “gives relatively higher weight (penalty) to large errors/outliers, while smoothening the gradient for smaller errors” (Hirekerur 2020). \n",
    "When training the model on my dataset I used only 5 epochs because my maximal length for the article text was 2000 words, the number of words was set to 5000 and I am training my model on almost 50 thousand words. The parameter batch size was set to 100. Larger batch sizes result in faster training of a model, smaller batch size trains slower (Brownlee 2019). The algorithm takes the first 100 rows from the training set and trains our model. Then it takes the next 100 samples and trains it. The algorithm repeats this again and again. \n",
    "The results of a benchmark model gave us on validation set the lowest MSE: 6588456.0000. During all five epochs we can see the trend to decrease of MSE.\n",
    "Even this relatively simple model took a surprisingly long to train but gave me important insight on which parameter I needed to tune in my next approaches. \n",
    "The code of the models' architectures is based on ADAM's tutorials and also on the blopgpost of Usman Malik."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "embedding_layer=Embedding(input_dim=NUM_WORDS, \n",
    "                          output_dim=EMBEDDING_DIM, \n",
    "                          input_length=MAX_TEXT_LENGTH\n",
    "                         )\n",
    "model1=Sequential()                        \n",
    "model1.add(embedding_layer)\n",
    "model1.add(GRU(NB_HIDDEN))\n",
    "model1.add(Dense(1, activation=\"relu\"))\n",
    "model1.compile(loss=\"mean_squared_error\", optimizer=RMSprop(clipvalue=1, clipnorm=1), metrics=[\"mse\"])\n",
    "model1.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2000, 100)         500000    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 16)                5616      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 505,633\n",
      "Trainable params: 505,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "model1_story = model1.fit(X_train_text, y_train, batch_size=BATCH_SIZE, epochs=EPOCH, validation_data=(X_test_text, y_test))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 46464 samples, validate on 19914 samples\n",
      "Epoch 1/5\n",
      "46464/46464 [==============================] - 1137s 24ms/step - loss: 2319150.7190 - mse: 2319149.7500 - val_loss: 6603751.6316 - val_mse: 6603749.0000\n",
      "Epoch 2/5\n",
      "46464/46464 [==============================] - 1095s 24ms/step - loss: 2314472.1168 - mse: 2314472.2500 - val_loss: 6598629.3819 - val_mse: 6598629.5000\n",
      "Epoch 3/5\n",
      "46464/46464 [==============================] - 1092s 24ms/step - loss: 2310674.1081 - mse: 2310671.2500 - val_loss: 6594342.3517 - val_mse: 6594343.0000\n",
      "Epoch 4/5\n",
      "46464/46464 [==============================] - 1097s 24ms/step - loss: 2307448.3870 - mse: 2307449.0000 - val_loss: 6590958.3334 - val_mse: 6590959.0000\n",
      "Epoch 5/5\n",
      "46464/46464 [==============================] - 1056s 23ms/step - loss: 2305102.3031 - mse: 2305102.7500 - val_loss: 6588455.9613 - val_mse: 6588456.0000\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#SCORE_BAG = {}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "to_disk = (model1, model1_story)\r\n",
    "with open('model1.pkl','wb') as file_name:\r\n",
    "    pickle.dump(to_disk, file_name)#"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Load the model \r\n",
    "#with open('model1.pkl','rb') as file_name:\r\n",
    "    #model1, model1_story = pickle.load(file_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "from gensim.models import KeyedVectors\r\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "text_vocab = KeyedVectors.load('C:\\\\ADAMS_tutorials\\\\ADAMS_Assignment\\\\w2v_model.model')\r\n",
    "print('Loaded pre-trained embeddings for {} words.'.format(len(text_vocab.wv.vocab)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded pre-trained embeddings for 36942 words.\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "from keras.layers import Input, Dense, CuDNNLSTM, CuDNNGRU, LSTM, GRU, GlobalAveragePooling1D, Embedding, Dropout, Concatenate"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "from keras.models import Model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Second model \n",
    "For the second model I wanted to include one more feature primary to learn to use multiple inputs. This was also developed in Keras but this time in a functional way. In this model I added the two inputs texts and header. I also add a pre-trained Word2Vec embedding layer for the text input, which I created early using the CBOW algorithm. The number of trained parameters was increase as a result. All other parameters stayed the same compared to the first model. The MSE results are improved in comparison with the first model. But the improvement is small. \n",
    "Develop a function that helps to incorporate a pre-trained embadding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def get_embedding_matrix(tokenizer, pretrain, vocab_size):\r\n",
    "    dim = 0\r\n",
    "    if isinstance(pretrain, KeyedVectors) or isinstance(pretrain, Word2VecKeyedVectors):\r\n",
    "        dim = pretrain.vector_size        \r\n",
    "    elif isinstance(pretrain, dict):\r\n",
    "        dim = next(iter(pretrain.values())).shape[0]  \r\n",
    "    else:\r\n",
    "        raise Exception('{} is not supported'.format(type(pretrain)))    \r\n",
    "\r\n",
    "    emb_mat = np.zeros((vocab_size, dim))\r\n",
    "    oov_words = []\r\n",
    "\r\n",
    "    for word, i in tokenizer.word_index.items():  \r\n",
    "        try:\r\n",
    "            emb_mat[i] = pretrain[word]\r\n",
    "        except:\r\n",
    "            oov_words.append(word)\r\n",
    "    print('Created embedding matrix of shape {}'.format(emb_mat.shape))\r\n",
    "    print('Encountered {} out-of-vocabulary words.'.format(len(oov_words)))\r\n",
    "    return (emb_mat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "w2v_weights = get_embedding_matrix(tokenizer, model_w2v, NUM_WORDS)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created embedding matrix of shape (5000, 100)\n",
      "Encountered 32267 out-of-vocabulary words.\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "input_tensor_text = Input(shape=(MAX_TEXT_LENGTH, ), name=\"text\")\n",
    "input_tensor_header = Input(shape=(MAX_HEADER_LENGTH, ), name=\"header\")\n",
    "\n",
    "embedding_layer=Embedding(input_dim=NUM_WORDS,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    embeddings_initializer=Constant(w2v_weights), #weights to start with, and not nouch during training\n",
    "    input_length=MAX_TEXT_LENGTH,\n",
    "    trainable=False\n",
    "    )(input_tensor_text)\n",
    "\n",
    "GRU1 = GRU(100)(embedding_layer)\n",
    "\n",
    "dense_layer1 = Dense(10, activation='relu')(input_tensor_header)\n",
    "dense_layer2 = Dense(10, activation='relu')(dense_layer1)\n",
    "\n",
    "concat_layer = Concatenate()([GRU1, dense_layer2])\n",
    "\n",
    "dense_layer3 = Dense(10, activation='relu')(concat_layer)\n",
    "\n",
    "output_tensor = Dense(1, activation='relu')(dense_layer3)\n",
    "\n",
    "model = Model(inputs = [input_tensor_text, input_tensor_header], outputs = output_tensor)\n",
    "model.compile(loss = 'mean_squared_error',\n",
    "                optimizer = RMSprop(clipvalue=1, clipnorm=1),\n",
    "                metrics=['mse'])\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               (None, 2000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "header (InputLayer)             (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 2000, 100)    500000      text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           90          header[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 100)          60300       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 10)           110         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 110)          0           gru_2[0][0]                      \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 10)           1110        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            11          dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 561,621\n",
      "Trainable params: 61,621\n",
      "Non-trainable params: 500,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "model = model.fit(\n",
    "    x=[X_train_text, X_train_header],\n",
    "    y=np.reshape(y_train, (-1,1)),\n",
    "    batch_size=100,\n",
    "    epochs=3,\n",
    "    validation_data=([X_test_text, X_test_header], np.reshape(y_test, (-1,1))))\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 46464 samples, validate on 19914 samples\n",
      "Epoch 1/3\n",
      "46464/46464 [==============================] - 4512s 97ms/step - loss: 2306939.7335 - mse: 2306940.0000 - val_loss: 6587806.8489 - val_mse: 6587806.0000\n",
      "Epoch 2/3\n",
      "46464/46464 [==============================] - 4906s 106ms/step - loss: 2301071.2121 - mse: 2301070.7500 - val_loss: 6583131.4522 - val_mse: 6583132.5000\n",
      "Epoch 3/3\n",
      "46464/46464 [==============================] - 4807s 103ms/step - loss: 2298781.3573 - mse: 2298781.5000 - val_loss: 6581357.1541 - val_mse: 6581356.5000\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Third model\n",
    "For the second model my focus was to include all available features not just the text content.\n",
    "Testing the architecture of a second model has shown some improvement. This improvement inspired me to add one more input – numeric input. It means I have three inputs, two embedding layers. I added also a pre-embedding CBOW layer to the header too. I also trained the model 10 epochs instead of 3. Other parameters are the same as in the previous model. The results are the best. The MSE is 4697665.30. \n",
    "In the last two models I used RMSprop optimizer. This optimizer is “keep the moving average of the squared gradients for each weight” (Bushaev 2018). It means that this optimizer through the average calculation of squared gradients normalizes the gradient. As a result the step size is balanced, it means that we avoid the explosion of the gradient’s step or we avoiding the vanishing if the step is small. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "input_tensor_text = Input(shape=(MAX_TEXT_LENGTH, ), name=\"text\")\n",
    "input_tensor_header = Input(shape=(MAX_HEADER_LENGTH, ), name=\"header\")\n",
    "input_tensor_numeric = Input(shape = (len(X_train_data.columns), ), name=\"numeric\")\n",
    "\n",
    "embedding_layer_text=Embedding(input_dim=NUM_WORDS,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    embeddings_initializer=Constant(w2v_weights), \n",
    "    input_length=MAX_TEXT_LENGTH,\n",
    "    trainable=True\n",
    "    )(input_tensor_text)\n",
    "\n",
    "embedding_layer_header=Embedding(input_dim=NUM_WORDS,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    embeddings_initializer=Constant(w2v_weights), \n",
    "    input_length=MAX_HEADER_LENGTH,\n",
    "    trainable=True\n",
    "    )(input_tensor_header)    \n",
    "\n",
    "GRU_text = GRU(100)(embedding_layer_text)\n",
    "GRU_header = GRU(100)(embedding_layer_header)\n",
    "dense_numeric = Dense(10, activation = 'relu')(input_tensor_numeric)\n",
    "\n",
    "concat_layer = Concatenate()([GRU_text, GRU_header, dense_numeric])\n",
    "dense_layer3 = Dense(10, activation='relu')(concat_layer)\n",
    "\n",
    "output_tensor = Dense(1, activation='relu')(dense_layer3) \n",
    "\n",
    "model_2 = Model(inputs = [input_tensor_text, input_tensor_header, input_tensor_numeric], outputs = output_tensor)\n",
    "model_2.compile(loss = 'mean_squared_error',\n",
    "                optimizer = RMSprop(clipvalue=1, clipnorm=1),\n",
    "                metrics=['mse'])\n",
    "model_2.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to\n",
      "==================================================================================================\n",
      "text (InputLayer)               (None, 2000)         0\n",
      "__________________________________________________________________________________________________\n",
      "header (InputLayer)             (None, 8)            0\n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 2000, 100)    500000      text[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 8, 100)       500000      header[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "numeric (InputLayer)            (None, 6)            0\n",
      "__________________________________________________________________________________________________\n",
      "gru_5 (GRU)                     (None, 100)          60300       embedding_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "gru_6 (GRU)                     (None, 100)          60300       embedding_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           70          numeric[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 210)          0           gru_5[0][0]\n",
      "                                                                 gru_6[0][0]\n",
      "                                                                 dense_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 10)           2110        concatenate_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            11          dense_6[0][0]\n",
      "==================================================================================================\n",
      "Total params: 1,122,791\n",
      "Trainable params: 1,122,791\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "model_2_history = model_2.fit(\n",
    "    x=[X_train_text, X_train_header, X_train_data],\n",
    "    y=np.reshape(y_train, (-1,1)),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=10,\n",
    "    validation_data=([X_test_text, X_test_header, X_test_data], np.reshape(y_test, (-1,1))))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 861682.5032960/46464 [====================>.........] - ETA: 31:54 - loss: 860051.7799 - mse: 860051.2533024/46464 [====================>.........] - ETA: 31:45 - loss: 858837.1486 - mse: 858836.6233088/46464 [====================>.........] - ETA: 31:36 - loss: 857546.3577 - mse: 857545.8133152/46464 [====================>.........] - ETA: 31:27 - loss: 855984.5481 - mse: 855984.0633216/46464 [====================>.........] - ETA: 31:18 - loss: 854427.1339 - mse: 854426.6233280/46464 [====================>.........] - ETA: 31:09 - loss: 852953.2456 - mse: 852952.7533344/46464 [====================>.........] - ETA: 31:00 - loss: 851338.8509 - mse: 851338.3133408/46464 [====================>.........] - ETA: 30:51 - loss: 850194.2973 - mse: 850193.7533472/46464 [====================>.........] - ETA: 30:41 - loss: 850940.8361 - mse: 850940.3133536/46464 [====================>.........] - ETA: 30:32 - loss: 849718.2801 - mse: 849717.8133600/46464 [====================>.........] - ETA: 30:23 - loss: 848192.7719 - mse: 848192.3133664/46464 [====================>.........] - ETA: 30:14 - loss: 847726.4834 - mse: 847726.0033728/46464 [====================>.........] - ETA: 30:05 - loss: 846290.4101 - mse: 846289.9333792/46464 [====================>.........] - ETA: 29:56 - loss: 844725.1476 - mse: 844724.6833856/46464 [====================>.........] - ETA: 29:47 - loss: 843183.2269 - mse: 843182.7533920/46464 [====================>.........] - ETA: 29:38 - loss: 841679.8237 - mse: 841679.3133984/46464 [====================>.........] - ETA: 29:29 - loss: 840145.7474 - mse: 840145.2534048/46464 [====================>.........] - ETA: 29:20 - loss: 838587.9099 - mse: 838587.4334112/46464 [=====================>........] - ETA: 29:11 - loss: 837079.3118 - mse: 837078.8134176/46464 [=====================>........] - ETA: 29:02 - loss: 835543.9128 - mse: 835543.4334240/46464 [=====================>........] - ETA: 28:53 - loss: 834036.3089 - mse: 834035.8134304/46464 [=====================>........] - ETA: 28:43 - loss: 832704.4489 - mse: 832703.9334368/46464 [=====================>........] - ETA: 28:34 - loss: 831371.2188 - mse: 831370.7534432/46464 [=====================>........] - ETA: 28:25 - loss: 829910.5971 - mse: 829910.1234496/46464 [=====================>........] - ETA: 28:16 - loss: 828440.2989 - mse: 828439.8134560/46464 [=====================>........] - ETA: 28:07 - loss: 826961.9814 - mse: 826961.5034624/46464 [=====================>........] - ETA: 27:58 - loss: 825450.2062 - mse: 825449.6834688/46464 [=====================>........] - ETA: 27:49 - loss: 826078.9984 - mse: 826078.4334752/46464 [=====================>........] - ETA: 27:40 - loss: 824589.7021 - mse: 824589.1234816/46464 [=====================>........] - ETA: 27:31 - loss: 823225.3605 - mse: 823224.8134880/46464 [=====================>........] - ETA: 27:22 - loss: 821848.6616 - mse: 821848.1234944/46464 [=====================>........] - ETA: 27:13 - loss: 820381.2701 - mse: 820380.6835008/46464 [=====================>........] - ETA: 27:04 - loss: 819000.3572 - mse: 818999.8135072/46464 [=====================>........] - ETA: 26:55 - loss: 817639.7967 - mse: 817639.2535136/46464 [=====================>........] - ETA: 26:45 - loss: 816500.1258 - mse: 816499.5635200/46464 [=====================>........] - ETA: 26:36 - loss: 815054.4880 - mse: 815053.9335264/46464 [=====================>........] - ETA: 26:27 - loss: 813807.8588 - mse: 813807.3135328/46464 [=====================>........] - ETA: 26:18 - loss: 812430.5304 - mse: 812430.0035392/46464 [=====================>........] - ETA: 26:09 - loss: 811139.8897 - mse: 811139.4335456/46464 [=====================>........] - ETA: 26:00 - loss: 809801.7208 - mse: 809801.2535520/46464 [=====================>........] - ETA: 25:51 - loss: 808988.3966 - mse: 808987.9335584/46464 [=====================>........] - ETA: 25:42 - loss: 807578.5571 - mse: 807578.0635648/46464 [======================>.......] - ETA: 25:33 - loss: 814027.6962 - mse: 814027.1835712/46464 [======================>.......] - ETA: 25:24 - loss: 812627.2644 - mse: 812626.7535776/46464 [======================>.......] - ETA: 25:15 - loss: 812353.5222 - mse: 812353.0035840/46464 [======================>.......] - ETA: 25:06 - loss: 810961.7209 - mse: 810961.2535904/46464 [======================>.......] - ETA: 24:57 - loss: 809541.1025 - mse: 809540.6235968/46464 [======================>.......] - ETA: 24:48 - loss: 808540.2261 - mse: 808539.7536032/46464 [======================>.......] - ETA: 24:39 - loss: 807316.3650 - mse: 807315.8736096/46464 [======================>.......] - ETA: 24:30 - loss: 818009.6445 - mse: 818009.1836160/46464 [======================>.......] - ETA: 24:20 - loss: 816649.8953 - mse: 816649.4336224/46464 [======================>.......] - ETA: 24:11 - loss: 861315.0404 - mse: 861314.6236288/46464 [======================>.......] - ETA: 24:02 - loss: 860609.6130 - mse: 860609.1836352/46464 [======================>.......] - ETA: 23:53 - loss: 859274.4967 - mse: 859274.0636416/46464 [======================>.......] - ETA: 23:44 - loss: 857815.6604 - mse: 857815.2536480/46464 [======================>.......] - ETA: 23:35 - loss: 856906.6674 - mse: 856906.1836544/46464 [======================>.......] - ETA: 23:26 - loss: 855492.7465 - mse: 855492.3136608/46464 [======================>.......] - ETA: 23:17 - loss: 854015.5865 - mse: 854015.1836672/46464 [======================>.......] - ETA: 23:09 - loss: 852550.0362 - mse: 852549.5636736/46464 [======================>.......] - ETA: 22:59 - loss: 851086.9004 - mse: 851086.4336800/46464 [======================>.......] - ETA: 22:50 - loss: 849949.1285 - mse: 849948.6836864/46464 [======================>.......] - ETA: 22:41 - loss: 848728.7315 - mse: 848728.2536928/46464 [======================>.......] - ETA: 22:32 - loss: 853677.9019 - mse: 853677.5036992/46464 [======================>.......] - ETA: 22:23 - loss: 852352.0147 - mse: 852351.6237056/46464 [======================>.......] - ETA: 22:14 - loss: 850918.2360 - mse: 850917.8737120/46464 [======================>.......] - ETA: 22:05 - loss: 849520.1988 - mse: 849519.8137184/46464 [=======================>......] - ETA: 21:56 - loss: 848158.8950 - mse: 848158.5037248/46464 [=======================>......] - ETA: 21:47 - loss: 847004.6078 - mse: 847004.1837312/46464 [=======================>......] - ETA: 21:38 - loss: 845581.7888 - mse: 845581.3737376/46464 [=======================>......] - ETA: 21:29 - loss: 844152.6973 - mse: 844152.3137440/46464 [=======================>......] - ETA: 21:20 - loss: 842741.8077 - mse: 842741.4337504/46464 [=======================>......] - ETA: 21:11 - loss: 843375.1619 - mse: 843374.8137568/46464 [=======================>......] - ETA: 21:01 - loss: 842515.2219 - mse: 842514.8737632/46464 [=======================>......] - ETA: 20:52 - loss: 841154.2949 - mse: 841153.9337696/46464 [=======================>......] - ETA: 20:43 - loss: 839758.5444 - mse: 839758.2537760/46464 [=======================>......] - ETA: 20:34 - loss: 838355.9865 - mse: 838355.6837824/46464 [=======================>......] - ETA: 20:25 - loss: 837037.0193 - mse: 837036.7537888/46464 [=======================>......] - ETA: 20:16 - loss: 837292.0239 - mse: 837291.7537952/46464 [=======================>......] - ETA: 20:07 - loss: 835926.7120 - mse: 835926.3738016/46464 [=======================>......] - ETA: 19:58 - loss: 834547.3422 - mse: 834547.0038080/46464 [=======================>......] - ETA: 19:49 - loss: 833172.3376 - mse: 833172.0038144/46464 [=======================>......] - ETA: 19:40 - loss: 843305.6357 - mse: 843305.3138208/46464 [=======================>......] - ETA: 19:31 - loss: 841926.7184 - mse: 841926.3738272/46464 [=======================>......] - ETA: 19:22 - loss: 856795.1503 - mse: 856794.8138336/46464 [=======================>......] - ETA: 19:13 - loss: 855463.8393 - mse: 855463.5038400/46464 [=======================>......] - ETA: 19:04 - loss: 854113.5184 - mse: 854113.1838464/46464 [=======================>......] - ETA: 18:55 - loss: 883160.7338 - mse: 883160.3738528/46464 [=======================>......] - ETA: 18:46 - loss: 881734.9120 - mse: 881734.5638592/46464 [=======================>......] - ETA: 18:36 - loss: 880304.3796 - mse: 880304.0038656/46464 [=======================>......] - ETA: 18:27 - loss: 879366.9391 - mse: 879366.5638720/46464 [========================>.....] - ETA: 18:18 - loss: 879520.0998 - mse: 879519.7538784/46464 [========================>.....] - ETA: 18:09 - loss: 878145.6923 - mse: 878145.3138848/46464 [========================>.....] - ETA: 18:00 - loss: 876747.7257 - mse: 876747.3138912/46464 [========================>.....] - ETA: 17:51 - loss: 875511.0365 - mse: 875510.6238976/46464 [========================>.....] - ETA: 17:42 - loss: 874988.5449 - mse: 874988.1239040/46464 [========================>.....] - ETA: 17:33 - loss: 874444.6985 - mse: 874444.2539104/46464 [========================>.....] - ETA: 17:24 - loss: 873040.8842 - mse: 873040.4339168/46464 [========================>.....] - ETA: 17:15 - loss: 871875.4817 - mse: 871875.0639232/46464 [========================>.....] - ETA: 17:06 - loss: 870471.4863 - mse: 870471.0639296/46464 [========================>.....] - ETA: 16:57 - loss: 871314.5580 - mse: 871314.1239360/46464 [========================>.....] - ETA: 16:48 - loss: 872941.9282 - mse: 872941.5039424/46464 [========================>.....] - ETA: 16:39 - loss: 871726.6123 - mse: 871726.2539488/46464 [========================>.....] - ETA: 16:30 - loss: 870422.1488 - mse: 870421.8139552/46464 [========================>.....] - ETA: 16:20 - loss: 869813.2617 - mse: 869812.9339616/46464 [========================>.....] - ETA: 16:11 - loss: 868486.7544 - mse: 868486.4339680/46464 [========================>.....] - ETA: 16:02 - loss: 867129.9403 - mse: 867129.6239744/46464 [========================>.....] - ETA: 15:53 - loss: 865777.3031 - mse: 865776.9339808/46464 [========================>.....] - ETA: 15:44 - loss: 865117.0541 - mse: 865116.6839872/46464 [========================>.....] - ETA: 15:35 - loss: 864797.8492 - mse: 864797.5039936/46464 [========================>.....] - ETA: 15:26 - loss: 863460.8248 - mse: 863460.4340000/46464 [========================>.....] - ETA: 15:17 - loss: 862416.7486 - mse: 862416.3740064/46464 [========================>.....] - ETA: 15:08 - loss: 861156.1340 - mse: 861155.8140128/46464 [========================>.....] - ETA: 14:59 - loss: 859814.7408 - mse: 859814.3740192/46464 [========================>.....] - ETA: 14:50 - loss: 858497.5013 - mse: 858497.1240256/46464 [========================>.....] - ETA: 14:41 - loss: 857788.9796 - mse: 857788.6240320/46464 [=========================>....] - ETA: 14:32 - loss: 856511.7124 - mse: 856511.3740384/46464 [=========================>....] - ETA: 14:23 - loss: 855268.1007 - mse: 855267.8140448/46464 [=========================>....] - ETA: 14:13 - loss: 853952.4593 - mse: 853952.1840512/46464 [=========================>....] - ETA: 14:04 - loss: 852620.3800 - mse: 852620.1240576/46464 [=========================>....] - ETA: 13:55 - loss: 851285.8692 - mse: 851285.6240640/46464 [=========================>....] - ETA: 13:46 - loss: 850127.9045 - mse: 850127.6240704/46464 [=========================>....] - ETA: 13:37 - loss: 848870.3765 - mse: 848870.1240768/46464 [=========================>....] - ETA: 13:28 - loss: 847621.4664 - mse: 847621.2540832/46464 [=========================>....] - ETA: 13:19 - loss: 850169.3399 - mse: 850169.0640896/46464 [=========================>....] - ETA: 13:10 - loss: 849034.1233 - mse: 849033.8140960/46464 [=========================>....] - ETA: 13:01 - loss: 861394.7528 - mse: 861394.3741024/46464 [=========================>....] - ETA: 12:52 - loss: 860067.1450 - mse: 860066.7541088/46464 [=========================>....] - ETA: 12:43 - loss: 858812.1866 - mse: 858811.8141152/46464 [=========================>....] - ETA: 12:33 - loss: 857496.8436 - mse: 857496.5041216/46464 [=========================>....] - ETA: 12:24 - loss: 856227.4508 - mse: 856227.0641280/46464 [=========================>....] - ETA: 12:15 - loss: 854954.7176 - mse: 854954.3741344/46464 [=========================>....] - ETA: 12:06 - loss: 853668.3684 - mse: 853668.0641408/46464 [=========================>....] - ETA: 11:57 - loss: 852689.1742 - mse: 852688.9341472/46464 [=========================>....] - ETA: 11:48 - loss: 851397.1415 - mse: 851396.8141536/46464 [=========================>....] - ETA: 11:39 - loss: 851050.6703 - mse: 851050.3741600/46464 [=========================>....] - ETA: 11:30 - loss: 850842.5362 - mse: 850842.3141664/46464 [=========================>....] - ETA: 11:21 - loss: 855078.3841 - mse: 855078.1241728/46464 [=========================>....] - ETA: 11:12 - loss: 853812.7864 - mse: 853812.5041792/46464 [=========================>....] - ETA: 11:03 - loss: 853078.2931 - mse: 853078.0641856/46464 [==========================>...] - ETA: 10:54 - loss: 851856.5825 - mse: 851856.3141920/46464 [==========================>...] - ETA: 10:44 - loss: 850632.6142 - mse: 850632.3741984/46464 [==========================>...] - ETA: 10:35 - loss: 849557.3896 - mse: 849557.1842048/46464 [==========================>...] - ETA: 10:26 - loss: 848470.3238 - mse: 848470.1242112/46464 [==========================>...] - ETA: 10:17 - loss: 847466.4646 - mse: 847466.1842176/46464 [==========================>...] - ETA: 10:08 - loss: 846268.2507 - mse: 846268.0042240/46464 [==========================>...] - ETA: 9:59 - loss: 845045.2996 - mse: 845045.06242304/46464 [==========================>...] - ETA: 9:50 - loss: 843849.2246 - mse: 843849.00042368/46464 [==========================>...] - ETA: 9:41 - loss: 843188.9031 - mse: 843188.68742432/46464 [==========================>...] - ETA: 9:32 - loss: 841959.9018 - mse: 841959.68742496/46464 [==========================>...] - ETA: 9:23 - loss: 840704.8038 - mse: 840704.56242560/46464 [==========================>...] - ETA: 9:14 - loss: 839604.7916 - mse: 839604.56242624/46464 [==========================>...] - ETA: 9:05 - loss: 942279.4751 - mse: 942279.18742688/46464 [==========================>...] - ETA: 8:55 - loss: 940967.4168 - mse: 940967.12542752/46464 [==========================>...] - ETA: 8:46 - loss: 940358.1141 - mse: 940357.87542816/46464 [==========================>...] - ETA: 8:37 - loss: 938984.9627 - mse: 938984.68742880/46464 [==========================>...] - ETA: 8:28 - loss: 943581.2116 - mse: 943580.93742944/46464 [==========================>...] - ETA: 8:19 - loss: 942222.8068 - mse: 942222.50043008/46464 [==========================>...] - ETA: 8:10 - loss: 941454.2517 - mse: 941453.87543072/46464 [==========================>...] - ETA: 8:01 - loss: 940097.8788 - mse: 940097.50043136/46464 [==========================>...] - ETA: 7:52 - loss: 938992.4023 - mse: 938992.06243200/46464 [==========================>...] - ETA: 7:43 - loss: 960995.8713 - mse: 960995.56243264/46464 [==========================>...] - ETA: 7:34 - loss: 959706.3004 - mse: 959705.93743328/46464 [==========================>...] - ETA: 7:25 - loss: 958347.8877 - mse: 958347.56243392/46464 [===========================>..] - ETA: 7:16 - loss: 956959.9491 - mse: 956959.62543456/46464 [===========================>..] - ETA: 7:06 - loss: 955595.1369 - mse: 955594.81243520/46464 [===========================>..] - ETA: 6:57 - loss: 955161.8311 - mse: 955161.50043584/46464 [===========================>..] - ETA: 6:48 - loss: 953881.2998 - mse: 953881.00043648/46464 [===========================>..] - ETA: 6:39 - loss: 952944.9105 - mse: 952944.62543712/46464 [===========================>..] - ETA: 6:30 - loss: 952028.6686 - mse: 952028.37543776/46464 [===========================>..] - ETA: 6:21 - loss: 950930.3914 - mse: 950930.12543840/46464 [===========================>..] - ETA: 6:12 - loss: 951231.9085 - mse: 951231.62543904/46464 [===========================>..] - ETA: 6:03 - loss: 949930.0097 - mse: 949929.68743968/46464 [===========================>..] - ETA: 5:54 - loss: 948831.3017 - mse: 948831.00044032/46464 [===========================>..] - ETA: 5:45 - loss: 947527.1183 - mse: 947526.87544096/46464 [===========================>..] - ETA: 5:36 - loss: 946190.3579 - mse: 946190.12544160/46464 [===========================>..] - ETA: 5:27 - loss: 963171.9095 - mse: 963171.68744224/46464 [===========================>..] - ETA: 5:18 - loss: 961825.7016 - mse: 961825.50044288/46464 [===========================>..] - ETA: 5:08 - loss: 960470.5585 - mse: 960470.37544352/46464 [===========================>..] - ETA: 4:59 - loss: 959127.9227 - mse: 959127.75044416/46464 [===========================>..] - ETA: 4:50 - loss: 958391.4211 - mse: 958391.25044480/46464 [===========================>..] - ETA: 4:41 - loss: 957179.7030 - mse: 957179.50044544/46464 [===========================>..] - ETA: 4:32 - loss: 955821.3458 - mse: 955821.12544608/46464 [===========================>..] - ETA: 4:23 - loss: 954475.0041 - mse: 954474.81244672/46464 [===========================>..] - ETA: 4:14 - loss: 953132.6162 - mse: 953132.37544736/46464 [===========================>..] - ETA: 4:05 - loss: 951781.4310 - mse: 951781.18744800/46464 [===========================>..] - ETA: 3:56 - loss: 950443.6465 - mse: 950443.43744864/46464 [===========================>..] - ETA: 3:47 - loss: 949110.6846 - mse: 949110.50044928/46464 [============================>.] - ETA: 3:38 - loss: 950120.7074 - mse: 950120.56244992/46464 [============================>.] - ETA: 3:28 - loss: 948803.8311 - mse: 948803.75045056/46464 [============================>.] - ETA: 3:19 - loss: 947914.3434 - mse: 947914.25045120/46464 [============================>.] - ETA: 3:10 - loss: 946660.9371 - mse: 946660.87545184/46464 [============================>.] - ETA: 3:01 - loss: 946145.1808 - mse: 946145.06245248/46464 [============================>.] - ETA: 2:52 - loss: 944916.9942 - mse: 944916.93745312/46464 [============================>.] - ETA: 2:43 - loss: 944024.3477 - mse: 944024.25045376/46464 [============================>.] - ETA: 2:34 - loss: 942779.2903 - mse: 942779.12545440/46464 [============================>.] - ETA: 2:25 - loss: 941550.4679 - mse: 941550.31245504/46464 [============================>.] - ETA: 2:16 - loss: 940330.3465 - mse: 940330.18745568/46464 [============================>.] - ETA: 2:07 - loss: 939059.3224 - mse: 939059.25045632/46464 [============================>.] - ETA: 1:58 - loss: 937800.4707 - mse: 937800.37545696/46464 [============================>.] - ETA: 1:49 - loss: 941480.4644 - mse: 941480.31245760/46464 [============================>.] - ETA: 1:39 - loss: 940823.2692 - mse: 940823.18745824/46464 [============================>.] - ETA: 1:30 - loss: 940973.5614 - mse: 940973.50045888/46464 [============================>.] - ETA: 1:21 - loss: 939698.9632 - mse: 939698.87545952/46464 [============================>.] - ETA: 1:12 - loss: 938590.8766 - mse: 938590.75046016/46464 [============================>.] - ETA: 1:03 - loss: 1572649.3816 - mse: 1572649.25046080/46464 [============================>.] - ETA: 54s - loss: 1570511.9945 - mse: 1570512.0046144/46464 [============================>.] - ETA: 45s - loss: 1568689.8671 - mse: 1568689.8746208/46464 [============================>.] - ETA: 36s - loss: 1566581.7175 - mse: 1566581.7546272/46464 [============================>.] - ETA: 27s - loss: 1564444.7301 - mse: 1564444.6246336/46464 [============================>.] - ETA: 18s - loss: 1562494.8903 - mse: 1562494.7546400/46464 [============================>.] - ETA: 9s - loss: 1560617.7467 - mse: 1560617.62546464/46464 [==============================] - 6868s 148ms/step - loss: 1558529.5294 - mse: 1558529.3750 - val_loss: 4697665.3086 - val_mse: 4697665.5000\n"
     ]
    }
   ],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "model_2.model.save(\"model_2.h5\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "validation_loss = np.amin(model_2_history.history['val_loss']) \n",
    "print('Lowest validation loss of epoch:', validation_loss)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lowest validation loss of epoch:4697665.308567183\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "model_2_history.model.save(\"model_2_history.h5\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Assignement with second model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "def split_test():\n",
    "    test_texts = test['texts']\n",
    "    test_header = test['header']\n",
    "    test_data = test[[col for col in test.columns if col not in ['header', 'texts']]]\n",
    "    \n",
    "    test_texts = pad_sequences(test.sequences_text, maxlen=MAX_TEXT_LENGTH, padding='post')\n",
    "    test_header = pad_sequences(test.sequences_header, maxlen=MAX_HEADER_LENGTH, padding='post')\n",
    "    \n",
    "    return test_texts, test_header, test_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "test_texts, test_header, test_data = split_test()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "test_data = test_data.drop(['sequences_header', 'sequences_text'], axis='columns')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "source": [
    "predictions = model_2_history.model.predict([test_texts, test_header, test_data], verbose=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32/514 [>.............................] - ETA: 13 64/514 [==>...........................] - ETA: 10 96/514 [====>.........................] - ETA:128/514 [======>.......................] - ETA:160/514 [========>.....................] - ETA:192/514 [==========>...................] - ETA:224/514 [============>.................] - ETA:256/514 [=============>................] - ETA:288/514 [===============>..............] - ETA:320/514 [=================>............] - ETA:352/514 [===================>..........] - ETA:384/514 [=====================>........] - ETA:416/514 [=======================>......] - ETA:448/514 [=========================>....] - ETA:480/514 [===========================>..] - ETA:512/514 [============================>.] - ETA:514/514 [==============================] - 7s 14ms/step\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "source": [
    "predictions[100]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 175
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "source": [
    "def getTestIndex():\r\n",
    "    test_csv = pd.read_csv('C:/ADAMS_tutorials/ADAMS_Assignment/Test.csv')\r\n",
    "    test_index = test_csv['index']\r\n",
    "    return test_index\r\n",
    "    \r\n",
    "test_index = getTestIndex()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "source": [
    "# reshape to 1-dim\r\n",
    "predictions = predictions.reshape(len(predictions), )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "source": [
    "df = pd.DataFrame({'Claps': predictions})\r\n",
    "df = df.set_index(test_index)\r\n",
    "\r\n",
    "print('DataFrame:\\n', df)\r\n",
    "\r\n",
    "# default CSV\r\n",
    "predictions_teslenko = df.to_csv()\r\n",
    "print('\\nCSV String:\\n', predictions_teslenko)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DataFrame:\n",
      "              Claps\n",
      "index              \n",
      "0      24515.074219\n",
      "1       8909.722656\n",
      "2       8612.857422\n",
      "5       5828.024902\n",
      "7       1081.209229\n",
      "...             ...\n",
      "598    10315.620117\n",
      "599     1912.135254\n",
      "600     1082.013672\n",
      "601     7907.884766\n",
      "602     2072.990479\n",
      "\n",
      "[514 rows x 1 columns]\n",
      "\n",
      "CSV String:\n",
      "index,Claps\n",
      "0,24515.074\n",
      "1,8909.723\n",
      "2,8612.857\n",
      "5,5828.025\n",
      "7,1081.2092\n",
      "8,4170.4224\n",
      "9,3766.5576\n",
      "11,1666.0199\n",
      "12,6017.356\n",
      "13,772.99786\n",
      "14,374.8884\n",
      "15,558.7997\n",
      "16,14071.78\n",
      "17,15336.468\n",
      "18,0.0\n",
      "19,2647.387\n",
      "20,895.46735\n",
      "21,1028.1501\n",
      "22,418.36618\n",
      "23,3395.9802\n",
      "25,921.56555\n",
      "26,400.0162\n",
      "27,2794.545\n",
      "28,6261.963\n",
      "29,6701.7935\n",
      "30,4540.3145\n",
      "31,6209.804\n",
      "32,3095.9604\n",
      "33,5974.814\n",
      "34,2897.896\n",
      "35,1473.134\n",
      "36,2588.4329\n",
      "37,1061.9233\n",
      "39,1150.4482\n",
      "40,2020.4186\n",
      "41,880.28033\n",
      "42,1114.1477\n",
      "43,946.53064\n",
      "44,1303.9707\n",
      "46,429.22244\n",
      "47,661.0153\n",
      "48,2172.3142\n",
      "49,665.6097\n",
      "50,0.0\n",
      "51,0.0\n",
      "52,980.8738\n",
      "53,359.71085\n",
      "54,1014.8097\n",
      "55,853.51483\n",
      "56,3830.272\n",
      "57,57.197784\n",
      "58,1308.01\n",
      "61,41.75418\n",
      "63,16.261564\n",
      "64,129.31839\n",
      "66,169.99274\n",
      "67,503.97586\n",
      "68,42.30789\n",
      "70,432.23343\n",
      "71,0.0\n",
      "72,776.155\n",
      "73,485.23026\n",
      "74,207.56851\n",
      "75,1243.095\n",
      "76,0.0\n",
      "77,824.87866\n",
      "78,0.0\n",
      "80,279.40378\n",
      "82,0.0\n",
      "85,269.04507\n",
      "86,1797.6929\n",
      "87,118.57425\n",
      "88,0.0\n",
      "91,0.0\n",
      "92,552.35754\n",
      "93,87.14169\n",
      "94,490.79214\n",
      "96,0.0\n",
      "98,0.0\n",
      "99,97.29648\n",
      "100,0.0\n",
      "101,0.0\n",
      "102,0.0\n",
      "103,154.035\n",
      "104,268.01028\n",
      "105,568.98486\n",
      "106,0.0\n",
      "107,0.0\n",
      "108,0.0\n",
      "109,123.38718\n",
      "110,12.552276\n",
      "111,0.0\n",
      "112,0.0\n",
      "113,0.0\n",
      "114,0.0\n",
      "116,250.70584\n",
      "117,332.79996\n",
      "119,306.10684\n",
      "120,0.0\n",
      "121,510.7758\n",
      "122,0.0\n",
      "123,350.92984\n",
      "124,367.07437\n",
      "125,0.0\n",
      "127,30.90695\n",
      "128,368.03043\n",
      "129,0.0\n",
      "130,0.0\n",
      "131,0.0\n",
      "133,0.0\n",
      "134,0.0\n",
      "135,0.0\n",
      "138,280.16788\n",
      "139,0.0\n",
      "140,0.0\n",
      "141,0.0\n",
      "142,378.1393\n",
      "144,0.0\n",
      "145,658.35046\n",
      "146,0.0\n",
      "147,0.0\n",
      "148,119.998566\n",
      "149,0.0\n",
      "150,0.0\n",
      "151,108.73233\n",
      "152,1257.7207\n",
      "153,803.1365\n",
      "154,0.0\n",
      "156,107.00534\n",
      "158,0.0\n",
      "160,0.0\n",
      "161,0.0\n",
      "162,0.0\n",
      "163,0.0\n",
      "164,497.9363\n",
      "165,19.688017\n",
      "166,0.0\n",
      "167,492.61185\n",
      "168,103.46564\n",
      "169,0.0\n",
      "171,0.0\n",
      "172,312.53854\n",
      "173,0.0\n",
      "174,0.0\n",
      "175,0.0\n",
      "176,174.13736\n",
      "177,0.0\n",
      "178,424.82108\n",
      "179,0.0\n",
      "181,0.0\n",
      "182,154.96402\n",
      "183,0.0\n",
      "184,0.0\n",
      "185,125.98123\n",
      "186,0.0\n",
      "187,0.0\n",
      "188,0.0\n",
      "189,861.3496\n",
      "191,0.0\n",
      "192,0.0\n",
      "193,269.94193\n",
      "195,0.0\n",
      "196,0.0\n",
      "197,0.0\n",
      "198,0.0\n",
      "199,153.45206\n",
      "200,667.0126\n",
      "201,0.0\n",
      "203,0.0\n",
      "204,0.0\n",
      "205,0.0\n",
      "207,992.51776\n",
      "208,341.78598\n",
      "209,360.74136\n",
      "210,0.0\n",
      "212,0.0\n",
      "213,0.0\n",
      "214,0.0\n",
      "216,61.13559\n",
      "217,0.0\n",
      "218,0.0\n",
      "219,959.19086\n",
      "220,336.92923\n",
      "221,0.0\n",
      "222,137.00772\n",
      "226,268.0933\n",
      "227,102.89029\n",
      "230,0.0\n",
      "231,0.0\n",
      "233,161.07706\n",
      "235,734.4302\n",
      "236,0.0\n",
      "237,0.0\n",
      "240,242.61844\n",
      "241,0.0\n",
      "242,453.1182\n",
      "243,2562.2239\n",
      "245,0.0\n",
      "246,408.76712\n",
      "247,536.51904\n",
      "248,0.0\n",
      "249,0.0\n",
      "250,133.50455\n",
      "251,226.70145\n",
      "252,121.11441\n",
      "253,0.0\n",
      "254,0.0\n",
      "257,178.68985\n",
      "258,0.0\n",
      "259,561.822\n",
      "261,0.0\n",
      "262,439.0508\n",
      "265,663.84875\n",
      "266,1185.7395\n",
      "267,542.7273\n",
      "268,0.0\n",
      "269,350.47025\n",
      "270,0.0\n",
      "272,0.0\n",
      "273,367.47025\n",
      "274,128.1528\n",
      "276,104.38394\n",
      "278,0.0\n",
      "279,0.0\n",
      "281,0.0\n",
      "282,97.605255\n",
      "283,0.0\n",
      "284,0.0\n",
      "285,246.11685\n",
      "287,750.5521\n",
      "288,179.6293\n",
      "289,212.60452\n",
      "290,207.07938\n",
      "291,134.55057\n",
      "292,0.0\n",
      "294,468.8096\n",
      "295,28694.172\n",
      "297,1823.0093\n",
      "298,9724.37\n",
      "300,2371.5312\n",
      "303,611.68555\n",
      "305,737.52484\n",
      "306,10044.929\n",
      "307,96.91092\n",
      "309,947.9099\n",
      "314,69.94809\n",
      "315,744.0391\n",
      "316,1616.3195\n",
      "318,1080.5066\n",
      "321,10036.884\n",
      "322,0.0\n",
      "324,3134.8408\n",
      "326,1042.7361\n",
      "329,1022.5952\n",
      "333,4000.3506\n",
      "335,0.0\n",
      "337,1299.0094\n",
      "340,719.54333\n",
      "342,750.49945\n",
      "343,1124.7913\n",
      "344,1797.0288\n",
      "345,157.61636\n",
      "346,4386.811\n",
      "348,867.2312\n",
      "349,629.30786\n",
      "350,142.7482\n",
      "351,1030.6857\n",
      "352,624.10706\n",
      "353,232.70407\n",
      "354,399.736\n",
      "355,0.0\n",
      "356,2178.8577\n",
      "357,0.0\n",
      "358,0.0\n",
      "359,0.0\n",
      "360,126.04288\n",
      "361,0.0\n",
      "362,0.0\n",
      "363,2537.276\n",
      "364,2411.9443\n",
      "365,88.76077\n",
      "366,508.93338\n",
      "367,805.9336\n",
      "368,0.0\n",
      "369,0.0\n",
      "370,1452.0779\n",
      "371,71.781525\n",
      "372,2186.4768\n",
      "373,951.38477\n",
      "374,0.0\n",
      "375,586.90625\n",
      "376,1079.5914\n",
      "377,379.29434\n",
      "378,435.1758\n",
      "379,1007.69214\n",
      "380,0.0\n",
      "381,2724.6394\n",
      "382,0.0\n",
      "383,2159.878\n",
      "384,143.23404\n",
      "385,266.57327\n",
      "386,1019.2745\n",
      "387,0.0\n",
      "388,0.0\n",
      "389,571.4599\n",
      "390,0.0\n",
      "391,0.0\n",
      "392,1246.0017\n",
      "393,0.0\n",
      "394,319.22928\n",
      "395,33.73355\n",
      "396,816.9114\n",
      "397,992.0338\n",
      "398,921.20776\n",
      "399,0.0\n",
      "400,101.390656\n",
      "401,227.98062\n",
      "403,0.0\n",
      "404,1713.7927\n",
      "405,3835.4253\n",
      "406,0.0\n",
      "407,349.6528\n",
      "408,553.855\n",
      "409,1184.9725\n",
      "410,70.257965\n",
      "411,0.0\n",
      "412,0.0\n",
      "413,409.9778\n",
      "414,2180.379\n",
      "415,650.5676\n",
      "416,264.705\n",
      "417,468.8096\n",
      "418,5812.1177\n",
      "419,1826.2087\n",
      "420,1374.0188\n",
      "421,2904.8164\n",
      "422,2291.2517\n",
      "423,630.7149\n",
      "424,89.09976\n",
      "425,281.1275\n",
      "426,513.5823\n",
      "428,1202.5759\n",
      "429,704.12274\n",
      "430,955.1835\n",
      "431,157.61636\n",
      "432,4386.811\n",
      "433,867.2312\n",
      "434,629.30786\n",
      "435,142.7482\n",
      "436,1030.6857\n",
      "437,624.1068\n",
      "438,232.70395\n",
      "439,399.736\n",
      "440,0.0\n",
      "441,126.04288\n",
      "442,84.459625\n",
      "443,276.1888\n",
      "444,2537.276\n",
      "445,2411.9443\n",
      "446,88.76077\n",
      "447,508.93338\n",
      "448,805.9336\n",
      "449,965.32654\n",
      "450,1095.5677\n",
      "451,781.4261\n",
      "452,0.0\n",
      "453,8611.851\n",
      "454,3496.0547\n",
      "455,2460.4097\n",
      "456,2983.055\n",
      "457,0.0\n",
      "458,951.38477\n",
      "459,1019.2745\n",
      "460,0.0\n",
      "461,0.0\n",
      "462,0.0\n",
      "463,1819.3176\n",
      "464,816.81885\n",
      "465,0.0\n",
      "466,143.23404\n",
      "467,979.886\n",
      "468,596.99207\n",
      "469,118.26346\n",
      "470,0.0\n",
      "471,730.3284\n",
      "472,2178.8577\n",
      "473,0.0\n",
      "474,0.0\n",
      "475,0.0\n",
      "476,0.0\n",
      "477,1452.0779\n",
      "478,71.78165\n",
      "479,2186.4775\n",
      "480,0.0\n",
      "481,586.90625\n",
      "482,1079.5914\n",
      "483,379.29434\n",
      "484,435.1758\n",
      "485,1007.69214\n",
      "486,0.0\n",
      "487,2724.6394\n",
      "488,0.0\n",
      "489,2159.878\n",
      "490,266.57327\n",
      "491,0.0\n",
      "492,571.4599\n",
      "493,0.0\n",
      "494,1246.0017\n",
      "495,0.0\n",
      "496,319.22928\n",
      "497,33.73355\n",
      "498,816.9114\n",
      "499,992.0338\n",
      "500,921.20776\n",
      "501,0.0\n",
      "502,101.3909\n",
      "503,227.98062\n",
      "505,0.0\n",
      "506,1713.7928\n",
      "507,3835.4253\n",
      "508,0.0\n",
      "509,349.6528\n",
      "510,553.855\n",
      "511,1184.9725\n",
      "512,70.257965\n",
      "513,0.0\n",
      "514,0.0\n",
      "515,409.9778\n",
      "516,2180.379\n",
      "517,650.5676\n",
      "518,264.705\n",
      "519,445.87003\n",
      "520,905.1841\n",
      "521,709.3524\n",
      "522,0.0\n",
      "523,906.50183\n",
      "524,223.751\n",
      "525,302.14664\n",
      "526,476.89066\n",
      "527,927.381\n",
      "528,987.2101\n",
      "529,2422.9915\n",
      "530,0.0\n",
      "531,0.0\n",
      "532,191.87222\n",
      "533,0.0\n",
      "534,1400.5999\n",
      "535,701.57825\n",
      "536,219.81424\n",
      "537,0.0\n",
      "538,536.22156\n",
      "539,1298.4253\n",
      "540,0.0\n",
      "541,202.78506\n",
      "542,443.83023\n",
      "543,449.2106\n",
      "544,2081.06\n",
      "545,0.0\n",
      "546,592.6693\n",
      "547,0.0\n",
      "548,73.47922\n",
      "549,501.02774\n",
      "550,891.23926\n",
      "551,0.0\n",
      "552,176.89261\n",
      "553,576.869\n",
      "554,2483.5344\n",
      "555,640.5089\n",
      "556,2309.7832\n",
      "557,1971.5328\n",
      "558,0.0\n",
      "559,451.49905\n",
      "560,451.55814\n",
      "561,246.57877\n",
      "562,924.791\n",
      "563,2474.8157\n",
      "564,1064.7059\n",
      "565,658.40857\n",
      "566,1265.5295\n",
      "567,547.8258\n",
      "568,962.9298\n",
      "569,405.94095\n",
      "570,9471.952\n",
      "571,10210.665\n",
      "572,5149.902\n",
      "573,6620.9106\n",
      "574,8527.982\n",
      "575,9270.985\n",
      "576,3420.375\n",
      "577,2096.7837\n",
      "578,4409.549\n",
      "580,17553.99\n",
      "581,9260.634\n",
      "582,1988.528\n",
      "583,3463.4263\n",
      "584,4993.53\n",
      "585,10216.948\n",
      "586,2242.4436\n",
      "587,2640.672\n",
      "588,667.3811\n",
      "589,6730.858\n",
      "590,1583.3201\n",
      "591,3709.6746\n",
      "592,6953.1284\n",
      "593,2429.1545\n",
      "594,2322.3428\n",
      "595,1158.4894\n",
      "596,4337.9653\n",
      "597,2943.6777\n",
      "598,10315.62\n",
      "599,1912.1353\n",
      "600,1082.0137\n",
      "601,7907.885\n",
      "602,2072.9905\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "source": [
    "with open('submission.csv', 'w', newline='') as csv_file:\r\n",
    "    df.to_csv(path_or_buf=csv_file)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "source": [
    "predictions[0][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "24515.074"
      ]
     },
     "metadata": {},
     "execution_count": 180
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Third model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "with open('C:\\\\ADAMS_tutorials\\\\ADAMS_Assignment\\\\train_dat_dat.pkl','rb') as path_name:\r\n",
    "    train = pickle.load(path_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.mean(text_length)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "EPOCH = 5\r\n",
    "BATCH_SIZE = 64 \r\n",
    "EMBEDDING_DIM = 128\r\n",
    "MAX_TEXT_LENGTH = 463 #mean text lenght  \r\n",
    "MAX_HEADER_LENGTH = 8\r\n",
    "VAL_SPLIT = 0.25\r\n",
    "#MAX_TEXT_LENGTH = 12180\r\n",
    "#MAX_HEADER_LENGTH = 14"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "DATA_COLUMN_NAMES = ['responses', 'month', 'online_since', 'read_time', 'author_tok', 'publisher_tok']\r\n",
    "\r\n",
    "def data(train):\r\n",
    "  y = train['claps'].values\r\n",
    "  #y = np.reshape(y, (-1,1))\r\n",
    "  X_train, X_test, y_train, y_test = train_test_split(train, y, test_size=VAL_SPLIT, random_state=42)\r\n",
    "    \r\n",
    "  #train data \r\n",
    "  X_train_text = pad_sequences(X_train.sequences_text, maxlen=MAX_TEXT_LENGTH, padding='post')\r\n",
    "  X_train_header = pad_sequences(X_train.sequences_header, maxlen=MAX_HEADER_LENGTH, padding='post')\r\n",
    "\r\n",
    "  #test data slices\r\n",
    "  X_test_text = pad_sequences(X_test.sequences_text, maxlen=MAX_TEXT_LENGTH, padding='post')\r\n",
    "  X_test_header = pad_sequences(X_test.sequences_header, maxlen=MAX_HEADER_LENGTH, padding='post')\r\n",
    "\r\n",
    "  X_train_data = X_train[[col for col in X_train.columns if col in DATA_COLUMN_NAMES]]\r\n",
    "  X_test_data = X_test[[col for col in X_test.columns if col in DATA_COLUMN_NAMES]]\r\n",
    "\r\n",
    "  return X_train_text, X_train_header, X_train_data, y_train, X_test_text, X_test_header, X_test_data, y_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "X_train_text, X_train_header, X_train_data, y_train, X_test_text, X_test_header, X_test_data, y_test = data(train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "from gensim.models import KeyedVectors\r\n",
    "embedding=\"skip_w2v_model.model\"\r\n",
    "SAVE_BIN = False\r\n",
    "skip_w2v_model = KeyedVectors.load_word2vec_format(embedding, binary=SAVE_BIN)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The weights initialzation is based on Tobias Sterbak's guide (https://www.depends-on-the-definition.com/guide-to-word-vectors-with-gensim-and-keras/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "word_vectors_try = skip_w2v_model.wv\r\n",
    "MAX_NB_WORDS_try = len(word_vectors_try.vocab)\r\n",
    "print(MAX_NB_WORDS_try)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34286\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "WV_DIM = 256\r\n",
    "nb_words = min(6000, len(word_vectors_try.vocab))\r\n",
    "# we initialize the matrix with random numbers\r\n",
    "wv_matrix = (np.random.rand(nb_words, WV_DIM) - 0.5) / 5.0\r\n",
    "for word, i in tokenizer.word_index.items():\r\n",
    "    if i >= MAX_NB_WORDS_try:\r\n",
    "        continue\r\n",
    "    try:\r\n",
    "        embedding_vector = word_vectors[word]\r\n",
    "        # words not found in embedding index will be all-zeros.\r\n",
    "        wv_matrix[i] = embedding_vector\r\n",
    "    except:\r\n",
    "        pass "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "train = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "input_tensor_text = Input(shape=(MAX_TEXT_LENGTH, ), name=\"text\")\r\n",
    "input_tensor_header = Input(shape=(MAX_HEADER_LENGTH, ), name=\"header\")\r\n",
    "input_tensor_numeric = Input(shape = (len(X_train_data.columns), ), name=\"numeric\")\r\n",
    "\r\n",
    "embedding_layer_text=Embedding(input_dim=NUM_WORDS,\r\n",
    "    output_dim=256,\r\n",
    "    weights=[wv_matrix], #weights to start with, and not nouch during training\r\n",
    "    input_length=MAX_TEXT_LENGTH,\r\n",
    "    trainable=False\r\n",
    "    )(input_tensor_text)\r\n",
    "\r\n",
    "embedding_layer_header=Embedding(input_dim=NUM_WORDS,\r\n",
    "    output_dim=EMBEDDING_DIM,\r\n",
    "    #embeddings_initializer=Constant(w2v_weights), #weights to start with, and not nouch during training\r\n",
    "    input_length=MAX_HEADER_LENGTH,\r\n",
    "    trainable=True\r\n",
    "    )(input_tensor_header)    \r\n",
    "\r\n",
    "GRU_text = GRU(256, return_sequences=True)(embedding_layer_text)\r\n",
    "GRU_text2 = Dropout(0.2)(GRU_text)\r\n",
    "\r\n",
    "GRU_text3 = GRU(128, return_sequences=True)(GRU_text2)\r\n",
    "GRU_text4 = Dropout(0.1)(GRU_text3)\r\n",
    "GRU_text5 = GlobalAveragePooling1D()(GRU_text4)\r\n",
    "\r\n",
    "\r\n",
    "#Header\r\n",
    "GRU_header = GRU(128, return_sequences=True)(embedding_layer_header)\r\n",
    "GRU_header2 = Dropout(0.2)(GRU_header)\r\n",
    "GRU_header3 = GlobalAveragePooling1D()(GRU_header2)\r\n",
    "\r\n",
    "#dense_layer1 = Dense(10, activation='relu')(input_tensor_header)\r\n",
    "#dense_layer2 = Dense(10, activation='relu')(dense_layer1)\r\n",
    "\r\n",
    "dense_numeric = Dense(128, activation = 'relu')(input_tensor_numeric)\r\n",
    "dense_numeric2 = Dropout(0.2)(dense_numeric)\r\n",
    "#dense_numeric3 = GlobalAveragePooling1D()(dense_numeric2)\r\n",
    "\r\n",
    "concat_layer = Concatenate()([GRU_text5, GRU_header3, dense_numeric2])\r\n",
    "\r\n",
    "dense_layer3 = Dense(10, activation='relu')(concat_layer)\r\n",
    "\r\n",
    "output_tensor = Dense(1, activation='relu')(dense_layer3) # result\r\n",
    "\r\n",
    "model_last = Model(inputs = [input_tensor_text, input_tensor_header, input_tensor_numeric], outputs = output_tensor)\r\n",
    "model_last.compile(loss = 'mean_squared_error',\r\n",
    "                          optimizer = RMSprop(clipvalue=1, clipnorm=1),\r\n",
    "                          metrics=['mse'])\r\n",
    "model_last.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               (None, 463)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 463, 256)     1536000     text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 463, 256)     393984      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "header (InputLayer)             (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 463, 256)     0           gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 8, 128)       768000      header[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 463, 128)     147840      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     (None, 8, 128)       98688       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "numeric (InputLayer)            (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 463, 128)     0           gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 8, 128)       0           gru_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          896         numeric[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           3850        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            11          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,949,269\n",
      "Trainable params: 1,413,269\n",
      "Non-trainable params: 1,536,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "model_last_history = model_last.fit(\r\n",
    "    x=[X_train_text, X_train_header, X_train_data],\r\n",
    "    y=np.reshape(y_train, (-1,1)),\r\n",
    "    batch_size=BATCH_SIZE,\r\n",
    "    epochs=10,\r\n",
    "    validation_data=([X_test_text, X_test_header, X_test_data], np.reshape(y_test, (-1,1))))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 46464 samples, validate on 19914 samples\n",
      "Epoch 1/10\n",
      "46464/46464 [==============================] - 5770s 124ms/step - loss: 2257313.5363 - mse: 2257313.5000 - val_loss: 6364151.9319 - val_mse: 6364153.0000\n",
      "Epoch 2/10\n",
      "46464/46464 [==============================] - 6479s 139ms/step - loss: 1952083.2930 - mse: 1952083.6250 - val_loss: 5525066.2813 - val_mse: 5525065.5000\n",
      "Epoch 3/10\n",
      "46464/46464 [==============================] - 6656s 143ms/step - loss: 1633875.5788 - mse: 1633875.2500 - val_loss: 4519588.3821 - val_mse: 4519587.5000\n",
      "Epoch 4/10\n",
      "46464/46464 [==============================] - 6692s 144ms/step - loss: 1689386.2680 - mse: 1689386.1250 - val_loss: 3649665.3507 - val_mse: 3649664.2500\n",
      "Epoch 5/10\n",
      "46464/46464 [==============================] - 6703s 144ms/step - loss: 1721006.2117 - mse: 1721005.5000 - val_loss: 3252305.1707 - val_mse: 3252305.5000\n",
      "Epoch 6/10\n",
      "46464/46464 [==============================] - 6676s 144ms/step - loss: 1772178.9971 - mse: 1772179.3750 - val_loss: 3036180.2011 - val_mse: 3036179.7500\n",
      "Epoch 7/10\n",
      "46464/46464 [==============================] - 6687s 144ms/step - loss: 2002081.8199 - mse: 2002083.6250 - val_loss: 2772714.2974 - val_mse: 2772714.7500\n",
      "Epoch 8/10\n",
      "46464/46464 [==============================] - 6690s 144ms/step - loss: 2451780.2826 - mse: 2451779.7500 - val_loss: 2700090.4826 - val_mse: 2700091.0000\n",
      "Epoch 9/10\n",
      "46464/46464 [==============================] - 6687s 144ms/step - loss: 1682214.0686 - mse: 1682213.5000 - val_loss: 2583798.9493 - val_mse: 2583798.0000\n",
      "Epoch 10/10\n",
      "46464/46464 [==============================] - 6702s 144ms/step - loss: 2079499.5310 - mse: 2079499.6250 - val_loss: 2529516.7566 - val_mse: 2529516.5000\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "model_last_history.model.save(\"model_last_history.h5\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model's validation loss outperforms all other models I did during this assignment. Sadly I later discovered a small error during my tokenization process. When I created tokens for the author and publisher I used a different tokenizer for the train and validation split each time. This means that it is very possible that the neural network just ignored the author and publisher features or even worse it used it and learned something random. That's why I deciced to create a final model afterwards."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment with third model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "def split_test():\r\n",
    "    test_texts = test['texts']\r\n",
    "    test_header = test['header']\r\n",
    "    test_data = test[[col for col in test.columns if col not in ['header', 'texts']]]\r\n",
    "    \r\n",
    "    test_texts = pad_sequences(test.sequences_text, maxlen=MAX_TEXT_LENGTH, padding='post')\r\n",
    "    test_header = pad_sequences(test.sequences_header, maxlen=MAX_HEADER_LENGTH, padding='post')\r\n",
    "    \r\n",
    "    return test_texts, test_header, test_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "test_texts, test_header, test_data = split_test()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "test_data = test_data.drop(['sequences_header', 'sequences_text'], axis='columns')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "test_data.head(1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   responses  month  online_since  read_time  publisher_tok  author_tok\n",
       "0        627      7          1101  75.069609              6           6"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responses</th>\n",
       "      <th>month</th>\n",
       "      <th>online_since</th>\n",
       "      <th>read_time</th>\n",
       "      <th>publisher_tok</th>\n",
       "      <th>author_tok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>627</td>\n",
       "      <td>7</td>\n",
       "      <td>1101</td>\n",
       "      <td>75.069609</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "predictions_last = model_last_history.model.predict([test_texts, test_header, test_data], verbose=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "514/514 [==============================] - 14s 26ms/step\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Indexing the data "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "def getTestIndex():\r\n",
    "    test_csv = pd.read_csv('C:/ADAMS_tutorials/ADAMS_Assignment/Test.csv')\r\n",
    "    test_index = test_csv['index']\r\n",
    "    return test_index\r\n",
    "    \r\n",
    "test_index = getTestIndex()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "# reshape to 1-dim\n",
    "predictions_last = predictions_last.reshape(len(predictions_last), )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "df = pd.DataFrame({'Claps': predictions_last})\n",
    "df = df.set_index(test_index)\n",
    "\n",
    "print('DataFrame:\\n', df)\n",
    "\n",
    "# default CSV\n",
    "predictions_last = df.to_csv()\n",
    "print('\\nCSV String:\\n', predictions_last)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DataFrame:\n",
      "               Claps\n",
      "index              \n",
      "0      64649.152344\n",
      "1      19159.210938\n",
      "2      18453.505859\n",
      "5       9612.684570\n",
      "7       2968.046631\n",
      "...             ...\n",
      "598    20226.867188\n",
      "599     2682.586914\n",
      "600     2938.522705\n",
      "601    13518.168945\n",
      "602     3597.792480\n",
      "\n",
      "[514 rows x 1 columns]\n",
      "\n",
      "CSV String:\n",
      " index,Claps\n",
      "0,64649.152\n",
      "1,19159.21\n",
      "2,18453.506\n",
      "5,9612.685\n",
      "7,2968.0466\n",
      "8,4767.1284\n",
      "9,7630.1343\n",
      "11,3590.9587\n",
      "12,7627.946\n",
      "13,537.08777\n",
      "14,692.2106\n",
      "15,1909.3906\n",
      "16,36660.652\n",
      "17,35518.11\n",
      "18,0.0\n",
      "19,2666.3179\n",
      "20,384.9386\n",
      "21,1503.4114\n",
      "22,1177.7771\n",
      "23,3779.5137\n",
      "25,594.69775\n",
      "26,560.9963\n",
      "27,1875.7062\n",
      "28,11782.3955\n",
      "29,11596.874\n",
      "30,7927.9653\n",
      "31,12809.884\n",
      "32,5131.9395\n",
      "33,8102.9907\n",
      "34,5882.6294\n",
      "35,3532.1912\n",
      "36,3758.157\n",
      "37,1879.908\n",
      "39,2602.9285\n",
      "40,2948.1077\n",
      "41,533.8366\n",
      "42,1520.439\n",
      "43,2908.3074\n",
      "44,2696.9358\n",
      "46,1150.7765\n",
      "47,1637.9927\n",
      "48,3879.281\n",
      "49,1404.4678\n",
      "50,0.0\n",
      "51,0.0\n",
      "52,2120.528\n",
      "53,794.7821\n",
      "54,1298.2517\n",
      "55,2808.5754\n",
      "56,6448.7827\n",
      "57,0.0\n",
      "58,3135.6648\n",
      "61,0.0\n",
      "63,1147.8313\n",
      "64,0.0\n",
      "66,1244.0669\n",
      "67,0.0\n",
      "68,0.0\n",
      "70,1166.3206\n",
      "71,0.0\n",
      "72,0.0\n",
      "73,0.0\n",
      "74,0.0\n",
      "75,0.0\n",
      "76,0.0\n",
      "77,50.723415\n",
      "78,0.0\n",
      "80,759.06366\n",
      "82,2.4619708\n",
      "85,0.0\n",
      "86,3090.1682\n",
      "87,0.0\n",
      "88,0.0\n",
      "91,0.0\n",
      "92,1082.5022\n",
      "93,112.76893\n",
      "94,1099.6493\n",
      "96,0.0\n",
      "98,0.0\n",
      "99,543.8776\n",
      "100,317.92706\n",
      "101,0.0\n",
      "102,0.0\n",
      "103,0.0\n",
      "104,0.0\n",
      "105,246.63173\n",
      "106,0.0\n",
      "107,0.0\n",
      "108,0.0\n",
      "109,0.0\n",
      "110,0.0\n",
      "111,0.0\n",
      "112,0.0\n",
      "113,0.0\n",
      "114,1403.6412\n",
      "116,0.0\n",
      "117,0.0\n",
      "119,35.14288\n",
      "120,0.0\n",
      "121,0.0\n",
      "122,0.0\n",
      "123,318.13943\n",
      "124,0.0\n",
      "125,1184.9287\n",
      "127,0.0\n",
      "128,496.46887\n",
      "129,43.789776\n",
      "130,0.0\n",
      "131,0.0\n",
      "133,0.0\n",
      "134,0.0\n",
      "135,0.0\n",
      "138,0.0\n",
      "139,0.0\n",
      "140,0.0\n",
      "141,0.0\n",
      "142,0.0\n",
      "144,0.0\n",
      "145,0.0\n",
      "146,0.0\n",
      "147,0.0\n",
      "148,841.79285\n",
      "149,0.0\n",
      "150,463.0414\n",
      "151,293.26773\n",
      "152,598.2346\n",
      "153,0.0\n",
      "154,0.0\n",
      "156,0.0\n",
      "158,440.44247\n",
      "160,0.0\n",
      "161,0.0\n",
      "162,0.0\n",
      "163,0.0\n",
      "164,0.0\n",
      "165,0.0\n",
      "166,0.0\n",
      "167,747.31006\n",
      "168,610.3772\n",
      "169,0.0\n",
      "171,0.0\n",
      "172,0.0\n",
      "173,0.0\n",
      "174,0.0\n",
      "175,0.0\n",
      "176,0.0\n",
      "177,0.0\n",
      "178,0.0\n",
      "179,0.0\n",
      "181,0.0\n",
      "182,0.0\n",
      "183,0.0\n",
      "184,0.0\n",
      "185,0.0\n",
      "186,0.0\n",
      "187,0.0\n",
      "188,0.0\n",
      "189,2177.24\n",
      "191,8.790218\n",
      "192,0.0\n",
      "193,0.0\n",
      "195,0.0\n",
      "196,0.0\n",
      "197,0.0\n",
      "198,0.0\n",
      "199,518.54175\n",
      "200,0.0\n",
      "201,598.81555\n",
      "203,0.0\n",
      "204,0.0\n",
      "205,0.0\n",
      "207,0.0\n",
      "208,386.5564\n",
      "209,0.0\n",
      "210,0.0\n",
      "212,0.0\n",
      "213,0.0\n",
      "214,0.0\n",
      "216,0.0\n",
      "217,0.0\n",
      "218,0.0\n",
      "219,1585.8857\n",
      "220,0.0\n",
      "221,0.0\n",
      "222,229.2929\n",
      "226,0.0\n",
      "227,0.0\n",
      "230,0.0\n",
      "231,0.0\n",
      "233,608.6345\n",
      "235,0.0\n",
      "236,0.0\n",
      "237,0.0\n",
      "240,0.0\n",
      "241,0.0\n",
      "242,0.0\n",
      "243,639.839\n",
      "245,0.0\n",
      "246,0.0\n",
      "247,0.0\n",
      "248,0.0\n",
      "249,0.0\n",
      "250,0.0\n",
      "251,0.0\n",
      "252,0.0\n",
      "253,0.0\n",
      "254,0.0\n",
      "257,0.0\n",
      "258,0.0\n",
      "259,0.0\n",
      "261,0.0\n",
      "262,0.0\n",
      "265,0.0\n",
      "266,1696.4075\n",
      "267,0.0\n",
      "268,0.0\n",
      "269,0.0\n",
      "270,0.0\n",
      "272,0.0\n",
      "273,103.03867\n",
      "274,0.0\n",
      "276,0.0\n",
      "278,0.0\n",
      "279,0.0\n",
      "281,442.01004\n",
      "282,0.0\n",
      "283,790.94165\n",
      "284,0.0\n",
      "285,0.0\n",
      "287,0.0\n",
      "288,0.0\n",
      "289,0.0\n",
      "290,0.0\n",
      "291,0.0\n",
      "292,0.0\n",
      "294,0.0\n",
      "295,81712.695\n",
      "297,3691.992\n",
      "298,19136.621\n",
      "300,3371.2173\n",
      "303,2272.4458\n",
      "305,2631.1763\n",
      "306,20262.586\n",
      "307,420.15067\n",
      "309,2991.5503\n",
      "314,42.42584\n",
      "315,2855.9895\n",
      "316,1726.1401\n",
      "318,1405.6414\n",
      "321,20259.602\n",
      "322,0.0\n",
      "324,6515.36\n",
      "326,1057.6654\n",
      "329,2780.5085\n",
      "333,7132.9497\n",
      "335,188.75229\n",
      "337,2186.7312\n",
      "340,1108.159\n",
      "342,1868.6115\n",
      "343,3137.703\n",
      "344,3307.3882\n",
      "345,2239.291\n",
      "346,7242.19\n",
      "348,2578.294\n",
      "349,1597.9836\n",
      "350,849.9608\n",
      "351,2928.0457\n",
      "352,2531.4822\n",
      "353,1202.7832\n",
      "354,239.88063\n",
      "355,0.0\n",
      "356,0.0\n",
      "357,0.0\n",
      "358,341.3024\n",
      "359,783.50244\n",
      "360,586.2915\n",
      "361,0.0\n",
      "362,0.0\n",
      "363,4387.353\n",
      "364,0.0\n",
      "365,1321.3696\n",
      "366,1238.0508\n",
      "367,2328.8997\n",
      "368,0.0\n",
      "369,83.397026\n",
      "370,1887.2693\n",
      "371,490.11\n",
      "372,0.0\n",
      "373,680.3428\n",
      "374,97.104485\n",
      "375,837.617\n",
      "376,0.0\n",
      "377,0.0\n",
      "378,0.0\n",
      "379,0.0\n",
      "380,0.0\n",
      "381,1650.64\n",
      "382,0.0\n",
      "383,0.0\n",
      "384,198.43394\n",
      "385,0.0\n",
      "386,1877.342\n",
      "387,1094.577\n",
      "388,78.44027\n",
      "389,533.23395\n",
      "390,344.1533\n",
      "391,0.0\n",
      "392,223.17711\n",
      "393,0.0\n",
      "394,342.0164\n",
      "395,858.82153\n",
      "396,0.0\n",
      "397,80.99055\n",
      "398,355.95743\n",
      "399,0.0\n",
      "400,901.0585\n",
      "401,0.0\n",
      "403,0.0\n",
      "404,0.0\n",
      "405,8259.891\n",
      "406,0.0\n",
      "407,1157.9963\n",
      "408,0.0\n",
      "409,460.60416\n",
      "410,961.8485\n",
      "411,36.097706\n",
      "412,0.0\n",
      "413,0.0\n",
      "414,0.0\n",
      "415,0.0\n",
      "416,0.0\n",
      "417,0.0\n",
      "418,12636.735\n",
      "419,4180.475\n",
      "420,4146.1016\n",
      "421,5454.7974\n",
      "422,3282.5603\n",
      "423,1406.7869\n",
      "424,1016.3085\n",
      "425,2168.0862\n",
      "426,2183.6387\n",
      "428,2366.671\n",
      "429,1821.6855\n",
      "430,2722.1362\n",
      "431,2239.291\n",
      "432,7242.19\n",
      "433,2578.294\n",
      "434,1597.9836\n",
      "435,849.9608\n",
      "436,2928.0457\n",
      "437,2531.4822\n",
      "438,1202.7827\n",
      "439,239.88063\n",
      "440,0.0\n",
      "441,586.2915\n",
      "442,1197.5701\n",
      "443,777.1666\n",
      "444,4387.353\n",
      "445,0.0\n",
      "446,1321.3696\n",
      "447,1238.0508\n",
      "448,2328.8997\n",
      "449,333.99524\n",
      "450,1165.2305\n",
      "451,1913.0083\n",
      "452,1192.0596\n",
      "453,19952.32\n",
      "454,6054.6987\n",
      "455,5499.8257\n",
      "456,5495.632\n",
      "457,83.397026\n",
      "458,680.3428\n",
      "459,1877.342\n",
      "460,1094.577\n",
      "461,641.5271\n",
      "462,0.0\n",
      "463,0.0\n",
      "464,1085.1025\n",
      "465,0.0\n",
      "466,198.43394\n",
      "467,1388.6589\n",
      "468,1517.9326\n",
      "469,0.0\n",
      "470,341.30234\n",
      "471,2403.53\n",
      "472,0.0\n",
      "473,0.0\n",
      "474,783.50244\n",
      "475,0.0\n",
      "476,0.0\n",
      "477,1887.2693\n",
      "478,490.10995\n",
      "479,0.0\n",
      "480,97.104485\n",
      "481,837.617\n",
      "482,0.0\n",
      "483,0.0\n",
      "484,0.0\n",
      "485,0.0\n",
      "486,0.0\n",
      "487,1650.64\n",
      "488,0.0\n",
      "489,0.0\n",
      "490,0.0\n",
      "491,78.44027\n",
      "492,533.23395\n",
      "493,344.1533\n",
      "494,223.17711\n",
      "495,0.0\n",
      "496,342.0164\n",
      "497,858.82153\n",
      "498,0.0\n",
      "499,80.99055\n",
      "500,355.95743\n",
      "501,0.0\n",
      "502,901.0586\n",
      "503,0.0\n",
      "505,0.0\n",
      "506,0.0\n",
      "507,8259.893\n",
      "508,0.0\n",
      "509,1157.9963\n",
      "510,0.0\n",
      "511,460.60416\n",
      "512,961.8485\n",
      "513,36.097706\n",
      "514,0.0\n",
      "515,0.0\n",
      "516,0.0\n",
      "517,0.0\n",
      "518,0.0\n",
      "519,0.0\n",
      "520,0.0\n",
      "521,1365.2122\n",
      "522,0.0\n",
      "523,625.6201\n",
      "524,0.0\n",
      "525,129.38058\n",
      "526,0.0\n",
      "527,0.0\n",
      "528,1964.4963\n",
      "529,5390.4023\n",
      "530,0.0\n",
      "531,0.0\n",
      "532,0.0\n",
      "533,0.0\n",
      "534,840.0056\n",
      "535,475.47897\n",
      "536,1563.908\n",
      "537,0.0\n",
      "538,0.0\n",
      "539,0.0\n",
      "540,0.0\n",
      "541,0.0\n",
      "542,0.0\n",
      "543,1362.037\n",
      "544,0.0\n",
      "545,681.79456\n",
      "546,259.14786\n",
      "547,261.55502\n",
      "548,84.63209\n",
      "549,727.85126\n",
      "550,0.0\n",
      "551,0.0\n",
      "552,0.0\n",
      "553,0.0\n",
      "554,0.0\n",
      "555,0.0\n",
      "556,0.0\n",
      "557,0.0\n",
      "558,0.0\n",
      "559,231.53651\n",
      "560,0.0\n",
      "561,0.0\n",
      "562,0.0\n",
      "563,0.0\n",
      "564,0.0\n",
      "565,207.47174\n",
      "566,0.0\n",
      "567,0.0\n",
      "568,0.0\n",
      "569,0.0\n",
      "570,22900.156\n",
      "571,18677.318\n",
      "572,12431.035\n",
      "573,10657.984\n",
      "574,17780.777\n",
      "575,16080.009\n",
      "576,5627.0757\n",
      "577,5197.5234\n",
      "578,8504.758\n",
      "580,43911.19\n",
      "581,18369.424\n",
      "582,2356.0168\n",
      "583,6144.72\n",
      "584,8949.717\n",
      "585,20373.277\n",
      "586,4610.4565\n",
      "587,5623.7124\n",
      "588,115.448494\n",
      "589,11791.323\n",
      "590,2030.1667\n",
      "591,7273.521\n",
      "592,13433.792\n",
      "593,4431.4087\n",
      "594,3450.4521\n",
      "595,3450.1096\n",
      "596,6983.3003\n",
      "597,5187.2026\n",
      "598,20226.867\n",
      "599,2682.587\n",
      "600,2938.5227\n",
      "601,13518.169\n",
      "602,3597.7925\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "with open('submission_tesl.csv', 'w', newline='') as csv_file:\n",
    "    df.to_csv(path_or_buf=csv_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "input_tensor_text = Input(shape=(MAX_TEXT_LENGTH, ), name=\"text\")\n",
    "input_tensor_header = Input(shape=(MAX_HEADER_LENGTH, ), name=\"header\")\n",
    "input_tensor_numeric = Input(shape = (len(X_train_data.columns), ), name=\"numeric\")\n",
    "\n",
    "embedding_layer_text=Embedding(input_dim=NUM_WORDS,\n",
    "    output_dim=256,\n",
    "    weights=[wv_matrix], #weights to start with, and not nouch during training\n",
    "    input_length=MAX_TEXT_LENGTH,\n",
    "    trainable=False\n",
    "    )(input_tensor_text)\n",
    "\n",
    "embedding_layer_header=Embedding(input_dim=NUM_WORDS,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    #embeddings_initializer=Constant(w2v_weights), #weights to start with, and not nouch during training\n",
    "    input_length=MAX_HEADER_LENGTH,\n",
    "    trainable=True\n",
    "    )(input_tensor_header)    \n",
    "\n",
    "GRU_text = GRU(256, return_sequences=True)(embedding_layer_text)\n",
    "GRU_text2 = Dropout(0.2)(GRU_text)\n",
    "\n",
    "GRU_text3 = GRU(128, return_sequences=True)(GRU_text2)\n",
    "GRU_text4 = Dropout(0.1)(GRU_text3)\n",
    "GRU_text5 = GlobalAveragePooling1D()(GRU_text4)\n",
    "\n",
    "\n",
    "#Header\n",
    "GRU_header = GRU(128, return_sequences=True)(embedding_layer_header)\n",
    "GRU_header2 = Dropout(0.2)(GRU_header)\n",
    "GRU_header3 = GlobalAveragePooling1D()(GRU_header2)\n",
    "\n",
    "dense_numeric = Dense(128, activation = 'relu')(input_tensor_numeric)\n",
    "dense_numeric2 = Dropout(0.2)(dense_numeric)\n",
    "#dense_numeric3 = GlobalAveragePooling1D()(dense_numeric2)\n",
    "\n",
    "concat_layer = Concatenate()([GRU_text5, GRU_header3, dense_numeric2])\n",
    "\n",
    "dense_layer3 = Dense(10, activation='relu')(concat_layer)\n",
    "\n",
    "output_tensor = Dense(1, activation='relu')(dense_layer3) # result\n",
    "\n",
    "model_final = Model(inputs = [input_tensor_text, input_tensor_header, input_tensor_numeric], outputs = output_tensor)\n",
    "model_final.compile(loss = 'mean_squared_error',\n",
    "                          optimizer = RMSprop(clipvalue=1, clipnorm=1),\n",
    "                          metrics=['mse'])\n",
    "model_final.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               (None, 463)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 463, 256)     1536000     text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "gru_4 (GRU)                     (None, 463, 256)     393984      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "header (InputLayer)             (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 463, 256)     0           gru_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 8, 128)       768000      header[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_5 (GRU)                     (None, 463, 128)     147840      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_6 (GRU)                     (None, 8, 128)       98688       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "numeric (InputLayer)            (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 463, 128)     0           gru_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 8, 128)       0           gru_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          896         numeric[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 128)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 384)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           3850        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            11          dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,949,269\n",
      "Trainable params: 1,413,269\n",
      "Non-trainable params: 1,536,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "from time import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "t = time()\n",
    "model_final_history = model_final.fit(\n",
    "    x=[X_train_text, X_train_header, X_train_data],\n",
    "    y=np.reshape(y_train, (-1,1)),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=10,\n",
    "    validation_data=([X_test_text, X_test_header, X_test_data], np.reshape(y_test, (-1,1))))\n",
    "print('Time to fit model: {} mins'.format(round((time() - t) / 60, 2)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 49783 samples, validate on 16595 samples\n",
      "Epoch 1/10\n",
      "49783/49783 [==============================] - 5616s 113ms/step - loss: 2483008.8048 - mse: 2483008.2500 - val_loss: 6628712.5763 - val_mse: 6628715.5000\n",
      "Epoch 2/10\n",
      "49783/49783 [==============================] - 6356s 128ms/step - loss: 2333764.9720 - mse: 2333763.0000 - val_loss: 6138931.2380 - val_mse: 6138930.0000\n",
      "Epoch 3/10\n",
      "49783/49783 [==============================] - 6504s 131ms/step - loss: 1992857.9371 - mse: 1992860.8750 - val_loss: 5365106.9834 - val_mse: 5365108.5000\n",
      "Epoch 4/10\n",
      "49783/49783 [==============================] - 6497s 130ms/step - loss: 1757697.0682 - mse: 1757696.3750 - val_loss: 4741883.6374 - val_mse: 4741886.0000\n",
      "Epoch 5/10\n",
      "49783/49783 [==============================] - 6559s 132ms/step - loss: 1578199.7079 - mse: 1578200.8750 - val_loss: 4243435.2952 - val_mse: 4243435.5000\n",
      "Epoch 6/10\n",
      "49783/49783 [==============================] - 6554s 132ms/step - loss: 1750611.0450 - mse: 1750611.6250 - val_loss: 3986890.9461 - val_mse: 3986891.2500\n",
      "Epoch 7/10\n",
      "49783/49783 [==============================] - 6560s 132ms/step - loss: 1548525.0740 - mse: 1548524.7500 - val_loss: 3734165.3832 - val_mse: 3734164.2500\n",
      "Epoch 8/10\n",
      "49783/49783 [==============================] - 6550s 132ms/step - loss: 1801898.3661 - mse: 1801898.2500 - val_loss: 3545009.4922 - val_mse: 3545009.0000\n",
      "Epoch 9/10\n",
      "49783/49783 [==============================] - 6473s 130ms/step - loss: 1802328.5713 - mse: 1802326.0000 - val_loss: 3411791.7462 - val_mse: 3411793.2500\n",
      "Epoch 10/10\n",
      "49783/49783 [==============================] - 6530s 131ms/step - loss: 1840916.6477 - mse: 1840917.6250 - val_loss: 3241812.5637 - val_mse: 3241812.2500\n",
      "Time to fit model: 1070.01 mins\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "model_final_history.model.save(\"model_last_history.h5\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment final"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "with open('C:\\\\ADAMS_tutorials\\\\ADAMS_Assignment\\\\data_pre_clean_test.pkl','rb') as path_name:\r\n",
    "    test = pickle.load(path_name) #load\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "test.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   responses                                             header  \\\n",
       "0        627  Why Everyone Missed the Most Mind-Blowing Feat...   \n",
       "1        156  NEO versus Ethereum: Why NEO might be 2018’s s...   \n",
       "2        176                   The Cryptocurrency Trading Bible   \n",
       "3         72  Stablecoins: designing a price-stable cryptocu...   \n",
       "4         19       Chaos vs. Order — The Cryptocurrency Dilemma   \n",
       "\n",
       "                                               texts           author  \\\n",
       "0  There’s one incredible feature of cryptocurren...  Daniel Jeffries   \n",
       "1  OnChainNEO’s founders Da HongFei and Erik Zhan...    Noam Levenson   \n",
       "2  So you want to trade cryptocurrency? You’ve se...  Daniel Jeffries   \n",
       "3  A useful currency should be a medium of exchan...   Haseeb Qureshi   \n",
       "4  Crypto crypto crypto crypto. It’s here. It’s h...     William Belk   \n",
       "\n",
       "     publisher  month  online_since  read_time  \n",
       "0  Hacker Noon      7          1101  75.069609  \n",
       "1  Hacker Noon     12           973  72.072011  \n",
       "2  Hacker Noon      7          1111   3.581081  \n",
       "3  Hacker Noon      2           898  63.087171  \n",
       "4  Hacker Noon      1           920   0.317746  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responses</th>\n",
       "      <th>header</th>\n",
       "      <th>texts</th>\n",
       "      <th>author</th>\n",
       "      <th>publisher</th>\n",
       "      <th>month</th>\n",
       "      <th>online_since</th>\n",
       "      <th>read_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>627</td>\n",
       "      <td>Why Everyone Missed the Most Mind-Blowing Feat...</td>\n",
       "      <td>There’s one incredible feature of cryptocurren...</td>\n",
       "      <td>Daniel Jeffries</td>\n",
       "      <td>Hacker Noon</td>\n",
       "      <td>7</td>\n",
       "      <td>1101</td>\n",
       "      <td>75.069609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156</td>\n",
       "      <td>NEO versus Ethereum: Why NEO might be 2018’s s...</td>\n",
       "      <td>OnChainNEO’s founders Da HongFei and Erik Zhan...</td>\n",
       "      <td>Noam Levenson</td>\n",
       "      <td>Hacker Noon</td>\n",
       "      <td>12</td>\n",
       "      <td>973</td>\n",
       "      <td>72.072011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>The Cryptocurrency Trading Bible</td>\n",
       "      <td>So you want to trade cryptocurrency? You’ve se...</td>\n",
       "      <td>Daniel Jeffries</td>\n",
       "      <td>Hacker Noon</td>\n",
       "      <td>7</td>\n",
       "      <td>1111</td>\n",
       "      <td>3.581081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72</td>\n",
       "      <td>Stablecoins: designing a price-stable cryptocu...</td>\n",
       "      <td>A useful currency should be a medium of exchan...</td>\n",
       "      <td>Haseeb Qureshi</td>\n",
       "      <td>Hacker Noon</td>\n",
       "      <td>2</td>\n",
       "      <td>898</td>\n",
       "      <td>63.087171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>Chaos vs. Order — The Cryptocurrency Dilemma</td>\n",
       "      <td>Crypto crypto crypto crypto. It’s here. It’s h...</td>\n",
       "      <td>William Belk</td>\n",
       "      <td>Hacker Noon</td>\n",
       "      <td>1</td>\n",
       "      <td>920</td>\n",
       "      <td>0.317746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "test.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 514 entries, 0 to 513\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   responses         514 non-null    int32  \n",
      " 1   header            514 non-null    object \n",
      " 2   texts             514 non-null    object \n",
      " 3   author            514 non-null    object \n",
      " 4   publisher         514 non-null    object \n",
      " 5   month             514 non-null    int64  \n",
      " 6   online_since      514 non-null    int64  \n",
      " 7   read_time         514 non-null    float64\n",
      " 8   sequences_text    514 non-null    object \n",
      " 9   sequences_header  514 non-null    object \n",
      " 10  author_tok        514 non-null    int64  \n",
      " 11  publisher_tok     514 non-null    int64  \n",
      "dtypes: float64(1), int32(1), int64(4), object(6)\n",
      "memory usage: 46.3+ KB\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "def split_test():\r\n",
    "    test_texts = test['texts']\r\n",
    "    test_header = test['header']\r\n",
    "    test_data = test[[col for col in test.columns if col in DATA_COLUMN_NAMES]]\r\n",
    "    \r\n",
    "    test_texts = pad_sequences(test.sequences_text, maxlen=MAX_TEXT_LENGTH, padding='post')\r\n",
    "    test_header = pad_sequences(test.sequences_header, maxlen=MAX_HEADER_LENGTH, padding='post')\r\n",
    "    \r\n",
    "    return test_texts, test_header, test_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "test_texts, test_header, test_data = split_test()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "test_data.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 514 entries, 0 to 513\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   responses      514 non-null    int32  \n",
      " 1   month          514 non-null    int64  \n",
      " 2   online_since   514 non-null    int64  \n",
      " 3   read_time      514 non-null    float64\n",
      " 4   author_tok     514 non-null    int64  \n",
      " 5   publisher_tok  514 non-null    int64  \n",
      "dtypes: float64(1), int32(1), int64(4)\n",
      "memory usage: 22.2 KB\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "predictions_final = model_final_history.model.predict([test_texts, test_header, test_data], verbose=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "514/514 [==============================] - 11s 21ms/step\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "# reshape to 1-dim\r\n",
    "predictions_final = predictions_final.reshape(len(predictions_final), )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "df = pd.DataFrame({'Claps': predictions_final})\r\n",
    "df = df.set_index(test_index)\r\n",
    "\r\n",
    "print('DataFrame:\\n', df)\r\n",
    "\r\n",
    "# default CSV\r\n",
    "predictions_final_csv = df.to_csv()\r\n",
    "print('\\nCSV String:\\n', predictions_final_csv)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DataFrame:\n",
      "               Claps\n",
      "index              \n",
      "0      53338.164062\n",
      "1      19299.582031\n",
      "2      20095.158203\n",
      "5       9737.634766\n",
      "7       2045.726685\n",
      "...             ...\n",
      "598    21398.576172\n",
      "599     1933.810181\n",
      "600     2635.977295\n",
      "601    13349.852539\n",
      "602     3213.890381\n",
      "\n",
      "[514 rows x 1 columns]\n",
      "\n",
      "CSV String:\n",
      " index,Claps\n",
      "0,53338.164\n",
      "1,19299.582\n",
      "2,20095.158\n",
      "5,9737.635\n",
      "7,2045.7267\n",
      "8,3773.4346\n",
      "9,8184.631\n",
      "11,3650.0989\n",
      "12,7231.986\n",
      "13,483.55698\n",
      "14,0.0\n",
      "15,1012.1825\n",
      "16,32414.014\n",
      "17,32683.674\n",
      "18,0.0\n",
      "19,1474.4896\n",
      "20,0.0\n",
      "21,1073.9667\n",
      "22,84.629715\n",
      "23,3398.239\n",
      "25,71.101845\n",
      "26,0.0\n",
      "27,1236.9233\n",
      "28,13359.181\n",
      "29,12483.3\n",
      "30,8396.718\n",
      "31,13829.962\n",
      "32,4876.7725\n",
      "33,7694.9946\n",
      "34,4379.144\n",
      "35,1793.4791\n",
      "36,2869.9705\n",
      "37,1136.2122\n",
      "39,2065.2336\n",
      "40,2113.179\n",
      "41,0.0\n",
      "42,689.72437\n",
      "43,1924.5586\n",
      "44,1915.6\n",
      "46,190.91785\n",
      "47,748.8784\n",
      "48,3061.946\n",
      "49,799.04565\n",
      "50,0.0\n",
      "51,0.0\n",
      "52,1684.5853\n",
      "53,0.0\n",
      "54,222.3475\n",
      "55,859.3357\n",
      "56,6249.48\n",
      "57,0.0\n",
      "58,2412.4739\n",
      "61,0.0\n",
      "63,0.0\n",
      "64,0.0\n",
      "66,0.0\n",
      "67,0.0\n",
      "68,0.0\n",
      "70,534.0254\n",
      "71,0.0\n",
      "72,0.0\n",
      "73,0.0\n",
      "74,0.0\n",
      "75,0.0\n",
      "76,0.0\n",
      "77,0.0\n",
      "78,0.0\n",
      "80,0.0\n",
      "82,0.0\n",
      "85,0.0\n",
      "86,3202.2004\n",
      "87,0.0\n",
      "88,0.0\n",
      "91,0.0\n",
      "92,200.78497\n",
      "93,0.0\n",
      "94,156.21162\n",
      "96,0.0\n",
      "98,0.0\n",
      "99,0.0\n",
      "100,0.0\n",
      "101,0.0\n",
      "102,0.0\n",
      "103,0.0\n",
      "104,0.0\n",
      "105,0.0\n",
      "106,0.0\n",
      "107,0.0\n",
      "108,0.0\n",
      "109,0.0\n",
      "110,0.0\n",
      "111,0.0\n",
      "112,0.0\n",
      "113,0.0\n",
      "114,581.203\n",
      "116,0.0\n",
      "117,0.0\n",
      "119,0.0\n",
      "120,0.0\n",
      "121,0.0\n",
      "122,0.0\n",
      "123,0.0\n",
      "124,0.0\n",
      "125,620.9337\n",
      "127,0.0\n",
      "128,0.0\n",
      "129,0.0\n",
      "130,0.0\n",
      "131,0.0\n",
      "133,0.0\n",
      "134,74.29418\n",
      "135,0.0\n",
      "138,0.0\n",
      "139,0.0\n",
      "140,0.0\n",
      "141,0.0\n",
      "142,0.0\n",
      "144,0.0\n",
      "145,0.0\n",
      "146,0.0\n",
      "147,0.0\n",
      "148,197.57356\n",
      "149,0.0\n",
      "150,0.0\n",
      "151,0.0\n",
      "152,1121.7811\n",
      "153,0.0\n",
      "154,0.0\n",
      "156,0.0\n",
      "158,0.0\n",
      "160,0.0\n",
      "161,0.0\n",
      "162,0.0\n",
      "163,0.0\n",
      "164,0.0\n",
      "165,0.0\n",
      "166,0.0\n",
      "167,0.0\n",
      "168,111.92434\n",
      "169,0.0\n",
      "171,0.0\n",
      "172,0.0\n",
      "173,0.0\n",
      "174,0.0\n",
      "175,0.0\n",
      "176,0.0\n",
      "177,0.0\n",
      "178,0.0\n",
      "179,0.0\n",
      "181,34.082573\n",
      "182,0.0\n",
      "183,0.0\n",
      "184,0.0\n",
      "185,0.0\n",
      "186,0.0\n",
      "187,0.0\n",
      "188,0.0\n",
      "189,1377.087\n",
      "191,0.0\n",
      "192,0.0\n",
      "193,0.0\n",
      "195,0.0\n",
      "196,0.0\n",
      "197,0.0\n",
      "198,0.0\n",
      "199,0.0\n",
      "200,0.0\n",
      "201,0.0\n",
      "203,0.0\n",
      "204,0.0\n",
      "205,0.0\n",
      "207,0.0\n",
      "208,0.0\n",
      "209,0.0\n",
      "210,0.0\n",
      "212,0.0\n",
      "213,0.0\n",
      "214,0.0\n",
      "216,0.0\n",
      "217,0.0\n",
      "218,0.0\n",
      "219,467.2026\n",
      "220,0.0\n",
      "221,0.0\n",
      "222,0.0\n",
      "226,0.0\n",
      "227,0.0\n",
      "230,0.0\n",
      "231,0.0\n",
      "233,176.19638\n",
      "235,0.0\n",
      "236,0.0\n",
      "237,0.0\n",
      "240,0.0\n",
      "241,0.0\n",
      "242,0.0\n",
      "243,214.41109\n",
      "245,0.0\n",
      "246,0.0\n",
      "247,0.0\n",
      "248,0.0\n",
      "249,0.0\n",
      "250,0.0\n",
      "251,0.0\n",
      "252,0.0\n",
      "253,0.0\n",
      "254,0.0\n",
      "257,0.0\n",
      "258,0.0\n",
      "259,0.0\n",
      "261,0.0\n",
      "262,0.0\n",
      "265,0.0\n",
      "266,604.6738\n",
      "267,0.0\n",
      "268,0.0\n",
      "269,0.0\n",
      "270,0.0\n",
      "272,0.0\n",
      "273,0.0\n",
      "274,0.0\n",
      "276,0.0\n",
      "278,0.0\n",
      "279,0.0\n",
      "281,0.0\n",
      "282,0.0\n",
      "283,0.0\n",
      "284,0.0\n",
      "285,0.0\n",
      "287,0.0\n",
      "288,0.0\n",
      "289,0.0\n",
      "290,0.0\n",
      "291,0.0\n",
      "292,0.0\n",
      "294,0.0\n",
      "295,64226.53\n",
      "297,3091.9197\n",
      "298,20837.814\n",
      "300,2683.607\n",
      "303,916.69116\n",
      "305,2086.0188\n",
      "306,22397.643\n",
      "307,0.0\n",
      "309,1423.9906\n",
      "314,0.0\n",
      "315,1552.4677\n",
      "316,989.1907\n",
      "318,475.64987\n",
      "321,22434.58\n",
      "322,0.0\n",
      "324,5880.7847\n",
      "326,0.0\n",
      "329,1152.1813\n",
      "333,7395.59\n",
      "335,0.0\n",
      "337,1609.0701\n",
      "340,509.59006\n",
      "342,1008.5338\n",
      "343,2654.1184\n",
      "344,2875.2478\n",
      "345,1418.8988\n",
      "346,6551.5674\n",
      "348,1265.0206\n",
      "349,207.98174\n",
      "350,1121.3252\n",
      "351,1562.5831\n",
      "352,2005.2389\n",
      "353,445.0527\n",
      "354,148.35127\n",
      "355,0.0\n",
      "356,0.0\n",
      "357,0.0\n",
      "358,0.0\n",
      "359,651.8683\n",
      "360,0.0\n",
      "361,0.0\n",
      "362,0.0\n",
      "363,4275.999\n",
      "364,0.0\n",
      "365,112.48835\n",
      "366,628.518\n",
      "367,1350.8961\n",
      "368,0.0\n",
      "369,0.0\n",
      "370,1383.0277\n",
      "371,0.0\n",
      "372,0.0\n",
      "373,0.0\n",
      "374,0.0\n",
      "375,0.0\n",
      "376,0.0\n",
      "377,0.0\n",
      "378,0.0\n",
      "379,0.0\n",
      "380,0.0\n",
      "381,1309.6891\n",
      "382,0.0\n",
      "383,0.0\n",
      "384,0.0\n",
      "385,0.0\n",
      "386,1538.2684\n",
      "387,7.5945425\n",
      "388,0.0\n",
      "389,801.79065\n",
      "390,0.0\n",
      "391,0.0\n",
      "392,94.1531\n",
      "393,0.0\n",
      "394,139.46858\n",
      "395,548.5789\n",
      "396,0.0\n",
      "397,0.0\n",
      "398,74.75141\n",
      "399,0.0\n",
      "400,564.35016\n",
      "401,0.0\n",
      "403,0.0\n",
      "404,0.0\n",
      "405,8402.052\n",
      "406,0.0\n",
      "407,733.032\n",
      "408,0.0\n",
      "409,0.0\n",
      "410,526.82794\n",
      "411,99.58928\n",
      "412,0.0\n",
      "413,0.0\n",
      "414,0.0\n",
      "415,0.0\n",
      "416,0.0\n",
      "417,0.0\n",
      "418,13727.88\n",
      "419,4120.3584\n",
      "420,2640.2566\n",
      "421,4939.288\n",
      "422,2629.4338\n",
      "423,1169.7924\n",
      "424,378.277\n",
      "425,1740.0364\n",
      "426,888.31396\n",
      "428,3082.164\n",
      "429,1001.6073\n",
      "430,2578.3643\n",
      "431,1418.8988\n",
      "432,6551.5674\n",
      "433,1265.0206\n",
      "434,207.98174\n",
      "435,1121.3252\n",
      "436,1562.5831\n",
      "437,2005.2396\n",
      "438,445.053\n",
      "439,148.35127\n",
      "440,0.0\n",
      "441,0.0\n",
      "442,0.0\n",
      "443,598.2877\n",
      "444,4275.999\n",
      "445,0.0\n",
      "446,112.48835\n",
      "447,628.518\n",
      "448,1350.8961\n",
      "449,0.0\n",
      "450,677.3251\n",
      "451,1187.7542\n",
      "452,640.8728\n",
      "453,20223.135\n",
      "454,5534.665\n",
      "455,3956.0364\n",
      "456,5613.283\n",
      "457,0.0\n",
      "458,0.0\n",
      "459,1538.2684\n",
      "460,7.5945425\n",
      "461,813.5529\n",
      "462,0.0\n",
      "463,0.0\n",
      "464,621.8253\n",
      "465,0.0\n",
      "466,0.0\n",
      "467,517.3396\n",
      "468,791.7338\n",
      "469,0.0\n",
      "470,0.0\n",
      "471,942.10034\n",
      "472,0.0\n",
      "473,0.0\n",
      "474,651.8683\n",
      "475,0.0\n",
      "476,0.0\n",
      "477,1383.0277\n",
      "478,0.0\n",
      "479,0.0\n",
      "480,0.0\n",
      "481,0.0\n",
      "482,0.0\n",
      "483,0.0\n",
      "484,0.0\n",
      "485,0.0\n",
      "486,0.0\n",
      "487,1309.6891\n",
      "488,0.0\n",
      "489,0.0\n",
      "490,0.0\n",
      "491,0.0\n",
      "492,801.79065\n",
      "493,0.0\n",
      "494,94.1531\n",
      "495,0.0\n",
      "496,139.46858\n",
      "497,548.5789\n",
      "498,0.0\n",
      "499,0.0\n",
      "500,74.75141\n",
      "501,0.0\n",
      "502,564.35114\n",
      "503,0.0\n",
      "505,0.0\n",
      "506,0.0\n",
      "507,8402.054\n",
      "508,0.0\n",
      "509,733.032\n",
      "510,0.0\n",
      "511,0.0\n",
      "512,526.82794\n",
      "513,99.58928\n",
      "514,0.0\n",
      "515,0.0\n",
      "516,0.0\n",
      "517,0.0\n",
      "518,0.0\n",
      "519,0.0\n",
      "520,0.0\n",
      "521,889.5488\n",
      "522,0.0\n",
      "523,186.9213\n",
      "524,0.0\n",
      "525,0.0\n",
      "526,0.0\n",
      "527,0.0\n",
      "528,1568.9852\n",
      "529,4357.2417\n",
      "530,0.0\n",
      "531,0.0\n",
      "532,0.0\n",
      "533,0.0\n",
      "534,0.0\n",
      "535,219.7577\n",
      "536,213.40756\n",
      "537,0.0\n",
      "538,0.0\n",
      "539,0.0\n",
      "540,0.0\n",
      "541,0.0\n",
      "542,0.0\n",
      "543,1085.4957\n",
      "544,0.0\n",
      "545,0.0\n",
      "546,0.0\n",
      "547,0.0\n",
      "548,0.0\n",
      "549,193.61028\n",
      "550,0.0\n",
      "551,0.0\n",
      "552,0.0\n",
      "553,0.0\n",
      "554,0.0\n",
      "555,0.0\n",
      "556,0.0\n",
      "557,0.0\n",
      "558,0.0\n",
      "559,0.0\n",
      "560,0.0\n",
      "561,0.0\n",
      "562,0.0\n",
      "563,0.0\n",
      "564,0.0\n",
      "565,334.9371\n",
      "566,0.0\n",
      "567,0.0\n",
      "568,0.0\n",
      "569,0.0\n",
      "570,22198.166\n",
      "571,19164.244\n",
      "572,12391.481\n",
      "573,11216.315\n",
      "574,18437.266\n",
      "575,17188.033\n",
      "576,5551.5444\n",
      "577,5185.982\n",
      "578,8263.146\n",
      "580,39910.645\n",
      "581,19103.207\n",
      "582,2015.9115\n",
      "583,6492.1104\n",
      "584,8416.552\n",
      "585,22573.54\n",
      "586,3516.2542\n",
      "587,5776.1514\n",
      "588,70.5568\n",
      "589,11211.738\n",
      "590,1077.4686\n",
      "591,5560.2656\n",
      "592,14120.026\n",
      "593,4358.1963\n",
      "594,2730.869\n",
      "595,2967.9731\n",
      "596,5993.738\n",
      "597,4848.577\n",
      "598,21398.576\n",
      "599,1933.8102\n",
      "600,2635.9773\n",
      "601,13349.853\n",
      "602,3213.8904\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "with open('submission_tesl_final.csv', 'w', newline='') as csv_file:\r\n",
    "    df.to_csv(path_or_buf=csv_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter tuning\n",
    "I tried to use hyperparameter tuning but sadly did not have enough time to finish the tuning and produce meaningful results. The coding procedures are present in extra [jupyter file](parameter_tuning.ipynb). As a template to develope the parameter tuning model I used the blog post article of \"Lianne & Justin\" from Just into Data: \"Hyperparameter Tuning with Python: Keras Step-by-Step Guide\". I used a desktop PC for this task and tried to utilized the available graphic card there. Sadly I wasn't able to archive this task and was forced to cancel the tuning process.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "During the project I developed several GRU models with the aim to predict the number of claps in the medium’s article. Different approaches and experiments with different parameters, words numbers, the length of the texts and different pre-trained word embeddings models, like CBOW and Skip-gram has shown that there is always room for improvement. The experiments have shown that adding the depth to the model is an effective tactic to model’s performance improvement. The other important conclusion is that parameters are playing a crucial role. I suppose that if the parameter tuning would be finished the resulting model could perform significantly better than any of my models.\n",
    "Because all models are based on the input data the importance of explanatory data analysis and then the data preparations remain a high priority.\n",
    "The next steps could be the further improvement of the model’s architecture, parameter tuning and using more computational power for example by using an online service (AWS) or a powerful setup consisting of Nvidia graphic cards to use the CUDA environment.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Literature\n",
    "\n",
    "Casey Botticello (2019): “How Do Claps Work on Medium?”, https://medium.com/blogging-guide/how-do-claps-work-on-medium-b2897784ce6b (last visited 31.08.2020)\n",
    "\n",
    "Dipanjan Sarkar (2019): Text Analytics with Python, Apress Media LLC\n",
    "\n",
    "Gabriel L. Schlomer, Sheri Bauman, and Noel A. Card (2010): Best Practices for Missing Data Management in Counseling Psychology, Journal of Counseling Psychology © 2010 American Psychological Association, Vol. 57, No. 1, pp. 1–10 \n",
    "\n",
    "Guillaume Lample, Miguel Ballesteros, Sandeep Sub-ramanian, Kazuya Kawakami, Chris Dyer (2016): Neural  architectures  for  named  entity  recognition. InHLT-NAACL\n",
    "\n",
    "Ivan Vasilev, Daniel Slater, Gianmario Spacagna, Peter Roelants, Valentino Zocca (2019): Python Deep Learning, Second Edition, Exploring deep learning techniques and neural network architectures with PyTorch, Keras, and TensorFlow, Published by Packt Publishing Ltd., Birmingham UK\n",
    "\n",
    "Jason Brownlee (2019): How to Choose Loss Functions When Training Deep Learning Neural Networks, https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/ (last visited 28.08.2020)\n",
    "\n",
    "Jeff Schneider (1997): Cross Validation, https://www.cs.cmu.edu/~schneide/tut5/node42.html (last visited 31.08.2020)\n",
    "\n",
    "Kavita Ganesan (2020): Word2Vec: A Comparison Between CBOW, SkipGram & SkipGramSI, https://kavita-ganesan.com/comparison-between-cbow-skipgram-subword/ (last visited 24.08.2019)\n",
    "\n",
    "Lalit Vyas (2020): Word2Vec — CBOW & Skip-gram : Algorithmic Optimizations, https://medium.com/analytics-vidhya/word2vec-cbow-skip-gram-algorithmic-optimizations-921d6f62d739 (last visited 24.08.2019)\n",
    "\n",
    "Lianne & Justin (2020): Hyperparameter Tuning with Python: Keras Step-by-Step Guide, https://www.justintodata.com/hyperparameter-tuning-with-python-keras-guide/ (last visited 31.08.2020)\n",
    "\n",
    "Medium https://medium.com/ (last visited 31.08.2020)\n",
    "\n",
    "Pierre Megret (2018): Gensim Word2Vec Tutorial, https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial (last visited 24.08.2019) \n",
    "\n",
    "Rohan Hirekerur (2020): A Comprehensive Guide To Loss Functions — Part 1 : Regression, https://medium.com/analytics-vidhya/a-comprehensive-guide-to-loss-functions-part-1-regression-ff8b847675d6 (last visited 31.08.2020) \n",
    "\n",
    "Tobias Sterbak (2018): Guide to word vectors with gensim and keras, https://www.depends-on-the-definition.com/guide-to-word-vectors-with-gensim-and-keras/ (last visited 31.08.2020)\n",
    "\n",
    "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean (2013): Efficient Estimation of Word Representations in Vector Space, https://arxiv.org/pdf/1301.3781.pdf (last visited 24.08.2019)\n",
    "\n",
    "Usman Malik (2020): Python for NLP: Creating Multi-Data-Type Classification Models with Keras, https://stackabuse.com/python-for-nlp-creating-multi-data-type-classification-models-with-keras/ (last visited 31.08.2020)\n",
    "\n",
    "Vitaly Bushaev (2018): Understanding RMSprop — faster neural network learning, https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a (last visited 20.08.2020) \n",
    "\n",
    "Xuezhe Ma and Eduard Hovy (2016): End-to-end se-quence labeling via bi-directional lstm-cnns-crf.  InProceedings of the 54th Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long  Papers), Association  for  Computational  Linguistics, Berlin, Germany, pp. 1064–1074,  http://www.aclweb.org/anthology/P16-1101 (last visited 24.08.2019)\n"
   ],
   "metadata": {}
  }
 ]
}